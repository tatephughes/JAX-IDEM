#+TITLE: Org Notebook for the IDEM project

:BOILERPLATE:
#+BIBLIOGRAPHY: ../../../bibliography.bib
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amsthm,amssymb,bm,bbm,tikz,tkz-graph, graphicx, subcaption, mathtools, algpseudocode}
#+LATEX_HEADER: \usepackage[cache=false]{minted}
#+LATEX_HEADER: \usetikzlibrary{arrows}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \usetikzlibrary{matrix}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb E}
#+LATEX_HEADER: \DeclareMathOperator{\prob}{\mathbb P}
#+LATEX_HEADER: \DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
#+LATEX_HEADER: \DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
#+LATEX_HEADER: \DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
#+LATEX_HEADER: \DeclareMathOperator{\normal}{\mathcal N}
#+LATEX_HEADER: \DeclareMathOperator{\invgam}{\mathcal{IG}}
#+LATEX_HEADER: \newcommand*{\mat}[1]{\bm{#1}}
#+LATEX_HEADER: \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
#+LATEX_HEADER: \renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
#+EXPORT_EXCLUDE_TAGS: noexport
:END:

* Import the Project

#+begin_src python :session example :results none
from IDEM import *
from utilities import create_grid
#+end_src

* Constructing the Process Grid and Kernel

Closely following [cite:@wikle2019spatio];

#+begin_src python :session example :results none
d = 2 # set the spatial dimension

ds = 0.01

# creates a grid 
s_grid = create_grid(jnp.array([[0,1]]),
                     jnp.array([0.01]))
N = len(s_grid)

nT = 201
t_grid = jnp.arange(0, nT-1)

# This flattening only works for 1D! I believe jnp.meshgrid doesn't support what will be at least 3D grids, so this may need rethinking. Thinking about it, I may be able to reuse some of the code from ~create_grid~ to get this to work properly.
st_grid = jnp.stack(jnp.meshgrid(s_grid.flatten(), t_grid), axis=-1)
#+end_src

We then define the transition kernel; as an example, this is using a Gaussian kernel
\begin{align*}
\kappa(\vec s, \vec x; \alpha, \vec\mu, \mat\Sigma) = \alpha\exp \left[ -(\vec s - \vec x - \vec \mu) \mat\Sigma^{-1} (\vec s - \vec x - \vec \mu) \right].
\end{align*}
Implemented as a JAX function,

#+begin_src python :session example :results none
from jax.numpy.linalg import solve
from utilities import outer_op

def kappa(s, x, thetap):
    
    scale = thetap[0]
    shift = thetap[1]
    shape = thetap[2]
    
    return scale * jnp.exp(-(s-x-shift)**2/shape)

#+end_src

Lets first try to recreate the results in the original R book; here, the dimension is 1, and we consider the point $s=0.5$ for the following parameter options

#+begin_src python :session example :results none
thetap1 = (jnp.array(40),jnp.array([0]),jnp.array([[0.0002]]))
thetap2 = (jnp.array(5.75),jnp.array([0]),jnp.array([[0.01]]))
thetap3 = (jnp.array(8),jnp.array([0.1]),jnp.array([[0.005]]))
thetap4 = (jnp.array(8),jnp.array([-0.1]),jnp.array([[0.005]]))
#+end_src

when we apply ~kappa_outer~, we can unpack these by ~*thatap1~, for example.

Lets make a 1D grid with the ~create_grid~ function;

#+begin_src python :session example :results none
kappa1 = lambda x,y: kappa(x,y,thetap1)
kappa2 = lambda x,y: kappa(x,y,thetap2)
kappa3 = lambda x,y: kappa(x,y,thetap3)
kappa4 = lambda x,y: kappa(x,y,thetap4)

k_x_1 = outer_op(jnp.array([[0.5]]), s_grid, kappa1)
k_x_2 = outer_op(jnp.array([[0.5]]), s_grid, kappa2)
k_x_3 = outer_op(jnp.array([[0.5]]), s_grid, kappa3)
k_x_4 = outer_op(jnp.array([[0.5]]), s_grid, kappa4)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (5,2)  # Width, Height in inches

fig, axs = plt.subplots(4, 1, figsize=(5, 4))

axs[0].plot(s_grid.flatten(), k_x_1.flatten())
axs[0].set_title('Kernel Plots', fontsize=10)
axs[1].plot(s_grid.flatten(), k_x_2.flatten())
axs[2].plot(s_grid.flatten(), k_x_3.flatten())
axs[3].plot(s_grid.flatten(), k_x_4.flatten())
plt.tight_layout()
plt.xlabel('x')
plt.show()
plt.close()
#+end_src

We should also define $\eta_t$. Being independent in time, this is simply a multivariate Gaussian with some covariance matrix $\mat \Sigma_{\eta}$. In the R book examples, they define this covariance as an exponential function as follows;

#+begin_src python :session example :results none
sigma_eta = 0.1 * jnp.exp(-jnp.abs(s_grid - s_grid.T)/0.1)
#+end_src

and then simulation can be done through the ~jax.random.multivariate_normal~ (or otherwise, of course).

#+begin_src python :session example :results none
key = jax.random.PRNGKey(seed=3)

sim = rand.multivariate_normal(key, jnp.zeros(100), sigma_eta)

plt.figure(figsize=(5, 2))
plt.plot(s_grid, sim)
plt.show()
plt.close()
#+end_src

* Simulation of the Process

We can now consider how to actually simulate a realisation of such a system. In the R book, they do this with a for loop; this simply won't do. Instead, we define how the model should step forward with a function, which we can then iterate across.

#+begin_src python :session example :results none
import jax.lax as jl

def forward_step(Y,
                 M,
                 s_grid,
                 key):

    sigma_eta = 0.1 * jnp.exp(-jnp.abs(outer_op(s_grid, s_grid, lambda x,y:x-y))/0.1)
    
    eta = rand.multivariate_normal(key, jnp.zeros(100), sigma_eta)

    ds = 0.01 # for now :(
    
    Y_next = (M @ Y)*ds + eta # Riemman estimation of the integral

    return Y_next

Y_init = jnp.zeros(100)

M1 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap1))
M2 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap2))
M3 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap3))
M4 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap4))

def step(carry, key):
        nextstate = forward_step(carry, M1, s_grid, key)
        return(nextstate, nextstate)

T=200
key = jax.random.PRNGKey(seed=2)
keys = rand.split(key, T)
    
simul = jl.scan(step, Y_init, keys)[1]

plt.contourf(simul, cmap='viridis')
plt.colorbar(label='process')
plt.xlabel('Space')
plt.ylabel('Time')
plt.title('Hovm√∂ller plot of the Process')

plt.show()
plt.close()
#+end_src

* Testing Outer Operation

#+begin_src python :session example :results output
import utilities
import importlib
from utilities import outer_op
import jax
import jax.numpy as jnp

importlib.reload(utilities)

def minus(x,y):

    return (x - y)[0]

vec1 = jnp.array([[1],[2]])
vec2 = jnp.array([[4],[5]])

result = outer_op(vec1, vec2, minus)

print(result)

#+end_src

#+RESULTS:
: [[-3 -4]
:  [-2 -3]]

and the harder test

#+begin_src python :session example :results output
import IDEM
from IDEM import kernel
from utilities import create_grid

importlib.reload(IDEM)
importlib.reload(utilities)

s_grid_2D = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([0.01, 0.01]))

thetap = jnp.array([1, 1, 0, 0])

print(outer_op(s_grid_2D, s_grid_2D, lambda s, x: kernel(s,x,thetap)))
#+end_src

#+RESULTS:
: [[1.         0.9999     0.99960005 ... 0.14646044 0.14363214 0.14083028]
:  [0.9999     1.         0.9999     ... 0.14931458 0.14646044 0.14363214]
:  [0.99960005 0.9999     1.         ... 0.15219389 0.14931458 0.14646044]
:  ...
:  [0.14646044 0.14931458 0.15219389 ... 1.         0.9999     0.99960005]
:  [0.14363214 0.14646044 0.14931458 ... 0.9999     1.         0.9999    ]
:  [0.14083028 0.14363214 0.14646044 ... 0.99960005 0.9999     1.        ]]

* Simulating from a 2D eta

Now I want to get a simulation from $\eta_t(\vec s)$ for some time $t$. The ~IDE~ R package assumes the covariance is proportional to the identity, but we can also do this exponential covariance function (which is less noisy, with closer points having a strong positive corelation)

#+begin_src python :session example :results none
from jax.numpy.linalg import vector_norm
import jax.random as rand
import matplotlib.pyplot as plt

key = jax.random.PRNGKey(seed=42)

sigma_eta = 0.1 * jnp.exp(-outer_op(
    s_grid, s_grid, lambda s,x: vector_norm(s-x))
                          /0.1) # exponential covariance
eta = rand.multivariate_normal(key, jnp.zeros(s_grid.shape[0]), sigma_eta)

plt.figure(figsize=(100, 6))
# There are three broadly similar ways to plot this, haven't got contour to do it properly though
plt.scatter(s_grid.T[0], s_grid.T[1], c=eta, cmap='viridis', marker='s')
#plt.imshow(eta.reshape((100,100)))

plt.colorbar(label='eta')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Innovation')
plt.show()
plt.close()
#+end_src
