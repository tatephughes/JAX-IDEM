#+TITLE: Org Notebook for the IDEM project

:BOILERPLATE:
#+BIBLIOGRAPHY: ../../../bibliography.bib
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amsthm,amssymb,bm,bbm,tikz,tkz-graph, graphicx, subcaption, mathtools, algpseudocode}
#+LATEX_HEADER: \usepackage[cache=false]{minted}
#+LATEX_HEADER: \usetikzlibrary{arrows}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \usetikzlibrary{matrix}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb E}
#+LATEX_HEADER: \DeclareMathOperator{\prob}{\mathbb P}
#+LATEX_HEADER: \DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
#+LATEX_HEADER: \DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
#+LATEX_HEADER: \DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
#+LATEX_HEADER: \DeclareMathOperator{\normal}{\mathcal N}
#+LATEX_HEADER: \DeclareMathOperator{\invgam}{\mathcal{IG}}
#+LATEX_HEADER: \newcommand*{\mat}[1]{\bm{#1}}
#+LATEX_HEADER: \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
#+LATEX_HEADER: \renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
#+EXPORT_EXCLUDE_TAGS: noexport
:END:

* Import the Project

#+begin_src python :session example :results none
from IDEM import *
from utilities import create_grid
#+end_src

* Constructing the Process Grid and Kernel

Closely following [cite:@wikle2019spatio];

#+begin_src python :session example :results none
d = 2 # set the spatial dimension

ds = 0.01

# creates a grid 
s_grid = create_grid(jnp.array([[0,1]]),
                     jnp.array([0.01]))
N = len(s_grid)

nT = 201
t_grid = jnp.arange(0, nT-1)

# This flattening only works for 1D! I believe jnp.meshgrid doesn't support what will be at least 3D grids, so this may need rethinking. Thinking about it, I may be able to reuse some of the code from ~create_grid~ to get this to work properly.
st_grid = jnp.stack(jnp.meshgrid(s_grid.flatten(), t_grid), axis=-1)
#+end_src

We then define the transition kernel; as an example, this is using a Gaussian kernel
\begin{align*}
\kappa(\vec s, \vec x; \alpha, \vec\mu, \mat\Sigma) = \alpha\exp \left[ -(\vec s - \vec x - \vec \mu) \mat\Sigma^{-1} (\vec s - \vec x - \vec \mu) \right].
\end{align*}
Implemented as a JAX function,

#+begin_src python :session example :results none
from jax.numpy.linalg import solve
from utilities import outer_op

def kappa(s, x, thetap):
    
    scale = thetap[0]
    shift = thetap[1]
    shape = thetap[2]
    
    return scale * jnp.exp(-(s-x-shift)**2/shape).flatten()[0]

#+end_src

Lets first try to recreate the results in the original R book; here, the dimension is 1, and we consider the point $s=0.5$ for the following parameter options

#+begin_src python :session example :results none
thetap1 = (jnp.array(40),jnp.array([0]),jnp.array([[0.0002]]))
thetap2 = (jnp.array(5.75),jnp.array([0]),jnp.array([[0.01]]))
thetap3 = (jnp.array(8),jnp.array([0.1]),jnp.array([[0.005]]))
thetap4 = (jnp.array(8),jnp.array([-0.1]),jnp.array([[0.005]]))
#+end_src

Lets make a 1D grid with the ~create_grid~ function;

#+begin_src python :session example :results none
kappa1 = lambda x,y: kappa(x,y,thetap1)
kappa2 = lambda x,y: kappa(x,y,thetap2)
kappa3 = lambda x,y: kappa(x,y,thetap3)
kappa4 = lambda x,y: kappa(x,y,thetap4)

k_x_1 = outer_op(jnp.array([[0.5]]), s_grid, kappa1)
k_x_2 = outer_op(jnp.array([[0.5]]), s_grid, kappa2)
k_x_3 = outer_op(jnp.array([[0.5]]), s_grid, kappa3)
k_x_4 = outer_op(jnp.array([[0.5]]), s_grid, kappa4)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (5,2)  # Width, Height in inches

fig, axs = plt.subplots(4, 1, figsize=(5, 4))

axs[0].plot(s_grid.flatten(), k_x_1.flatten())
axs[0].set_title('Kernel Plots', fontsize=10)
axs[1].plot(s_grid.flatten(), k_x_2.flatten())
axs[2].plot(s_grid.flatten(), k_x_3.flatten())
axs[3].plot(s_grid.flatten(), k_x_4.flatten())
plt.tight_layout()
plt.xlabel('x')
plt.show()
plt.close()
#+end_src

We should also define $\eta_t$. Being independent in time, this is simply a multivariate Gaussian with some covariance matrix $\mat \Sigma_{\eta}$. In the R book examples, they define this covariance as an exponential function as follows;

#+begin_src python :session example :results none
sigma_eta = 0.1 * jnp.exp(-jnp.abs(s_grid - s_grid.T)/0.1)
#+end_src

and then simulation can be done through the ~jax.random.multivariate_normal~ (or otherwise, of course).

#+begin_src python :session example :results none
key = jax.random.PRNGKey(seed=3)

sim = rand.multivariate_normal(key, jnp.zeros(100), sigma_eta)

plt.figure(figsize=(5, 2))
plt.plot(s_grid, sim)
plt.show()
plt.close()
#+end_src

* Simulation of the Process

We can now consider how to actually simulate a realisation of such a system. In the R book, they do this with a for loop; this simply won't do. Instead, we define how the model should step forward with a function, which we can then iterate across.

#+begin_src python :session example :results none
import jax.lax as jl

def forward_step(Y,
                 M,
                 s_grid,
                 key):

    sigma_eta = 0.1 * jnp.exp(-jnp.abs(outer_op(s_grid, s_grid, lambda x,y:(x-y)[0]))/0.1)
    
    eta = rand.multivariate_normal(key, jnp.zeros(100), sigma_eta)

    ds = 0.01 # for now :(
    
    Y_next = (M @ Y)*ds + eta # Riemman estimation of the integral

    return Y_next

Y_init = jnp.zeros(100)

M1 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap1))
M2 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap2))
M3 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap3))
M4 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap4))

def step(carry, M, key):
        nextstate = forward_step(carry, M, s_grid, key)
        return(nextstate, nextstate)

T=200
key = jax.random.PRNGKey(seed=628)
keys = rand.split(key, 4*T)
    
simul1 = jl.scan(lambda carry, key: step(carry, M1, key), Y_init, keys[:T])[1]
simul2 = jl.scan(lambda carry, key: step(carry, M2, key), Y_init, keys[T:2*T])[1]
simul3 = jl.scan(lambda carry, key: step(carry, M3, key), Y_init, keys[2*T:3*T])[1]
simul4 = jl.scan(lambda carry, key: step(carry, M4, key), Y_init, keys[3*T:4*T])[1]

fig, axs = plt.subplots(4, 1, figsize=(5, 10))

axs[0].contourf(simul1, cmap='viridis')
axs[0].set_title('Hovm√∂ller plots', fontsize=10)
axs[1].contourf(simul2, cmap='viridis')
axs[2].contourf(simul3, cmap='viridis')
axs[3].contourf(simul4, cmap='viridis')
plt.tight_layout()
plt.xlabel('x')
plt.show()
plt.close()

plt.show()
plt.close()
#+end_src

* Testing Outer Operation

#+begin_src python :session example :results output
import utilities
import importlib
from utilities import outer_op
import jax
import jax.numpy as jnp

importlib.reload(utilities)

def minus(x,y):

    return (x - y)[0]

vec1 = jnp.array([[1],[2]])
vec2 = jnp.array([[4],[5]])

result = outer_op(vec1, vec2, minus)

print(result)

#+end_src

#+RESULTS:
: [[-3 -4]
:  [-2 -3]]

and the harder test

#+begin_src python :session example :results output
import IDEM
from IDEM import kernel
from utilities import create_grid

importlib.reload(IDEM)
importlib.reload(utilities)

s_grid_2D = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([0.01, 0.01]))

thetap = jnp.array([1, 1, 0, 0])

print(outer_op(s_grid_2D, s_grid_2D, lambda s, x: kernel(s,x,thetap)))
#+end_src

#+RESULTS:
: [[1.         0.9999     0.99960005 ... 0.14646044 0.14363214 0.14083028]
:  [0.9999     1.         0.9999     ... 0.14931458 0.14646044 0.14363214]
:  [0.99960005 0.9999     1.         ... 0.15219389 0.14931458 0.14646044]
:  ...
:  [0.14646044 0.14931458 0.15219389 ... 1.         0.9999     0.99960005]
:  [0.14363214 0.14646044 0.14931458 ... 0.9999     1.         0.9999    ]
:  [0.14083028 0.14363214 0.14646044 ... 0.99960005 0.9999     1.        ]]

* Simulating from a 2D eta

Now I want to get a simulation from $\eta_t(\vec s)$ for some time $t$. The ~IDE~ R package takes a bisquare expansion of it, and assumes the coefficients associated are proportional to the identity,, but we can also do this more directly exponential covariance function (which is less noisy, with closer points having a strong positive corelation)

#+begin_src python :session example :results none
from jax.numpy.linalg import vector_norm
import jax.random as rand
import matplotlib.pyplot as plt
from utilities import outer_op

key = jax.random.PRNGKey(seed=42)

s_grid = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([0.01, 0.01]))
                     
sigma_eta = 0.1 * jnp.exp(-outer_op(
    s_grid, s_grid, lambda s,x: vector_norm(s-x))
                          /0.1) # exponential covariance
eta = rand.multivariate_normal(key, jnp.zeros(s_grid.shape[0]), sigma_eta)

plt.figure(figsize=(100, 6))
# There are three broadly similar ways to plot this, haven't got contour to do it properly though
plt.scatter(s_grid.T[0], s_grid.T[1], c=eta, cmap='viridis', marker='s')
#plt.imshow(eta.reshape((100,100)))

plt.colorbar(label='eta')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Innovation')
plt.show()
plt.close()
#+end_src


* Spatial Basis Function Decomposition

We want to decompose the process (and potentially the innovation term) into a basis expansion, in this case using the bisquare functions (as is done in the R package). The Bisquare basis functions are
\begin{align*}
\phi_j(\vec u) = \left[2 - \frac{\Vert \vec u - \vec c_j \Vert^2}{w}
\right]^2 \mathrm{I}(\Vert \vec u - \vec c_j \Vert < w),
\end{align*}
where $\vec c_j$ are the 'knots', the points where we 'place' basis functions. 
Implemented in JAX,

#+begin_src python :session example :results none
from IDEM import *
from utilities import create_grid

def psi(s, knot, w=1):
    squarenorm = jnp.array([jnp.sum((s-knot)**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < w, 1, 0))[0]
#+end_src

Then, choosing some random 'observation' points and a grid of knots, we can use ~outer_op~ to find the basis function matrix

#+begin_src python :session example :results none
import matplotlib.pyplot as plt

key = jax.random.PRNGKey(0)
stations = jax.random.uniform(key, (30, 2))
knots = create_grid(jnp.array([[0,1],[0,1]]),
                    jnp.array([0.1,0.1]))

fig, ax = plt.subplots()
scatter = ax.scatter(stations[:,0], stations[:,1], c='green', s=10, marker='^')
scatter = ax.scatter(knots[:,0], knots[:,1], c='black', marker='+', linewidth=0.5, s=10)
ax.set_aspect('equal', adjustable='box')
plt.show()
plt.close()

PHI = outer_op(stations, knots, lambda s,k: psi(s, k, 1))
#+end_src

* 2D IDE simulation

#+begin_src python :session example :results none
# still using this basis expansion on a grid
def psi(s, knot, w=1):
    squarenorm = jnp.array([jnp.sum((s-knot)**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < w, 1, 0))[0]
knots = create_grid(jnp.array([[0,1],[0,1]]),
                    jnp.array([0.1,0.1]))

# initialise the process beta to be 0s 
beta0 = jnp.array(jnp.zeros(100))

# where data is actually 'read'
stations = jax.random.uniform(key, (30, 2))
#+end_src

Finding $\mat M$ and the covariance of $\eta$ relies on the equations
\begin{align}
\mat\Phi (\vec s) &= \int_{\mathcal D_s} \vec\psi(\vec s-\vec r) \vec\psi(\vec r)^{\intercal}d\vec r &\in \mathcal M_{I\times J}[\mathbb R]\\
\mat\Psi^{(Z)} &= \int_{\mathcal D_s} \vec \psi(\vec s)\vec \psi(\vec s)^{\intercal} d\vec s &\in \mathbb M_{I\times I}[\mathbb R]\\
\mat\Psi^{(\alpha)} &= \int_{\mathcal D_s} \vec \psi(\vec s)\vec \alpha^{\intercal} \mat \Phi(\vec s) d\vec s &\in \mathbb M_{I\times I}[\mathbb R],
\end{align}
where $\vec \alpha$ is the vector of coefficients of the kernel expanded by the basis functions. Then, $\mat M = (\mat\Psi^{(Z)})^{-1}\mat\Psi^{(\alpha)}$ and $C_{\vec \eta} = \sigma^2(\mat \Psi^{(Z)})^{-1}$, where $\sigma$ is the standard deviation of the driving term for the process $\vec Y$.

We evaluate these integrals by numerical integration;

(This is a lot of messing about with numerical integration, it isn't really working)

#+begin_src python :session example :results none
from jax.numpy.linalg import solve
import jax.random as rand
from jax.scipy.linalg import solve_triangular, cholesky

def psi(s, knot, w=1):
    squarenorm = jnp.array([jnp.sum((s-knot)**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < w, 1, 0))[0]

def psivec(s): return jl.map(lambda knot: psi(s,knot), knots).T


intgrid = create_grid(jnp.array([[0,1],[0,1]]), jnp.array([0.1,0.1]))
gridarea = 0.1*0.1

PHIS = lambda s:  jnp.sum(jl.map(lambda r: jnp.outer(psivec(s-r), psivec(r)), intgrid) , axis=0) * gridarea

PSIZ = jnp.sum(jl.map(lambda r: jnp.outer(psivec(r), psivec(r)), intgrid) , axis=0) * gridarea
# NOTE: PSIZ is symmetric, could that be taken advantage of?
# Also, since this is defined by the sum of outer products, I can likely avoid computing it
# directly altogether, since I technically only need it's cholesky decomposition!

alpha0 = jnp.ones( knots.shape[0])

PSIALPH = jnp.sum(jl.map(lambda r: jnp.outer(psivec(r), alpha0) @ PHIS(r), intgrid) , axis=0) * gridarea

sigmaeta = 0.1

#def beta_step(beta_t, key):
#
#    L = jax.scipy.linalg.cholesky(PSIZ, lower=True)
#    
#    eta = sigmaeta * solve_triangular(L,  rand.normal(shape = (PSIZ.shape[0], )))
#    
#    return solve(PSIZ, PSIALPHA @ beta_t) + eta
#+end_src

* Testing ~construct_basis~

I'm not quite sure how python/jax deals with lists of functions, so I need to do some testing.

#+begin_src python :session example :results none
def make_function(i):
    return lambda x: x**i

functions = [make_function(i) for  i in range(0,10)]
#+end_src

Great, that's about what I want (though not usable in a jit-loop).

Now I want to test the function;

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import importlib
import utilities
from utilities import place_basis
importlib.reload(utilities)

# just using the default values; creates a list of 90 basis funcs regularily places across the
# unit square across two resolutions
basis_params = place_basis(nres=1)

# we will try and evaluate all these functions at a point s randomly placed in the unit square
key = jax.random.PRNGKey(1)
s = jax.random.uniform(key, shape=(2,), minval=0.0, maxval=1.0)

def psi(s, params):
    squarenorm = jnp.array([jnp.sum((s-params[0:2])**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < params[2], 1, 0))[0]

#vectorise this function across params
vec_phi = jax.vmap(psi, in_axes=(None, 0))

# now we can compute \vec{\phi}(\vec s)
phis = vec_phi(s, basis_params)

# then we can recreate FRK's eval_basis (kinda) by further vectorisation
eval_basis = jax.vmap(vec_phi, in_axes=(0, None))
#+end_src

* ~construct_M~

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import importlib
import utilities
from utilities import place_basis
from utilities import create_grid
import IDEM
from IDEM import construct_M
importlib.reload(utilities)
importlib.reload(IDEM)

key = jax.random.PRNGKey(1)
keys = rand.split(key, 2)

process_basis = place_basis()
nbasis = process_basis.shape[0]

def psi(s, params):
    squarenorm = jnp.array([jnp.sum((s-params[0:2])**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < params[2], 1, 0))[0]

vec_phi = jax.vmap(psi, in_axes=(None, 0))

K_basis = (jnp.array(1),
           jnp.array(1),
           place_basis(nres=1),
           place_basis(nres=1))
k = (jnp.array(200),
     jnp.array(0.2),
     0.01*rand.normal(keys[0], shape=(K_basis[2].shape[0], )),
     0.01*rand.normal(keys[2], shape=(K_basis[3].shape[0], )))

def kernel(s,r):
    
    theta = (k[0], k[1],
             jnp.array([k[2] @ vec_phi(s, K_basis[2]),
                        k[3] @ vec_phi(s, K_basis[3])]))
    
    return theta[0] * jnp.exp(-(jnp.sum((r-s-theta[2])**2)) / theta[1])


# we can visualise this kernel by plotting it on a grid
griddelta = 0.01
s_grid = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([griddelta, griddelta]))

centre = jnp.array([0.5,0.5])
z = jax.vmap(kernel, in_axes=(None, 0))(centre, s_grid)

plt.figure(figsize=(3, 2))
plt.scatter(s_grid.T[0], s_grid.T[1], c=z, cmap='viridis', marker='s')

plt.colorbar(label='kernel strength')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Kernel')
plt.show()
plt.close()

# now we attempt to create the matrix M

M = construct_M(kernel, process_basis, s_grid, griddelta)
#+end_src

* Testing ~simIDE~

#+begin_src python :session example :results none :tangle simIDEM.py
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import importlib
import utilities
from utilities import *
import IDEM
from IDEM import *
importlib.reload(utilities)
importlib.reload(IDEM)

griddelta = 0.01
s_grid = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([griddelta, griddelta]))

k = (jnp.array(200),
     jnp.array(0.2),
     0.01*rand.normal(keys[0], shape=(K_basis[2].shape[0], )),
     0.01*rand.normal(keys[2], shape=(K_basis[3].shape[0], )))

process_vals = simIDEM(T=9, k=k)

fig, axes = plt.subplots(3, 3, figsize=(8, 5))

for i in range(9):
    ax = axes[i // 3, i % 3]
    scatter = ax.scatter(s_grid.T[0], s_grid.T[1], c=process_vals[i], cmap='viridis', marker='s')
    ax.set_title(f'T = {i+1}')
    fig.colorbar(scatter, ax=ax)

plt.tight_layout()
plt.show()
plt.close()

#+end_src
