#+TITLE: Org Notebook for the IDEM project

:BOILERPLATE:
#+BIBLIOGRAPHY: ../../../bibliography.bib
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amsthm,amssymb,bm,bbm,tikz,tkz-graph, graphicx, subcaption, mathtools, algpseudocode}
#+LATEX_HEADER: \usepackage[cache=false]{minted}
#+LATEX_HEADER: \usetikzlibrary{arrows}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \usetikzlibrary{matrix}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb E}
#+LATEX_HEADER: \DeclareMathOperator{\prob}{\mathbb P}
#+LATEX_HEADER: \DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
#+LATEX_HEADER: \DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
#+LATEX_HEADER: \DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
#+LATEX_HEADER: \DeclareMathOperator{\normal}{\mathcal N}
#+LATEX_HEADER: \DeclareMathOperator{\invgam}{\mathcal{IG}}
#+LATEX_HEADER: \newcommand*{\mat}[1]{\bm{#1}}
#+LATEX_HEADER: \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
#+LATEX_HEADER: \renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
#+EXPORT_EXCLUDE_TAGS: noexport
:END:

* Import the Project

#+begin_src python :session example :results none
from IDEM import *
from utilities import create_grid
#+end_src

* Constructing the Process Grid and Kernel

Closely following [cite:@wikle2019spatio];

#+begin_src python :session example :results none
d = 2 # set the spatial dimension

ds = 0.01

# creates a grid 
s_grid = create_grid(jnp.array([[0,1]]),
                     jnp.array([0.01]))
N = len(s_grid)

nT = 201
t_grid = jnp.arange(0, nT-1)

# This flattening only works for 1D! I believe jnp.meshgrid doesn't support what will be at least 3D grids, so this may need rethinking. Thinking about it, I may be able to reuse some of the code from ~create_grid~ to get this to work properly.
st_grid = jnp.stack(jnp.meshgrid(s_grid.flatten(), t_grid), axis=-1)
#+end_src

We then define the transition kernel; as an example, this is using a Gaussian kernel
\begin{align*}
\kappa(\vec s, \vec x; \alpha, \vec\mu, \mat\Sigma) = \alpha\exp \left[ -(\vec s - \vec x - \vec \mu) \mat\Sigma^{-1} (\vec s - \vec x - \vec \mu) \right].
\end{align*}
Implemented as a JAX function,

#+begin_src python :session example :results none
from jax.numpy.linalg import solve
from utilities import outer_op

def kappa(s, x, thetap):
    
    scale = thetap[0]
    shift = thetap[1]
    shape = thetap[2]
    
    return scale * jnp.exp(-(s-x-shift)**2/shape).flatten()[0]

#+end_src

Lets first try to recreate the results in the original R book; here, the dimension is 1, and we consider the point $s=0.5$ for the following parameter options

#+begin_src python :session example :results none
thetap1 = (jnp.array(40),jnp.array([0]),jnp.array([[0.0002]]))
thetap2 = (jnp.array(5.75),jnp.array([0]),jnp.array([[0.01]]))
thetap3 = (jnp.array(8),jnp.array([0.1]),jnp.array([[0.005]]))
thetap4 = (jnp.array(8),jnp.array([-0.1]),jnp.array([[0.005]]))
#+end_src

Lets make a 1D grid with the ~create_grid~ function;

#+begin_src python :session example :results none
kappa1 = lambda x,y: kappa(x,y,thetap1)
kappa2 = lambda x,y: kappa(x,y,thetap2)
kappa3 = lambda x,y: kappa(x,y,thetap3)
kappa4 = lambda x,y: kappa(x,y,thetap4)

k_x_1 = outer_op(jnp.array([[0.5]]), s_grid, kappa1)
k_x_2 = outer_op(jnp.array([[0.5]]), s_grid, kappa2)
k_x_3 = outer_op(jnp.array([[0.5]]), s_grid, kappa3)
k_x_4 = outer_op(jnp.array([[0.5]]), s_grid, kappa4)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (5,2)  # Width, Height in inches

fig, axs = plt.subplots(4, 1, figsize=(5, 4))

axs[0].plot(s_grid.flatten(), k_x_1.flatten())
axs[0].set_title('Kernel Plots', fontsize=10)
axs[1].plot(s_grid.flatten(), k_x_2.flatten())
axs[2].plot(s_grid.flatten(), k_x_3.flatten())
axs[3].plot(s_grid.flatten(), k_x_4.flatten())
plt.tight_layout()
plt.xlabel('x')
plt.show()
plt.close()
#+end_src

We should also define $\eta_t$. Being independent in time, this is simply a multivariate Gaussian with some covariance matrix $\mat \Sigma_{\eta}$. In the R book examples, they define this covariance as an exponential function as follows;

#+begin_src python :session example :results none
sigma_eta = 0.1 * jnp.exp(-jnp.abs(s_grid - s_grid.T)/0.1)
#+end_src

and then simulation can be done through the ~jax.random.multivariate_normal~ (or otherwise, of course).

#+begin_src python :session example :results none
key = jax.random.PRNGKey(seed=3)

sim = rand.multivariate_normal(key, jnp.zeros(100), sigma_eta)

plt.figure(figsize=(5, 2))
plt.plot(s_grid, sim)
plt.show()
plt.close()
#+end_src

* Simulation of the Process

We can now consider how to actually simulate a realisation of such a system. In the R book, they do this with a for loop; this simply won't do. Instead, we define how the model should step forward with a function, which we can then iterate across.

#+begin_src python :session example :results none
import jax.lax as jl

def forward_step(Y,
                 M,
                 s_grid,
                 key):

    sigma_eta = 0.1 * jnp.exp(-jnp.abs(outer_op(s_grid, s_grid, lambda x,y:(x-y)[0]))/0.1)
    
    eta = rand.multivariate_normal(key, jnp.zeros(100), sigma_eta)

    ds = 0.01 # for now :(
    
    Y_next = (M @ Y)*ds + eta # Riemman estimation of the integral

    return Y_next

Y_init = jnp.zeros(100)

M1 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap1))
M2 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap2))
M3 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap3))
M4 = outer_op(s_grid, s_grid, lambda x,y: kappa(x,y,thetap4))

def step(carry, M, key):
        nextstate = forward_step(carry, M, s_grid, key)
        return(nextstate, nextstate)

T=200
key = jax.random.PRNGKey(seed=628)
keys = rand.split(key, 4*T)
    
simul1 = jl.scan(lambda carry, key: step(carry, M1, key), Y_init, keys[:T])[1]
simul2 = jl.scan(lambda carry, key: step(carry, M2, key), Y_init, keys[T:2*T])[1]
simul3 = jl.scan(lambda carry, key: step(carry, M3, key), Y_init, keys[2*T:3*T])[1]
simul4 = jl.scan(lambda carry, key: step(carry, M4, key), Y_init, keys[3*T:4*T])[1]

fig, axs = plt.subplots(4, 1, figsize=(5, 10))

axs[0].contourf(simul1, cmap='viridis')
axs[0].set_title('Hovm√∂ller plots', fontsize=10)
axs[1].contourf(simul2, cmap='viridis')
axs[2].contourf(simul3, cmap='viridis')
axs[3].contourf(simul4, cmap='viridis')
plt.tight_layout()
plt.xlabel('x')
plt.show()
plt.close()

plt.show()
plt.close()
#+end_src

* Testing Outer Operation

#+begin_src python :session example :results output
import importlib
import jax
import jax.numpy as jnp

import sys
sys.path.append(os.path.abspath('../src'))
from jax_idem.utilities import *

importlib.reload(jax_idem.utilities)

def minus(x,y):

    return (x - y)

vec1 = jnp.array([1,2])
vec2 = jnp.array([4,5,4])

result = outer_op(vec1, vec2, minus)

print(result)

#+end_src

#+RESULTS:
: [[-3 -4 -3]
:  [-2 -3 -2]]

and the harder test

#+begin_src python :session example :results output
import IDEM
from IDEM import kernel
from utilities import create_grid

importlib.reload(IDEM)
importlib.reload(utilities)

s_grid_2D = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([0.01, 0.01]))

thetap = jnp.array([1, 1, 0, 0])

print(outer_op(s_grid_2D, s_grid_2D, lambda s, x: kernel(s,x,thetap)))
#+end_src

#+RESULTS:
: [[1.         0.9999     0.99960005 ... 0.14646044 0.14363214 0.14083028]
:  [0.9999     1.         0.9999     ... 0.14931458 0.14646044 0.14363214]
:  [0.99960005 0.9999     1.         ... 0.15219389 0.14931458 0.14646044]
:  ...
:  [0.14646044 0.14931458 0.15219389 ... 1.         0.9999     0.99960005]
:  [0.14363214 0.14646044 0.14931458 ... 0.9999     1.         0.9999    ]
:  [0.14083028 0.14363214 0.14646044 ... 0.99960005 0.9999     1.        ]]

* Simulating from a 2D eta

Now I want to get a simulation from $\eta_t(\vec s)$ for some time $t$. The ~IDE~ R package takes a bisquare expansion of it, and assumes the coefficients associated are proportional to the identity,, but we can also do this more directly exponential covariance function (which is less noisy, with closer points having a strong positive corelation)

#+begin_src python :session example :results none
from jax.numpy.linalg import vector_norm
import jax.random as rand
import matplotlib.pyplot as plt
from utilities import outer_op

key = jax.random.PRNGKey(seed=42)

s_grid = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([0.01, 0.01]))
                     
sigma_eta = 0.1 * jnp.exp(-outer_op(
    s_grid, s_grid, lambda s,x: vector_norm(s-x))
                          /0.1) # exponential covariance
eta = rand.multivariate_normal(key, jnp.zeros(s_grid.shape[0]), sigma_eta)

plt.figure(figsize=(100, 6))
# There are three broadly similar ways to plot this, haven't got contour to do it properly though
plt.scatter(s_grid.T[0], s_grid.T[1], c=eta, cmap='viridis', marker='s')
#plt.imshow(eta.reshape((100,100)))

plt.colorbar(label='eta')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Innovation')
plt.show()
plt.close()
#+end_src


* Spatial Basis Function Decomposition

We want to decompose the process (and potentially the innovation term) into a basis expansion, in this case using the bisquare functions (as is done in the R package). The Bisquare basis functions are
\begin{align*}
\phi_j(\vec u) = \left[2 - \frac{\Vert \vec u - \vec c_j \Vert^2}{w}
\right]^2 \mathrm{I}(\Vert \vec u - \vec c_j \Vert < w),
\end{align*}
where $\vec c_j$ are the 'knots', the points where we 'place' basis functions. 
Implemented in JAX,

#+begin_src python :session example :results none
from IDEM import *
from utilities import create_grid

def psi(s, knot, w=1):
    squarenorm = jnp.array([jnp.sum((s-knot)**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < w, 1, 0))[0]
#+end_src

Then, choosing some random 'observation' points and a grid of knots, we can use ~outer_op~ to find the basis function matrix

#+begin_src python :session example :results none
import matplotlib.pyplot as plt

key = jax.random.PRNGKey(0)
stations = jax.random.uniform(key, (30, 2))
knots = create_grid(jnp.array([[0,1],[0,1]]),
                    jnp.array([0.1,0.1]))

fig, ax = plt.subplots()
scatter = ax.scatter(stations[:,0], stations[:,1], c='green', s=10, marker='^')
scatter = ax.scatter(knots[:,0], knots[:,1], c='black', marker='+', linewidth=0.5, s=10)
ax.set_aspect('equal', adjustable='box')
plt.show()
plt.close()

PHI = outer_op(stations, knots, lambda s,k: psi(s, k, 1))
#+end_src

* 2D IDE simulation

#+begin_src python :session example :results none
# still using this basis expansion on a grid
def psi(s, knot, w=1):
    squarenorm = jnp.array([jnp.sum((s-knot)**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < w, 1, 0))[0]
knots = create_grid(jnp.array([[0,1],[0,1]]),
                    jnp.array([0.1,0.1]))

# initialise the process beta to be 0s 
beta0 = jnp.array(jnp.zeros(100))

# where data is actually 'read'
stations = jax.random.uniform(key, (30, 2))
#+end_src

Finding $\mat M$ and the covariance of $\eta$ relies on the equations
\begin{align}
\mat\Phi (\vec s) &= \int_{\mathcal D_s} \vec\psi(\vec s-\vec r) \vec\psi(\vec r)^{\intercal}d\vec r &\in \mathcal M_{I\times J}[\mathbb R]\\
\mat\Psi^{(Z)} &= \int_{\mathcal D_s} \vec \psi(\vec s)\vec \psi(\vec s)^{\intercal} d\vec s &\in \mathbb M_{I\times I}[\mathbb R]\\
\mat\Psi^{(\alpha)} &= \int_{\mathcal D_s} \vec \psi(\vec s)\vec \alpha^{\intercal} \mat \Phi(\vec s) d\vec s &\in \mathbb M_{I\times I}[\mathbb R],
\end{align}
where $\vec \alpha$ is the vector of coefficients of the kernel expanded by the basis functions. Then, $\mat M = (\mat\Psi^{(Z)})^{-1}\mat\Psi^{(\alpha)}$ and $C_{\vec \eta} = \sigma^2(\mat \Psi^{(Z)})^{-1}$, where $\sigma$ is the standard deviation of the driving term for the process $\vec Y$.

We evaluate these integrals by numerical integration;

(This is a lot of messing about with numerical integration, it isn't really working)

#+begin_src python :session example :results none
from jax.numpy.linalg import solve
import jax.random as rand
from jax.scipy.linalg import solve_triangular, cholesky

def psi(s, knot, w=1):
    squarenorm = jnp.array([jnp.sum((s-knot)**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < w, 1, 0))[0]

def psivec(s): return jl.map(lambda knot: psi(s,knot), knots).T


intgrid = create_grid(jnp.array([[0,1],[0,1]]), jnp.array([0.1,0.1]))
gridarea = 0.1*0.1

PHIS = lambda s:  jnp.sum(jl.map(lambda r: jnp.outer(psivec(s-r), psivec(r)), intgrid) , axis=0) * gridarea

PSIZ = jnp.sum(jl.map(lambda r: jnp.outer(psivec(r), psivec(r)), intgrid) , axis=0) * gridarea
# NOTE: PSIZ is symmetric, could that be taken advantage of?
# Also, since this is defined by the sum of outer products, I can likely avoid computing it
# directly altogether, since I technically only need it's cholesky decomposition!

alpha0 = jnp.ones( knots.shape[0])

PSIALPH = jnp.sum(jl.map(lambda r: jnp.outer(psivec(r), alpha0) @ PHIS(r), intgrid) , axis=0) * gridarea

sigmaeta = 0.1

#def beta_step(beta_t, key):
#
#    L = jax.scipy.linalg.cholesky(PSIZ, lower=True)
#    
#    eta = sigmaeta * solve_triangular(L,  rand.normal(shape = (PSIZ.shape[0], )))
#    
#    return solve(PSIZ, PSIALPHA @ beta_t) + eta
#+end_src

* Testing ~construct_basis~

I'm not quite sure how python/jax deals with lists of functions, so I need to do some testing.

#+begin_src python :session example :results none
def make_function(i):
    return lambda x: x**i

functions = [make_function(i) for  i in range(0,10)]
#+end_src

Great, that's about what I want (though not usable in a jit-loop).

Now I want to test the function;

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import importlib
import utilities
from utilities import place_basis
importlib.reload(utilities)

# just using the default values; creates a list of 90 basis funcs regularily places across the
# unit square across two resolutions
basis_params = place_basis(nres=1)

# we will try and evaluate all these functions at a point s randomly placed in the unit square
key = jax.random.PRNGKey(1)
s = jax.random.uniform(key, shape=(2,), minval=0.0, maxval=1.0)

def psi(s, params):
    squarenorm = jnp.array([jnp.sum((s-params[0:2])**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < params[2], 1, 0))[0]

#vectorise this function across params
vec_phi = jax.vmap(psi, in_axes=(None, 0))

# now we can compute \vec{\phi}(\vec s)
phis = vec_phi(s, basis_params)

# then we can recreate FRK's eval_basis (kinda) by further vectorisation
eval_basis = jax.vmap(vec_phi, in_axes=(0, None))
#+end_src

* ~construct_M~

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))

import importlib
import utilities
from utilities import *
import IDEM
from IDEM import *
importlib.reload(utilities)
importlib.reload(IDEM)

key = jax.random.PRNGKey(1)
keys = rand.split(key, 2)

process_basis = place_basis()
nbasis = process_basis.shape[0]

def psi(s, params):
    squarenorm = jnp.array([jnp.sum((s-params[0:2])**2)])
    return ((2 - squarenorm)**2 * jnp.where(squarenorm < params[2], 1, 0))[0]

vec_phi = jax.vmap(psi, in_axes=(None, 0))

K_basis = (jnp.array(1),
           jnp.array(1),
           place_basis(nres=1),
           place_basis(nres=1))
k = (jnp.array(200),
     jnp.array(0.2),
     0.01*rand.normal(keys[0], shape=(K_basis[2].shape[0], )),
     0.01*rand.normal(keys[2], shape=(K_basis[3].shape[0], )))

def kernel(s,r):
    
    theta = (k[0], k[1],
             jnp.array([k[2] @ vec_phi(s, K_basis[2]),
                        k[3] @ vec_phi(s, K_basis[3])]))
    
    return theta[0] * jnp.exp(-(jnp.sum((r-s-theta[2])**2)) / theta[1])


# we can visualise this kernel by plotting it on a grid
griddelta = 0.01
s_grid = create_grid(jnp.array([[0,1],[0,1]]),
                     jnp.array([griddelta, griddelta]))

centre = jnp.array([0.5,0.5])
z = jax.vmap(kernel, in_axes=(None, 0))(centre, s_grid)

plt.figure(figsize=(3, 2))
plt.scatter(s_grid.T[0], s_grid.T[1], c=z, cmap='viridis', marker='s')

plt.colorbar(label='kernel strength')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Kernel')
plt.show()
plt.close()

# now we attempt to create the matrix M

M = construct_M(kernel, process_basis, s_grid, griddelta)
#+end_src

* Testing ~simIDE~

#+begin_src python :session example :results none :tangle simIDEM.py
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))


import importlib
import utilities
from utilities import *
import IDEM
from IDEM import *
importlib.reload(utilities)
importlib.reload(IDEM)

key = jax.random.PRNGKey(1)
keys = rand.split(key, 2)

ngrids=jnp.array([41,41])
s_grid, griddeltas = create_grid(jnp.array([[0,1],[0,1]]),
                                 ngrids)

K_basis = (
    place_basis(nres=1, basis_fun=lambda s, r: 1),
    place_basis(nres=1, basis_fun=lambda s, r: 1),
    place_basis(nres=1),
    place_basis(nres=1),
)
k = (
    jnp.array(200),
    jnp.array(0.002),
    0.01 * rand.normal(keys[0], shape=(K_basis[2].shape[0],)),
    0.01 * rand.normal(keys[2], shape=(K_basis[3].shape[0],)),
)

process_vals = simIDEM(T=9, k=k, K_basis=K_basis, ngrids=ngrids)

vmin = jnp.min(process_vals)
vmax = jnp.max(process_vals)

fig, axes = plt.subplots(3, 3, figsize=(8, 5))

for i in range(9):
    ax = axes[i // 3, i % 3]
    scatter = ax.scatter(s_grid.T[0], s_grid.T[1], c=process_vals[i], cmap='viridis', marker='s', vmin=vmin, vmax=vmax)
    ax.set_title(f'T = {i+1}')
    ax.set_title(f'T = {i+1}', fontsize=5)  # Set title font size
    ax.tick_params(axis='both', which='major', labelsize=4)  # Set tick labels font size
    fig.colorbar(scatter, ax=ax)
    
plt.tight_layout()
plt.show()
plt.close()

#+end_src

#+begin_src python :session example :results none
def psi(s, params):
    squarenorm = jnp.array([jnp.sum((s - params[0:2]) ** 2)])
    return ((1 - squarenorm / (params[2]**2)) ** 2 * jnp.where(squarenorm < params[2], 1, 0))[0]

vec_phi = jax.vmap(psi, in_axes=(None, 0))
#+end_src

** Adjusting place_basis

#+begin_src python :session example :results none
def bisquare(s, params):
    squarenorm = jnp.array([jnp.sum((s - params[0:2]) ** 2)])
    return (
        (1 - squarenorm / (params[2] ** 2)) ** 2
        ,* jnp.where(squarenorm < params[2], 1, 0)
    )[0]

def place_basis(
    data=jnp.array([[0, 0], [1, 1]]),
    nres=2,
    aperture=1.25,
    min_knot_num=3,
    basis_fun=bisquare,
):
    """
            Distributes knots and scales for basis functions over a number of resolutions,
            similar to auto_basis from the R package FRK.
            This function must be run outside of a jit loop, since it involves varying the
            length of arrays.

    Parameters:
      data: Arraylike[ArrayLike[Double]]; array of 2D points defining the space on which to put the basis functions
      nres: Int; The number of resolutions at which to place basis functions
      aperture: Double; Scaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)
      min_knot_num: Int; The number of basis functions to place in each dimension at the coursest resolution
      basis_fun: (ArrayLike[Double], ArrayLike[Double]) -> Double; the basis functions being used. The basis function's second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.

    Returns:
      A tuple of two functions and an integer, the first evaluating the basis functions at a point, and the second evaluating the basis functions on an array of points.
    """

    xmin = jnp.min(data[:, 0])
    xmax = jnp.max(data[:, 0])
    ymin = jnp.min(data[:, 1])
    ymax = jnp.max(data[:, 1])

    asp_ratio = (ymax - ymin) / (xmax - xmin)

    if asp_ratio < 1:
        ny = min_knot_num
        nx = jnp.round(ny / asp_ratio).astype(int)
    else:
        nx = min_knot_num
        ny = jnp.round(asp_ratio * nx).astype(int)

    def basis_at_res(res):
        bounds = jnp.array([[xmin, xmax], [ymin, ymax]])
        ngrids = jnp.array([nx, ny]) * 3**res

        knots, deltas = create_grid(bounds, ngrids)
        w = jnp.min(deltas) * aperture

        return jnp.hstack([knots, jnp.full((knots.shape[0], 1), w)])

    basis_vfun = jax.vmap(basis_fun, in_axes=(None, 0))
    eval_basis = jax.vmap(jax.vmap(basis_fun, in_axes=(None, 0)), in_axes=(0, None))

    params = jnp.vstack([basis_at_res(res) for res in range(nres)])
    nbasis = params.shape[0]

    print("params shape is", params.shape)

    return (
        lambda s: basis_vfun(s, params),
        lambda s_array: eval_basis(s_array, params),
        nbasis,
    )

#+end_src

** testing a unit basis

#+begin_src python :session example :results none
const_basis = place_basis(nres=1, min_knot_num=1, basis_fun = lambda s, params: 1)
#+end_src


* Even more testing

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))


import importlib
import utilities
from utilities import *
import IDEM
from IDEM import *
importlib.reload(utilities)
importlib.reload(IDEM)

key = jax.random.PRNGKey(1)
keys = rand.split(key, 3)

ngrids = jnp.array([41, 41])
s_grid, griddeltas = create_grid(jnp.array([[0, 1], [0, 1]]), ngrids)

K_basis = (
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1),
        place_basis(nres=1),
)
k = (
        jnp.array([200]),
        jnp.array([0.002]),
        0.01 * rand.normal(keys[0], shape=(K_basis[2][2],)),
        0.01 * rand.normal(keys[2], shape=(K_basis[3][2],)),
)

def kernel(s, r):
        theta = (
                k[0] @ K_basis[0][0](s),
                k[1] @ K_basis[1][0](s),
                jnp.array(
                        [
                                k[2] @ K_basis[2][0](s),
                                k[3] @ K_basis[3][0](s),
                        ]
                ),
        )

        return theta[0] * jnp.exp(-(jnp.sum((r - s - theta[2]) ** 2)) / theta[1])

'''
process_vals = simIDEM(T=9, k=k, K_basis=K_basis, ngrids=ngrids)

fig, axes = plt.subplots(3, 3, figsize=(8, 5))

for i in range(9):
    ax = axes[i // 3, i % 3]
    scatter = ax.scatter(
        s_grid.T[0], s_grid.T[1], c=process_vals[i], cmap="viridis", marker="s"
        )
    ax.set_title(f"T = {i+1}")
    fig.colorbar(scatter, ax=ax)
    
plt.tight_layout()
plt.show()'''
#+end_src


* Recreating AZM's code beat-for-beat
#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))


import importlib
import utilities
from utilities import *
import IDEM
from IDEM import *
#importlib.reload(utilities)
#importlib.reload(IDEM)

key = jax.random.PRNGKey(1)
keys = rand.split(key, 3)
#+end_src

#+begin_src python :session example :results none
key = jax.random.PRNGKey(5)
keys = rand.split(key, 3)

T = 9

nobs = 50
ngrids = jnp.array([41, 41])
nints = jnp.array([100, 100])
process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), ngrids)
obs_locs = rand.uniform(keys[3], shape=(T, nobs, 2))
int_grid = create_grid(jnp.array([[0, 1], [0, 1]]), nints)

process_basis = place_basis()
nbasis = process_basis.nbasis

k_spat_inv = 0

if k_spat_inv == 1:
    K_basis = (
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        )
    k = (
        jnp.array([150]),
        jnp.array([0.002]),
        jnp.array([-0.1]),
        jnp.array([0.1]),
        )
    alpha0 = jnp.zeros(nbasis).at[jnp.array([64])].set(1)
else:
    K_basis = (
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1),
        place_basis(nres=1),
        )
    k = (
        jnp.array([200]),
        jnp.array([0.002]),
        0.1 * rand.normal(keys[0], shape=(K_basis[2].nbasis,)),
        0.1 * rand.normal(keys[1], shape=(K_basis[3].nbasis,)),
        )
    alpha0 = (
        jnp.zeros(nbasis)
        .at[jnp.array([77, 66, 19, 1, 34, 75, 31, 35, 46, 88])]
        .set(1)
        )
    
@jax.jit
def kernel(s, r):
    """Generates the kernel function from the kernel basis and basis coefficients"""
    theta = (
        k[0] @ K_basis[0].vfun(s),
        k[1] @ K_basis[1].vfun(s),
        jnp.array(
            [
                k[2] @ K_basis[2].vfun(s),
                k[3] @ K_basis[3].vfun(s),
            ]
        ),
    )
    
    return theta[0] * jnp.exp(-(jnp.sum((r - s - theta[2]) ** 2)) / theta[1])

M = construct_M(kernel, process_basis, int_grid)

PHI_proc = process_basis.mfun(process_grid.coords)
PHI_obs = jl.map(process_basis.mfun, obs_locs)

    # Other Coefficients
sigma2_eta = 0.01**2
sigma2_eps = 0.01**2
Q_eta = jnp.eye(nbasis) / sigma2_eta
Q_eps = jnp.eye(nobs * T) / sigma2_eps

#+end_src

#+begin_src python :session example :results none
keys = rand.split(key, 5)

nbasis = PHI_proc.shape[1]

nobs = obs_locs.shape[1]

@jax.jit
def step(carry, key):
    nextstate = M @ carry + jnp.sqrt(sigma2_eta) * rand.normal(key, shape=(nbasis,))
    return (nextstate, nextstate)

alpha_keys = rand.split(keys[3], T)

alpha = jl.scan(step, alpha0, alpha_keys)[1]

@jax.jit
def get_process(alpha):
    return PHI_proc @ alpha

vget_process = jax.vmap(get_process)

process_vals = vget_process(alpha)

# X_proc = jnp.column_stack([jnp.ones(s_grid.shape[0]), s_grid])
beta = jnp.array([0.2, 0.2, 0.2])

X_obs = jl.map(
    lambda arr: jnp.column_stack([jnp.ones(arr.shape[0]), arr]), obs_locs
    )

@jax.jit
def get_obs(X_obs_1, PHI_obs_1, alpha_1):
    return (
        X_obs_1 @ beta
        + PHI_obs_1 @ alpha_1
        + jnp.sqrt(sigma2_eps) * rand.normal(key, shape=(nobs,))
        )

obs_vals = jax.vmap(get_obs)(X_obs, PHI_obs, alpha)

t_obs_locs = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, obs_locs[i].shape[0]), obs_locs[i]]
                 ),
        jnp.arange(T)),
                        )
    )

#+end_src


* 6th October

** imports

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))


import importlib
import utilities
import IDEM

jax.config.update('jax_enable_x64', False)
#+end_src

** reloading and keys

#+begin_src python :session example :results none

import utilities
import IDEM

importlib.reload(utilities)
importlib.reload(IDEM)

from utilities import *
from IDEM import *

key = jax.random.PRNGKey(1)
keys = rand.split(key, 3)

#+end_src

** Repeated Simulations

#+begin_src python :session example :results none

import time

start_time = time.time()

T = 9

nobs = 40
ngrids = jnp.array([41, 41])
nints = jnp.array([100, 100])
process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), ngrids)
obs_locs = rand.uniform(keys[3], shape=(T, nobs, 2))
int_grid = create_grid(jnp.array([[0, 1], [0, 1]]), nints)

process_basis = place_basis()
nbasis = process_basis.nbasis

k_spat_inv = 0

if k_spat_inv == 1:
    K_basis = (
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        )
    k = (
        jnp.array([150]),
        jnp.array([0.002]),
        jnp.array([-0.1]),
        jnp.array([0.1]),
        )
    alpha0 = jnp.zeros(nbasis).at[jnp.array([64])].set(1)
else:
    K_basis = (
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
        place_basis(nres=1),
        place_basis(nres=1),
        )
    k = (
        jnp.array([200]),
        jnp.array([0.002]),
        jnp.array([0.043910943, 0.162829078, 0.081960908, 0.104412083, 0.002953001, 0.043431696, 0.046788517, 0.026617711, -0.121628583]),
        jnp.array([0.081031263, -0.109359565,  0.139565001, -0.059344792, -0.020561265, 0.007228824, -0.123947415, -0.030153943, 0.035854375]),
        #0.1 * rand.normal(keys[0], shape=(K_basis[2].nbasis,)),
        #0.1 * rand.normal(keys[1], shape=(K_basis[3].nbasis,)),
        )
    alpha0 = (
        jnp.zeros(nbasis)
        .at[jnp.array([76, 65, 18, 0, 33, 74, 30, 34, 45, 87])]
        .set(1)
        )
    #alpha0 = (
    #    jnp.zeros(nbasis)
    #    .at[jnp.array([4])]
    #    .set(1)
    #    )
    
@jax.jit
def kernel(s, r):
    """Generates the kernel function from the kernel basis and basis coefficients"""
    theta = (
        k[0] @ K_basis[0].vfun(s),
        k[1] @ K_basis[1].vfun(s),
        jnp.array(
        [
    k[2] @ K_basis[2].vfun(s),
    k[3] @ K_basis[3].vfun(s),
    ]
             ),
        )

    return theta[0] * jnp.exp(-(jnp.sum((r - s - theta[2]) ** 2)) / theta[1])

M = construct_M(kernel, process_basis, int_grid)

PHI_proc = process_basis.mfun(process_grid.coords)
PHI_obs = jl.map(process_basis.mfun, obs_locs)

# Other Coefficients
sigma2_eta = 0.01**2
sigma2_eps = 0.01**2
Q_eta = jnp.eye(nbasis) / sigma2_eta
Q_eps = jnp.eye(nobs * T) / sigma2_eps

nreps = 10000
sim_keys = rand.split(keys[2], nreps)

process_vals_sample, obs_vals_sample = jl.map(
    lambda key: simIDEM(
        key=key,
        T=T,
        M=M,
        PHI_proc=PHI_proc,
        PHI_obs=PHI_obs,
        alpha0=alpha0,
        obs_locs=obs_locs,
        process_grid=process_grid,
        int_grid=int_grid,
        ),
        sim_keys,
    )



t_obs_locs = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, obs_locs[i].shape[0]), obs_locs[i]]
           ),
        jnp.arange(T),
            )
    )

duration = time.time()-start_time

print(f"The time elapsed was {duration} seconds")
#+end_src

** making the process vals into long format

#+begin_src python :session example :results none

process_grid_t = jnp.tile(process_grid.coords, (T,1,1))

t_process_locs = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, process_grid_t[i].shape[0]), process_grid_t[i]]
           ),
        jnp.arange(T),
             )
    )

def vals_to_long(process_vals):
    return jnp.column_stack([t_process_locs, jnp.concatenate(process_vals)])
    

# keep in mind, for poarallelisation, vmap is usually recommended
process_data_sample = jl.map(vals_to_long, process_vals_sample)

means = jnp.mean(process_data_sample[:, :, 3], axis=0)
variances = jnp.var(process_data_sample[:, :, 3], axis=0)

# now i just need to write a function to plot these long-format data

pdata1 = process_data_sample[3]
plong = ST_Data_Long(x = pdata1[:,1], y = pdata1[:,2], t=pdata1[:,0], z = pdata1[:,3])
plot_st_long(plong)

#+end_src

great, it working.

** plotting the mean and variances

#+begin_src python :session example :results none
means = jnp.mean(process_data_sample[:, :, 3], axis=0)
variances = jnp.var(process_data_sample[:, :, 3], axis=0)

pdata1 = process_data_sample[0]
mean_data = ST_Data_Long(x = pdata1[:,1], y = pdata1[:,2], t=pdata1[:,0], z = means)
var_data =  ST_Data_Long(x = pdata1[:,1], y = pdata1[:,2], t=pdata1[:,0], z = variances)

plot_st_long(mean_data)
plot_st_long(var_data)
#+end_src

** wait what is the to long piece of code doing?

#+begin_src python :session example :results none

key = jax.random.PRNGKey(1)
keys = rand.split(key, 3)

model = gen_example_idem(keys[0], k_spat_inv=False)

obs_locs_wide = rand.uniform(keys[1], shape=(T, nobs, 2))
obs_locs_t = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, obs_locs_wide[i].shape[0]), obs_locs_wide[i]]
           ),
        jnp.arange(T),
            )
    ) # add a t column

times = jnp.unique(obs_locs_t[:,0])

X_obs = jnp.column_stack([jnp.ones(obs_locs_t.shape[0]), obs_locs_t[:,-2:]])

process_basis = place_basis()
eval_basis = process_basis.mfun

obs_locs_tree = jax.tree.map(lambda t: obs_locs_t[jnp.where(obs_locs_t[:, 0] == t)][:,1:], list(times))
PHI_tree = jax.tree.map(eval_basis, obs_locs_tree)

# really should consider exploring a sparse matrix solution!
PHI_obs = jax.scipy.linalg.block_diag(*PHI_tree)


int_grid = create_grid(jnp.array([[0, 1], [0, 1]]), nints)
process_vals, obs_vals = model.simulate(keys[2],obs_locs, int_grid)

# Create ST_Data_Long object
process_grids = jnp.tile(model.process_grid.coords, (T, 1, 1)) # repeat process grid t times

process_locs_t = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, process_grids[i].shape[0]), process_grids[i]]
           ),
        jnp.arange(T),
            )
    ) # add a t column

pdata = jnp.column_stack([process_locs_t, jnp.concatenate(process_vals)]) # add the values

process_data = ST_Data_Long(
    x=pdata[:, 1], y=pdata[:, 2], t=pdata[:, 0], z=pdata[:, 3]
)
#+end_src

#+begin_src python :session example :results none
(
    M,
    PHI_proc,
    beta,
    sigma2_eta,
    sigma2_eps,
    alpha0,
    process_grid,
    int_grid,
) = model.get_sim_params()

T = 9

obs_locs_wide = rand.uniform(keys[1], shape=(T, nobs, 2))
obs_locs_t = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, obs_locs_wide[i].shape[0]), obs_locs_wide[i]]
           ),
        jnp.arange(T),
            )
    ) # add a t column

times = jnp.unique(obs_locs_t[:,0])

X_obs = jnp.column_stack([jnp.ones(obs_locs_t.shape[0]), obs_locs_t[:,-2:]])

obs_locs_tree = jax.tree.map(lambda t: obs_locs_t[jnp.where(obs_locs_t[:, 0] == t)][:,1:], list(times))
PHI_tree = jax.tree.map(model.process_basis.mfun, obs_locs_tree)

# really should consider exploring a sparse matrix solution!
PHI_obs = jax.scipy.linalg.block_diag(*PHI_tree)

process_vals, obs_vals = simIDEM(
    key=key,
    T=T,
    M=M,
    PHI_proc=PHI_proc,
    PHI_obs=PHI_obs,
    alpha0=alpha0,
    obs_locs=obs_locs_t,
    process_grid=process_grid,
    int_grid=int_grid,
    )

# Create ST_Data_Long object
process_grids = jnp.tile(process_grid.coords, (T, 1, 1))

t_process_locs = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, process_grids[i].shape[0]), process_grids[i]]
           ),
        jnp.arange(T),
                            )
    )

pdata = jnp.column_stack([t_process_locs, jnp.concatenate(process_vals)])

process_data = ST_Data_Long(
    x=pdata[:, 1], y=pdata[:, 2], t=pdata[:, 0], z=pdata[:, 3]
    )

obs_data = ST_Data_Long(
    x=obs_locs_t[:,1], y=obs_locs_t[:,2], t=obs_locs_t[:,0], z=obs_vals
    )
#+end_src


* Gifs maybe

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))


import importlib

import utilities
import IDEM

importlib.reload(utilities)
importlib.reload(IDEM)

from utilities import *
from IDEM import *

key = jax.random.PRNGKey(1)
keys = rand.split(key, 3)

model = gen_example_idem(keys[0], k_spat_inv=False)

# Simulation
T = 9

process_data, obs_data = model.simulate(key)

# plot the object
#gif_st_long(process_data)
#+end_src

#+begin_src python :session example :results none
t=1

data_array = jnp.column_stack(process_data)
time_data = data_array[data_array[:, 2] == t]
x = time_data[:, 0]
y = time_data[:, 1]
values = time_data[:, 3]
valmat = values.reshape(41, 41)

fig, ax = plt.subplots()

sns.heatmap(valmat)

ax.set_title(f"Time: {t}")
ax.set_xlabel(data.x)
ax.set_ylabel(data.y)

plt.close(fig)

image_path = f"{t}.png"
fig.savefig(image_path)

#+end_src

#+begin_src python :session example :results none
from jax.scipy.optimize import minimize
from jax import grad, jit
grid = create_grid(jnp.array([[0.1, 0.9], [0.1, 0.9]]), jnp.array([10,10])).coords

@jax.jit
def kernel(s, r):
    """Generates the kernel function from the kernel basis and basis coefficients"""
    theta = (
        k[0] @ K_basis[0].vfun(s),
        k[1] @ K_basis[1].vfun(s),
        jnp.array(
        [
    k[2] @ K_basis[2].vfun(s),
    k[3] @ K_basis[3].vfun(s),
    ]
             ),
        )

    return theta[0] * jnp.exp(-(jnp.sum((r - s - theta[2]) ** 2)) / theta[1])

def offset(s):
    return jnp.array(
        [
         k[2] @ K_basis[2].vfun(s),
         k[3] @ K_basis[3].vfun(s),
        ]
    )

vecoffset = jax.vmap(offset)

offsets = vecoffset(grid)

fig, ax = plt.subplots()
q = ax.quiver(grid[:,0], grid[:,1], offsets[:,0], offsets[:,1])
ax.quiverkey(q, X=0.3, Y=1.1, U=10,
             label='Quiver key, length = 10', labelpos='E')

plt.show()
#+end_src


#+begin_src python :session example :results none
m_0 = model.m_0
Sigma_0 = model.Sigma_0
nbasis = m_0.shape[0] 

obs_locs = jnp.column_stack([obs_data.t,obs_data.x,obs_data.y])
obs_vals = obs_data.z
times = jnp.unique(obs_locs[:, 0])
obs_locs_tree = jax.tree.map(
        lambda t: obs_locs[jnp.where(obs_locs[:, 0] == t)][:, 1:], list(times)
)
PHI_tree = jax.tree.map(model.process_basis.mfun, obs_locs_tree)
X_obs = jnp.column_stack([jnp.ones(obs_locs.shape[0]), obs_locs[:, -2:]])
nobs = jnp.array([PHI_tree[int(i)].shape[0] for i in times])
T = len(times)
Sigma_eta = model.sigma2_eta * jnp.eye(nbasis)
Sigma_eps_tree = jax.tree.map(lambda mat: model.sigma2_eps * mat, jax.tree.map(lambda n: jnp.eye(n), tuple(nobs)))
beta = model.beta


@partial(jax.jit, static_argnames=["i"])
def step(carry, i):
        m_tt = carry[0]
        P_tt = carry[1]
        z_t = jnp.where(obs_data.z, size=nobs[i])[0]
        PHI = PHI_tree[i]
        X = X_obs[i * nobs[i] : (i + 1) * nobs[i]]
        Sigma_eps = Sigma_eps_tree[i]
        
        # predict
        m_pred = M @ m_tt
        P_pred = M @ P_tt @ M.T + Sigma_eta
        
        # Update
        eps_t = z_t - PHI @ m_pred - X @ beta
        K_t = P_pred @ PHI.T @ solve(PHI @ P_pred @ PHI.T + Sigma_eps, jnp.eye(nobs[i]))
        
        m_up = m_pred + K_t @ eps_t
        P_up = (jnp.eye(r) - K_t @ PHI) @ P_pred
        
        Sigma_t = M @ P_pred @ M.T + Sigma_eps
        ll = jnp.linalg.slogdet(Sigma_t) + eps_t.T @ solve(Sigma_t, eps_t)
        return (carry + ll, (m_up, P_up))
#+end_src

* The Kalman Filter Testing

** Create a model and simulate data with fixed locations

#+begin_src python :session example :results none
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand

import matplotlib.pyplot as plt

import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))


import importlib

import utilities
import IDEM

importlib.reload(utilities)
importlib.reload(IDEM)

from utilities import *
from IDEM import *

#+end_src
#+begin_src python :session example :results none

key = jax.random.PRNGKey(12)
keys = rand.split(key, 3)

process_basis = place_basis(nres=2, min_knot_num=5)
nbasis = process_basis.nbasis

m_0 = jnp.zeros(nbasis).at[16].set(1)
P_0 = 0.001 * jnp.eye(nbasis)

truemodel = gen_example_idem(keys[0], k_spat_inv=True, process_basis=process_basis, m_0=m_0, P_0=P_0)

# Simulation
T = 10

process_data, obs_data = truemodel.simulate(nobs=50, T=T+1, key=key)

#data_array = jnp.column_stack((data.x, data.y, data.t, data.z))

obs_data_wide = ST_towide(obs_data)
obs_locs = jnp.column_stack((obs_data_wide.x, obs_data_wide.y))
nobs = obs_locs.shape[0]
X_obs = jnp.column_stack([jnp.ones(nobs), obs_locs])

#ll, ms, Ps, mpreds, Ppreds, Ks = model.filter(obs_data_wide, X_obs)

K_basis = truemodel.K_basis
k = (jnp.array([150]),jnp.array([0.002]),jnp.array([0]),jnp.array([0]),)

newmodel = gen_example_idem(keys[0],
                            k_spat_inv=True, process_basis=place_basis(nres=1, min_knot_num=5),
                            K_basis = K_basis,
                            k=truemodel.k,
                            m_0=jnp.zeros(25).at[16].set(1),
                            P_0 = 0.001*jnp.eye(25),)

unfit_process_data, unif_obs_data = newmodel.simulate(nobs=50, T=T+1, key=key)

gif_st_grid(unfit_process_data, output_file="unfit_process.gif")

ll, ms, Ps, mpreds, Ppreds, Ks = newmodel.filter(obs_data_wide, X_obs)

PHI_proc = newmodel.PHI_proc

@jax.jit
def get_process(alpha):
    return PHI_proc @ alpha

vget_process = jax.vmap(get_process)

filt_vals = vget_process(ms)

filt_grid = newmodel.process_grid

filt_grids = jnp.tile(filt_grid.coords, (T+1, 1, 1))

t_filt_locs = jnp.vstack(
    jl.map(
        lambda i: jnp.column_stack(
            [jnp.tile(i, filt_grids[i].shape[0]), filt_grids[i]]
           ),
        jnp.arange(T+1),
                )
    )

pdata = jnp.column_stack([t_filt_locs, jnp.concatenate(filt_vals)])

filt_data = ST_Data_Long(
    x=pdata[:, 1], y=pdata[:, 2], t=pdata[:, 0], z=pdata[:, 3]
)

gif_st_grid(filt_data, output_file="new_filt.gif")

obs_locs = jnp.column_stack([obs_data_wide.x, obs_data_wide.y])
m_0 = newmodel.m_0
P_0 = newmodel.P_0
M = newmodel.M
PHI_obs = newmodel.process_basis.mfun(obs_locs)
nbasis = newmodel.process_basis.nbasis
nobs = obs_locs.shape[0]
grid = newmodel.process_grid
PHI = newmodel.process_basis.mfun(grid.coords)
GRAM = (PHI.T @ PHI) * grid.area


#+end_src

* Kalman Smoothers

#+begin_src python :session example :results none
m_tTs, P_tTs, Js= model.smooth(ms, Ps, mpreds, Ppreds)

PHI_obs = model.process_basis.mfun(obs_locs)

P_ttmTs = model.lag1smooth(Ps, Js, Ks[-1], PHI_obs)

#+end_src

* Maximum Likelihood

#+begin_src python :session example :results none

newmodel = gen_example_idem(keys[0], k_spat_inv=True, process_basis=place_basis(nres=1, min_knot_num=5))

obs_locs = jnp.column_stack([obs_data_wide.x, obs_data_wide.y])
m_0 = newmodel.m_0
P_0 = newmodel.P_0
M = newmodel.M
PHI_obs = newmodel.process_basis.mfun(obs_locs)
nbasis = newmodel.process_basis.nbasis
nobs = obs_locs.shape[0]
grid = newmodel.process_grid
PHI = newmodel.process_basis.mfun(grid.coords)
GRAM = (PHI.T @ PHI) * grid.area

if truemodel.K_basis == None:
    raise "Please equip the model with a kernel basis"

K_basis = truemodel.K_basis

@jax.jit
def con_M(k):
    @jax.jit
    def kernel(s, r):
        theta = (
            k[0] @ K_basis[0].vfun(s),
            k[1] @ K_basis[1].vfun(s),
            jnp.array(
                [
                 k[2] @ K_basis[2].vfun(s),
                 k[3] @ K_basis[3].vfun(s),
                      ]
            ),
            )
        return theta[0] * jnp.exp(
            -(jnp.sum((r - s - theta[2]) ** 2)) / theta[1]
            )

    K = outer_op(grid.coords, grid.coords, kernel)
    return solve(GRAM, PHI.T @ K @ PHI) * grid.area**2

@jax.jit
def objective(x_0):
    
    m_0 = x_0[0:nbasis]
    sigma2_0 = x_0[nbasis]
    sigma2_eps = x_0[nbasis+1]
    sigma2_eta = x_0[nbasis+2]

    ks = (jnp.array([x_0[nbasis+3]]),
        jnp.array([x_0[nbasis+4]]),
        jnp.array([x_0[nbasis+5]]),
        jnp.array([x_0[nbasis+6]]))
    
    M = con_M(ks)
    Sigma_eta = sigma2_eta * jnp.eye(nbasis)
    Sigma_eps = sigma2_eps * jnp.eye(nobs)
    P_0 = sigma2_0 * jnp.eye(nbasis)
    carry, seq = kalman_filter(
                m_0,
                P_0,
                M,
                PHI_obs,
                Sigma_eta,
                Sigma_eps,
                beta,
                obs_data_wide,
                X_obs,
            )
    return -carry[4]

ks = (
            jnp.array([150]),
            jnp.array([0.002]),
            jnp.array([-0.1]),
            jnp.array([0.1]),
        )

sigma2_0 = 0.1
sigma2_eta = 0.05**2
sigma2_eps = 0.01**2
beta = jnp.array([0,0,0])

x_0 = jnp.append(jnp.append(m_0, jnp.array([sigma2_0, sigma2_eps, sigma2_eta, ks[0][0], ks[1][0], ks[2][0], ks[3][0]])), beta)

#result = jax.scipy.optimize.minimize(objective, x_0, method="BFGS")

#+end_src

* Testing Wrap & unwrap

#+begin_src python :session example :results none

key = jax.random.PRNGKey(12)
keys = rand.split(key, 3)

K_basis = (
                place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
                place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1),
                place_basis(nres=1),
                place_basis(nres=1),
            )


k = (
                jnp.array([200]),
                jnp.array([0.002]),
                0.1 * rand.normal(keys[0], shape=(K_basis[2].nbasis,)),
                0.1 * rand.normal(keys[1], shape=(K_basis[3].nbasis,)),
            )

m_0 = jnp.ones(25)
sigma2_0 = 0.01
sigma2_eps = 0.025
sigma2_eta = 0.005

params = (m_0, sigma2_0, sigma2_eps, sigma2_eta, k)

wrapped, dims = param_wrap(params)

unwrapped = param_unwrap(wrapped, dims)

#+end_src
