#+Title: Tasks

* CURRENT GOALS 15/04/25

** TODO Parallel Kalman Filter(s)
*** TODO Is the parallel square root filter possible?

** TODO It should also be possible to implement a infinite-initial-variance to all(?) or the filters; confirm and implement this!

** TODO Smoothers (sqrt, information, and the corresponding lag-1 smoothers)
*** TODO EM algorithm follows quickly, implement to test
*** TODO Corresponding parallel equivalents as well

** TODO Clean and update documentation, particularily docstrings
*** TODO Continue the mathematics page, correcting any errors and expanding and completing
*** TODO Once documentation is cleaned up and certain rubbish is removed, a proper published early version of the package is nearly ready

** TODO Remove (or at least git-ignore) unnessecary or outdated scripts

** TODO Implement prediction for the filters
- Is relatively simple wiht these algorithms, just add a new argument to each filter to predict for that many extra time steps. 
*** TODO Also, backcasting for the smoothers

** TODO Complete and test the ~fit_mle~ and ~sample_posterior~ interfaces for ~idem.Model~

** TODO Finish analysing the Sydney Radar explorative example
- Include visualisation, discussion, complete the two MCMC examples, introduce using BlackJax for HMC, cover using the  ~fit_mle~ and ~sample_posterior~ methods above for ease of use
*** TODO As an extra side, I want to look at the German air quality data, due to it's challenging missing data. 
**** With missing data, sqrt_information filter now works fully. Interestingly, it required what I thought to be a worse method of computation:

#+begin_src python :session example :results none
R_k = safe_cholesky(PHI_k.T @ PHI_k / sigma2_eps_k)
#+end_src

instead of

#+begin_src python :session example :results none
R_k = jnp.linalg.qr(PHI_k/jnp.sqrt(sigma2_eps_k), mode="r")
#+end_src

But upon testing, despite ~safe_cholesky~ having a built-in conditional _and_ being slightly less efficient (and, perhaps notably, precise!) the new version actually gets noticably better sppeds (in the partially-censored Sydney Radar data set with 129 basis functions and an invariant kernel, the new option had an average likelihood-computation of 0.21361656522750855s, compared to the old one's 0.25794637274742127).
***** TODO Investigate this more. is it simply that cholesky decopmosition is _that_ much quicker than QR? What about other implementations/methods for the two decomposers? What about on a GPU?
****** TODO Take a quick look at Tall-Skinny QR decomposition

*** TODO Also, try finding a data set with which to test /invariant/ kernels

** Other MCMC related tasks
*** TODO Tuning is a problem right now!
  DEADLINE: <2025-04-22 Tue>
- preconditioning
  
** Speed improvements
*** TODO Find what parts of likelihood computation are cumbersome
- I believe the ~con_M~ method is somewhat of a bottleneck right now?
- Integration is done rather Naively right now (the same as in R-IDE), perhaps a better method could be used?
  
** TODO Compare the filters!
- Especially once the parallel filters are implemented, I want to expand on how they all compare, especially as process & observation dimension change.
    - At least partially done using the scale tests, with results as you'd expect; sqrt filters are slightly slower than their counterparts (assuming lower precision is ok for all), and the information filter performs significantly better for high data-dimensional cases like the used Sydney Radar data set.
      - More proper comparisons, gradually increasing the observation dimension, is still necessary.
      - Now that `util.timejit` is fully working, this should be easy.

** Other improvements to do
*** DONE Re-work how covariate data is handled
- I'm not happy with the relative clunkyness of it right now.
*** TODO Improve how data can be placed during the initialisation to automate things like grid placements
- ~init_model~ will implement this!

** Get Hamilton Access and test with some large chains
DEADLINE: <2025-04-22 Tue>

- apptainer for Hamilton
- check that MALA and RMH are targetting the same posterior

** TODO Look into how much faster jl.scan (with it's broken tqdm) is to a traditional loop for blackjax

* TODO Scaling tests
- Number of CPU cores
  
- start at mle
- better tuning
  
  - 

* TODO Can zs/zs_tree/obs_data be static? would that be faster?

- Could I pull the 'informationify' logic out of the information filters, reducing th likelihood computation in those cases?
  - This would actually be necessary for the information filters, since PyTrees cannot be static arguments. This also complicates likelihood computation.
- This has one significant roblem; likelihood computation. If i pre-compute informations (including beta after as in 'informationified X_obs' multiplied by beta), the ztildes still need to be computed for likelihood computation.
  - Again, this could be a not-problem if I was able to write the information form of the likelihood in a way that actually works! For now, lets just power through without.

- To test though, I should try without the covariate problem, to see how much this pre-computation actually makes a difference.
  - Could I handle covariates differently? I believe they are not directly optimised in R-IDE








Seriously get apptainer to work, and use it to restrict core counts!
Try interactive nodes
ABC
wilkinson owen golesby 2015 statistics and computing
NCC


