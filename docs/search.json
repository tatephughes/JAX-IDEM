[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "",
    "text": "Code\nimport sys\nimport os\nsys.path.append(os.path.abspath('src/jax_idem'))\n\nimport jax\nimport importlib\nimport utilities\nimport IDEM\n\nimportlib.reload(utilities)\nimportlib.reload(IDEM)\n\nfrom utilities import *\nfrom IDEM import *\nimport warnings\n\nimport matplotlib.pyplot as plt\n\nseed = 4\nkey = jax.random.PRNGKey(seed)\nkeys = rand.split(key, 2)\n\nmodel = gen_example_idem(keys[0], k_spat_inv=False, ngrid=jnp.array([40, 40]))\n\n# Simulation\nT = 35\nprocess_data, obs_data = model.simulate(key, T=T, nobs=50)\n\ndpi = 200\nwidth = 576 / dpi\nheight = 480 / dpi\n\n# plot the objects\ngif_st_grid(process_data, \"site/process.gif\", width=width, height=height)\ngif_st_pts(obs_data, \"site/obs.gif\", width=width, height=height)\nmodel.kernel.save_fig(\"site/kernel.png\", width=width, height=height)\n\nfrom PIL import Image, ImageSequence\n\ngif1 = Image.open('site/process.gif')\ngif2 = Image.open('site/tardis.gif')\n\nwidth, height = gif1.size\n\nframes = []\nnum_frames_gif1 = len(list(ImageSequence.Iterator(gif1)))\nnum_frames_gif2 = len(list(ImageSequence.Iterator(gif2)))\nmax_frames = max(num_frames_gif1, num_frames_gif2)\n\nfor i in range(max_frames):\n    frame1 = ImageSequence.Iterator(gif1)[i % num_frames_gif1].convert(\"RGBA\")\n    frame2 = ImageSequence.Iterator(gif2)[i % num_frames_gif2].convert(\"RGBA\")\n\n    frame2 = frame2.resize((width, height), Image.LANCZOS)\n    \n    combined = Image.alpha_composite(frame1, frame2)\n    frames.append(combined)\n\n\nframes[0].save('site/process.gif', save_all=True, append_images=frames[1:], duration=gif1.info['duration'], loop=0)"
  },
  {
    "objectID": "index.html#the-technicalities",
    "href": "index.html#the-technicalities",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "1 The Technicalities",
    "text": "1 The Technicalities\nFor a rundown of the mathematics underpinning this model and implementation, see here."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "2 Documentation",
    "text": "2 Documentation\nDocumentation for the package is available here"
  },
  {
    "objectID": "site/filtering_and_smoothing.html",
    "href": "site/filtering_and_smoothing.html",
    "title": "Filtering in JAX-IDEM",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "site/filtering_and_smoothing.html#the-simple-model",
    "href": "site/filtering_and_smoothing.html#the-simple-model",
    "title": "Filtering in JAX-IDEM",
    "section": "1.1 The simple model",
    "text": "1.1 The simple model\nConsider the simple system, for \\(t=1,\\dots,T\\)\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{\\alpha}}_{t+1} &= M\\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\eta}}_t,\\\\\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\alpha_0 = (1,1)^\\intercal\\) and\n\\[\\begin{split}\nM = \\left[\\begin{matrix}\n    \\cos(0.3) & -\\sin(0.3)\\\\\n    \\sin(0.3) & \\sin(0.3)\n\\end{matrix}\\right].\n\\end{split}\n\\tag{2}\\]\nThe error terms are mutually independant and have variances \\(\\sigma^{2}_\\epsilon=0.02\\) and \\(\\sigma^{2}_{\\eta}=0.03\\) and \\(\\boldsymbol{\\mathbf{z}}_t\\) are transformed linear ‘observations’ of \\(\\boldsymbol{\\mathbf{\\alpha}}\\)\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{z}}_t &= \\Phi \\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\epsilon}}_t,\\\\\n\\Phi &= \\left[\\begin{matrix}\n1   & 0  \\\\\n0.6 & 0.4\\\\\n0.4 & 0.6\n\\end{matrix}\\right]\n\\end{split}.\n\\tag{3}\\]\nThe process, \\(\\alpha\\), simply spins in a circle with some noise. Lets simulate from this system;\n\n\nCode\nimport sys\nimport os\nsys.path.append(os.path.abspath('../src/jax_idem'))\n\nimport jax.random as rand\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nimport filter_smoother_functions as fsf\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objs as go\nimport pandas as pd\n\n\n\n\nCode\nkey = jax.random.PRNGKey(1)\n\n\nalpha_0 = jnp.ones(2)  # 2D, easily plottable\nM = jnp.array([[jnp.cos(0.3), -jnp.sin(0.3)],\n              [jnp.sin(0.3), jnp.cos(0.3)]])  # spinny\n\nalphas = [alpha_0]\nzs = []\n\nT = 50\nkeys = rand.split(key, T*2)\n\nsigma2_eta = 0.03\nSigma_eta = sigma2_eta*jnp.eye(2)\nchol_s_eta = jax.scipy.linalg.cholesky(Sigma_eta, lower=True)\nsigma2_eps = 0.02\nSigma_eps = sigma2_eps*jnp.eye(3)\nchol_s_eps = jax.scipy.linalg.cholesky(Sigma_eps, lower=True)\nPHI = jnp.array([[1, 0], [0.6, 0.4], [0.4, 0.6]])\n\nfor i in range(T):\n    alphas.append(M@alphas[i] + chol_s_eta @\n                  rand.normal(keys[2*i], shape=(2,)))\n    zs.append(PHI @ alphas[i+1] + chol_s_eps @\n              rand.normal(keys[2*i+1], shape=(3,)))\n\nalphas_df = pd.DataFrame(alphas, columns = [\"x\", \"y\"])\nzs_df = pd.DataFrame(zs, columns = [\"x\", \"y\", \"z\"])\n\n\nalphas = jnp.array(alphas)\nzs = jnp.array(zs)\n\n    \nfig1 = px.line(alphas_df, x='x', y='y', height=200)\nfig1.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n\n\n\n\n                                                \n\n\nFigure 1\n\n\n\n\n\n\nCode\nfig2 = go.Figure(data=[go.Scatter3d(\n    x=zs_df['x'],\n    y=zs_df['y'],\n    z=zs_df['z'],\n    mode='markers',\n    marker=dict(\n        symbol='cross',  # Change marker to cross\n        size=5           # Adjust marker size\n    )\n)])\n\nfig2.update_layout(height=200)\n    \nfig2.show()\n\n\n\n\n                                                \n\n\nFigure 2\n\n\n\n\nWe can see how the process is an odd random spiral, and the observations are skewed noisy observations of this in 3D space\nWith filtering, we aim to recover the process {fig-truth} from the observations {fig-obs}. We do this with two ‘forms’ of the filter, which should be equivalent."
  },
  {
    "objectID": "reference/place_basis.html",
    "href": "reference/place_basis.html",
    "title": "1 place_basis",
    "section": "",
    "text": "utilities.place_basis(\n    data=jnp.array([[0, 0], [1, 1]]),\n    nres=2,\n    aperture=1.25,\n    min_knot_num=3,\n    basis_fun=bisquare,\n)\nDistributes knots (centroids) and scales for basis functions over a number of resolutions,similar to auto_basis from the R package FRK. This function must be run outside of a jit loop, since it involves varying the length of arrays.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n\nArray of 2D points defining the space on which to put the basis functions\njnp.array([[0, 0], [1, 1]])\n\n\nnres\n\nThe number of resolutions at which to place basis functions\n2\n\n\naperture\n\nScaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)\n1.25\n\n\nmin_knot_num\n\nThe number of basis functions to place in each dimension at the coursest resolution\n3\n\n\nbasis_fun\n\nThe basis functions being used. The basis function’s second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.\nbisquare\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA Basis object (NamedTuple) with the vector and matrix functions, and the\n\n\n\n\nparameters associated to the basis functions."
  },
  {
    "objectID": "reference/place_basis.html#parameters",
    "href": "reference/place_basis.html#parameters",
    "title": "1 place_basis",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n\nArray of 2D points defining the space on which to put the basis functions\njnp.array([[0, 0], [1, 1]])\n\n\nnres\n\nThe number of resolutions at which to place basis functions\n2\n\n\naperture\n\nScaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)\n1.25\n\n\nmin_knot_num\n\nThe number of basis functions to place in each dimension at the coursest resolution\n3\n\n\nbasis_fun\n\nThe basis functions being used. The basis function’s second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.\nbisquare"
  },
  {
    "objectID": "reference/place_basis.html#returns",
    "href": "reference/place_basis.html#returns",
    "title": "1 place_basis",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA Basis object (NamedTuple) with the vector and matrix functions, and the\n\n\n\n\nparameters associated to the basis functions."
  },
  {
    "objectID": "reference/outer_op.html",
    "href": "reference/outer_op.html",
    "title": "1 outer_op",
    "section": "",
    "text": "utilities.outer_op(a, b, op=lambda x, y: x * y)\nComputes the outer operation of two vectors, a generalisation of the outer product.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nArrayLike\nArray of the first vector\nrequired\n\n\nb\nArrayLike\nArray of the second vector\nrequired\n\n\nop\nCallable\nA jit-function acting on an element of vec1 and an element of vec2. By default, this is the outer product.\nlambda x, y: x * y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nArrayLike[C] (n, m):\nThe matrix of the result of applying operation to every pair of elements from the two vectors."
  },
  {
    "objectID": "reference/outer_op.html#parameters",
    "href": "reference/outer_op.html#parameters",
    "title": "1 outer_op",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\na\nArrayLike\nArray of the first vector\nrequired\n\n\nb\nArrayLike\nArray of the second vector\nrequired\n\n\nop\nCallable\nA jit-function acting on an element of vec1 and an element of vec2. By default, this is the outer product.\nlambda x, y: x * y"
  },
  {
    "objectID": "reference/outer_op.html#returns",
    "href": "reference/outer_op.html#returns",
    "title": "1 outer_op",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nArrayLike[C] (n, m):\nThe matrix of the result of applying operation to every pair of elements from the two vectors."
  },
  {
    "objectID": "reference/Basis.html",
    "href": "reference/Basis.html",
    "title": "1 Basis",
    "section": "",
    "text": "utilities.Basis()\nA simple class for spatial basis expansions.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvfun\nArrayLike (ndim,) -&gt; ArrayLike (nbasis,)\nApplying to a single spatial location, evaluates all the basis functions on the single location and returns the result as a vector.\n\n\nmfun\nArrayLike (ndim, n) -&gt; ArrayLike (nbasis, n)\nApplying to a array of spatial points, evaluates all the basis functions on each point and returns the results in a matrix.\n\n\nparams\nArrayLike(nparams, nbasis)\nThe parameters defining each basis function. For example, for bisquare basis functions, the parameters are the locations of the centers of each function, and the function’s scale.\n\n\nnbasis\nint\nThe number of basis functions in the expansion."
  },
  {
    "objectID": "reference/Basis.html#attributes",
    "href": "reference/Basis.html#attributes",
    "title": "1 Basis",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nvfun\nArrayLike (ndim,) -&gt; ArrayLike (nbasis,)\nApplying to a single spatial location, evaluates all the basis functions on the single location and returns the result as a vector.\n\n\nmfun\nArrayLike (ndim, n) -&gt; ArrayLike (nbasis, n)\nApplying to a array of spatial points, evaluates all the basis functions on each point and returns the results in a matrix.\n\n\nparams\nArrayLike(nparams, nbasis)\nThe parameters defining each basis function. For example, for bisquare basis functions, the parameters are the locations of the centers of each function, and the function’s scale.\n\n\nnbasis\nint\nThe number of basis functions in the expansion."
  },
  {
    "objectID": "reference/basis_params_to_st_data.html",
    "href": "reference/basis_params_to_st_data.html",
    "title": "1 basis_params_to_st_data",
    "section": "",
    "text": "IDEM.basis_params_to_st_data(alphas, process_basis, process_grid, times=None)\nConverts the process expansion coefficients back into the original process \\(Y_t(s)\\) on the inputted process grid.\n\n\nalphas: ArrayLike (T, r) The basis coefficients of the process process_basis: Basis The basis to use in the expansion process_grid: Grid The grid points on which to evaluate \\(Y\\) times: ArrayLike (T,) (optional) The array of times which the processes correspond to"
  },
  {
    "objectID": "reference/basis_params_to_st_data.html#params",
    "href": "reference/basis_params_to_st_data.html#params",
    "title": "1 basis_params_to_st_data",
    "section": "",
    "text": "alphas: ArrayLike (T, r) The basis coefficients of the process process_basis: Basis The basis to use in the expansion process_grid: Grid The grid points on which to evaluate \\(Y\\) times: ArrayLike (T,) (optional) The array of times which the processes correspond to"
  },
  {
    "objectID": "reference/simIDEM.html",
    "href": "reference/simIDEM.html",
    "title": "1 simIDEM",
    "section": "",
    "text": "IDEM.simIDEM(\n    key,\n    T,\n    M,\n    PHI_proc,\n    PHI_obs,\n    obs_locs,\n    beta,\n    alpha0,\n    sigma2_eta=0.01 ** 2,\n    sigma2_eps=0.01 ** 2,\n    process_grid=create_grid(bounds, ngrids),\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from a IDE model. For jit-ability, this only takes in certain parameters. For ease of use, use IDEM.simulate.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/simIDEM.html#parameters",
    "href": "reference/simIDEM.html#parameters",
    "title": "1 simIDEM",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)"
  },
  {
    "objectID": "reference/simIDEM.html#returns",
    "href": "reference/simIDEM.html#returns",
    "title": "1 simIDEM",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/IDEM_Model.html",
    "href": "reference/IDEM_Model.html",
    "title": "1 IDEM_Model",
    "section": "",
    "text": "IDEM.IDEM_Model(\n    self,\n    process_basis,\n    kernel,\n    process_grid,\n    sigma2_eta,\n    sigma2_eps,\n    beta,\n    int_grid=create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n    m_0=None,\n    sigma2_0=None,\n)\nThe Integro-differential Equation Model.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\ndata_mle_fit\nMAY BE OUT OF DATE\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfilter_information\nNOT IMPLEMENTED\n\n\nfit_information_filter\nNOT FULLY IMPLEMENTED\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function simIDEM.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\n\n\n\nIDEM.IDEM_Model.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nIDEM.IDEM_Model.data_mle_fit(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nMAY BE OUT OF DATE\n\n\n\nIDEM.IDEM_Model.filter(obs_data_wide, X_obs)\nRuns the Kalman filter on the inputted data.\n\n\n\nIDEM.IDEM_Model.filter_information(obs_data, X_obs, nu_0=None, Q_0=None)\nNOT IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.fit_information_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nNOT FULLY IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nIDEM.IDEM_Model.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from the model, using the jit-able function simIDEM.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the ST_Data_Long type (see utilities)\n\n\n\n\n\n\n\nIDEM.IDEM_Model.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the"
  },
  {
    "objectID": "reference/IDEM_Model.html#methods",
    "href": "reference/IDEM_Model.html#methods",
    "title": "1 IDEM_Model",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\ndata_mle_fit\nMAY BE OUT OF DATE\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfilter_information\nNOT IMPLEMENTED\n\n\nfit_information_filter\nNOT FULLY IMPLEMENTED\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function simIDEM.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\n\n\n\nIDEM.IDEM_Model.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nIDEM.IDEM_Model.data_mle_fit(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nMAY BE OUT OF DATE\n\n\n\nIDEM.IDEM_Model.filter(obs_data_wide, X_obs)\nRuns the Kalman filter on the inputted data.\n\n\n\nIDEM.IDEM_Model.filter_information(obs_data, X_obs, nu_0=None, Q_0=None)\nNOT IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.fit_information_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nNOT FULLY IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nIDEM.IDEM_Model.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from the model, using the jit-able function simIDEM.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the ST_Data_Long type (see utilities)\n\n\n\n\n\n\n\nIDEM.IDEM_Model.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the"
  },
  {
    "objectID": "reference/kalman_smoother.html",
    "href": "reference/kalman_smoother.html",
    "title": "1 kalman_smoother",
    "section": "",
    "text": "1 kalman_smoother\nfilter_smoother_functions.kalman_smoother(ms, Ps, mpreds, Ppreds, M)\nNOT FULLY IMPLEMENTED"
  },
  {
    "objectID": "reference/information_filter.html",
    "href": "reference/information_filter.html",
    "title": "1 information_filter",
    "section": "",
    "text": "filter_smoother_functions.information_filter(\n    nu_0,\n    Q_0,\n    M,\n    PHI_obs_tuple,\n    Sigma_eta,\n    Sigma_eps_tuple,\n    ztildes,\n    full_likelihood=False,\n)\nApplies the Information Filter to a PyTree of data.\n\n\nnu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nSigma_eta: ArrayLike (r,r)\n    The Covariance matrix of the process noise\nsigma2_eps_tuple: Pytree[ArrayLike (n_t,n_t)]\n    The Covariance matrix of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation\n\n\n\nA tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/information_filter.html#parameters",
    "href": "reference/information_filter.html#parameters",
    "title": "1 information_filter",
    "section": "",
    "text": "nu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nSigma_eta: ArrayLike (r,r)\n    The Covariance matrix of the process noise\nsigma2_eps_tuple: Pytree[ArrayLike (n_t,n_t)]\n    The Covariance matrix of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation"
  },
  {
    "objectID": "reference/information_filter.html#returns",
    "href": "reference/information_filter.html#returns",
    "title": "1 information_filter",
    "section": "",
    "text": "A tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/kalman_filter.html",
    "href": "reference/kalman_filter.html",
    "title": "1 kalman_filter",
    "section": "",
    "text": "filter_smoother_functions.kalman_filter(\n    m_0,\n    P_0,\n    M,\n    PHI_obs,\n    Sigma_eta,\n    Sigma_eps,\n    ztildes,\n    full_likelihood=False,\n)\nApplies the Kalman Filter to a wide-format matrix of data. For jit-ability, this only allows full (no missing) data in a wide format. For changing data locations or changing data dimension, see information_filter_indep.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nSigma_eta\nArrayLike\nThe Covariance matrix of the process noise\nrequired\n\n\nSigma_eps\nArrayLike\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter.html#parameters",
    "href": "reference/kalman_filter.html#parameters",
    "title": "1 kalman_filter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nSigma_eta\nArrayLike\nThe Covariance matrix of the process noise\nrequired\n\n\nSigma_eps\nArrayLike\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired"
  },
  {
    "objectID": "reference/kalman_filter.html#returns",
    "href": "reference/kalman_filter.html#returns",
    "title": "1 kalman_filter",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "1 Function reference",
    "section": "",
    "text": "Functions to apply Kalman/information filters, smoothers and so on.\n\n\n\nkalman_filter\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\nkalman_filter_indep\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\ninformation_filter\nApplies the Information Filter to a PyTree of data.\n\n\ninformation_filter_indep\nApplies the Information Filter to a PyTree of data.\n\n\nkalman_smoother\nNOT FULLY IMPLEMENTED\n\n\n\n\n\n\nClasses and functions to perform simulation, fitting, filtering and prediction on IDEMs.\n\n\n\nKernel\nGeneric class defining a kernel, or a basis expansion of a kernel with\n\n\nIDEM_Model\nThe Integro-differential Equation Model.\n\n\nparam_exp_kernel\nCreates a kernel in the style of AZM’s R-IDE package\n\n\nsimIDEM\nSimulates from a IDE model.\n\n\ngen_example_idem\nCreates an example IDE model, with randomly generated kernel on the\n\n\nbasis_params_to_st_data\nConverts the process expansion coefficients back into the original process\n\n\n\n\n\n\nGeneral classes and functions used to supplement the main package\n\n\n\nGrid\nA simple grid class to store (currently exclusively regular) grids, along\n\n\nBasis\nA simple class for spatial basis expansions.\n\n\ncreate_grid\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\nouter_op\nComputes the outer operation of two vectors, a generalisation of the outer\n\n\nbisquare\nGeneric bisquare function\n\n\nplace_basis\nDistributes knots (centroids) and scales for basis functions over a\n\n\nst_data\nFor storing spatio-temporal data and appropriate methods for plotting such"
  },
  {
    "objectID": "reference/index.html#filtering-and-smoothing-functions",
    "href": "reference/index.html#filtering-and-smoothing-functions",
    "title": "1 Function reference",
    "section": "",
    "text": "Functions to apply Kalman/information filters, smoothers and so on.\n\n\n\nkalman_filter\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\nkalman_filter_indep\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\ninformation_filter\nApplies the Information Filter to a PyTree of data.\n\n\ninformation_filter_indep\nApplies the Information Filter to a PyTree of data.\n\n\nkalman_smoother\nNOT FULLY IMPLEMENTED"
  },
  {
    "objectID": "reference/index.html#integro-difference-models-in-jax",
    "href": "reference/index.html#integro-difference-models-in-jax",
    "title": "1 Function reference",
    "section": "",
    "text": "Classes and functions to perform simulation, fitting, filtering and prediction on IDEMs.\n\n\n\nKernel\nGeneric class defining a kernel, or a basis expansion of a kernel with\n\n\nIDEM_Model\nThe Integro-differential Equation Model.\n\n\nparam_exp_kernel\nCreates a kernel in the style of AZM’s R-IDE package\n\n\nsimIDEM\nSimulates from a IDE model.\n\n\ngen_example_idem\nCreates an example IDE model, with randomly generated kernel on the\n\n\nbasis_params_to_st_data\nConverts the process expansion coefficients back into the original process"
  },
  {
    "objectID": "reference/index.html#utilties",
    "href": "reference/index.html#utilties",
    "title": "1 Function reference",
    "section": "",
    "text": "General classes and functions used to supplement the main package\n\n\n\nGrid\nA simple grid class to store (currently exclusively regular) grids, along\n\n\nBasis\nA simple class for spatial basis expansions.\n\n\ncreate_grid\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\nouter_op\nComputes the outer operation of two vectors, a generalisation of the outer\n\n\nbisquare\nGeneric bisquare function\n\n\nplace_basis\nDistributes knots (centroids) and scales for basis functions over a\n\n\nst_data\nFor storing spatio-temporal data and appropriate methods for plotting such"
  },
  {
    "objectID": "reference/kalman_filter_indep.html",
    "href": "reference/kalman_filter_indep.html",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "filter_smoother_functions.kalman_filter_indep(\n    m_0,\n    P_0,\n    M,\n    PHI_obs,\n    sigma2_eta,\n    sigma2_eps,\n    ztildes,\n    full_likelihood=False,\n)\nApplies the Kalman Filter to a wide-format matrix of data. Includes some optimisation for uncorrelated errors. For jit-ability, this only allows full (no missing) data in a wide format. For changing data locations or changing data dimension, see information_filter_indep.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nsigma2_eta\nfloat\nThe Covariance matrix of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter_indep.html#parameters",
    "href": "reference/kalman_filter_indep.html#parameters",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nsigma2_eta\nfloat\nThe Covariance matrix of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired"
  },
  {
    "objectID": "reference/kalman_filter_indep.html#returns",
    "href": "reference/kalman_filter_indep.html#returns",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/information_filter_indep.html",
    "href": "reference/information_filter_indep.html",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "filter_smoother_functions.information_filter_indep(\n    nu_0,\n    Q_0,\n    M,\n    PHI_obs_tuple,\n    sigma2_eta,\n    sigma2_eps,\n    ztildes,\n    full_likelihood=True,\n)\nApplies the Information Filter to a PyTree of data.\nIncludes some optimisation for uncorrelated errors.\n\n\nnu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nsigma2_eta: float\n    The variance of the process noise\nsigma2_eps: float\n    The variance of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation\n\n\n\nA tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/information_filter_indep.html#parameters",
    "href": "reference/information_filter_indep.html#parameters",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "nu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nsigma2_eta: float\n    The variance of the process noise\nsigma2_eps: float\n    The variance of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation"
  },
  {
    "objectID": "reference/information_filter_indep.html#returns",
    "href": "reference/information_filter_indep.html#returns",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "A tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/Kernel.html",
    "href": "reference/Kernel.html",
    "title": "1 Kernel",
    "section": "",
    "text": "IDEM.Kernel(self, function, basis=None, params=None, form='expansion')\nGeneric class defining a kernel, or a basis expansion of a kernel with its parameters.\n\n\n\n\n\nName\nDescription\n\n\n\n\nsave_fig\nSaves a plot of the direction of the kernel.\n\n\nshow_plot\nShows a plot of the direction of the kernel.\n\n\n\n\n\nIDEM.Kernel.save_fig(name='kernel.png', width=5, height=4, dpi=300)\nSaves a plot of the direction of the kernel.\n\n\n\nIDEM.Kernel.show_plot(width=5, height=4)\nShows a plot of the direction of the kernel."
  },
  {
    "objectID": "reference/Kernel.html#methods",
    "href": "reference/Kernel.html#methods",
    "title": "1 Kernel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsave_fig\nSaves a plot of the direction of the kernel.\n\n\nshow_plot\nShows a plot of the direction of the kernel.\n\n\n\n\n\nIDEM.Kernel.save_fig(name='kernel.png', width=5, height=4, dpi=300)\nSaves a plot of the direction of the kernel.\n\n\n\nIDEM.Kernel.show_plot(width=5, height=4)\nShows a plot of the direction of the kernel."
  },
  {
    "objectID": "reference/param_exp_kernel.html",
    "href": "reference/param_exp_kernel.html",
    "title": "1 param_exp_kernel",
    "section": "",
    "text": "1 param_exp_kernel\nIDEM.param_exp_kernel(K_basis, k)\nCreates a kernel in the style of AZM’s R-IDE package"
  },
  {
    "objectID": "reference/gen_example_idem.html",
    "href": "reference/gen_example_idem.html",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "IDEM.gen_example_idem(\n    key,\n    k_spat_inv=True,\n    ngrid=jnp.array([41, 41]),\n    nints=jnp.array([100, 100]),\n    nobs=50,\n    m_0=None,\n    sigma2_0=None,\n    process_basis=None,\n    sigma2_eta=0.5 ** 2,\n    sigma2_eps=0.1 ** 2,\n)\nCreates an example IDE model, with randomly generated kernel on the domain [0,1]x[0,1]. Intial value of the process is simply some of the coefficients for the process basis are set to 1. The kernel has a Gaussian shape, with parameters defined as basis expansions in order to allow for spatial variance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nk_spat_inv\nbool\nWhether or not the generated kernel should be spatially invariant.\nTrue\n\n\nngrid\nArrayLike\nThe resolution of the grid at which the process is computed. Should have shape (2,).\njnp.array([41, 41])\n\n\nnints\nArrayLike\nThe resolution of the grid at which Riemann integrals are computed. Should have shape (2,)\njnp.array([100, 100])\n\n\nnobs\nint\nThe number of observations per time interval.\n50\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA model of type IDEM."
  },
  {
    "objectID": "reference/gen_example_idem.html#parameters",
    "href": "reference/gen_example_idem.html#parameters",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nk_spat_inv\nbool\nWhether or not the generated kernel should be spatially invariant.\nTrue\n\n\nngrid\nArrayLike\nThe resolution of the grid at which the process is computed. Should have shape (2,).\njnp.array([41, 41])\n\n\nnints\nArrayLike\nThe resolution of the grid at which Riemann integrals are computed. Should have shape (2,)\njnp.array([100, 100])\n\n\nnobs\nint\nThe number of observations per time interval.\n50"
  },
  {
    "objectID": "reference/gen_example_idem.html#returns",
    "href": "reference/gen_example_idem.html#returns",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA model of type IDEM."
  },
  {
    "objectID": "reference/Grid.html",
    "href": "reference/Grid.html",
    "title": "1 Grid",
    "section": "",
    "text": "1 Grid\nutilities.Grid()\nA simple grid class to store (currently exclusively regular) grids, along with some key quantities such as the lenth between grid points, the number of grid points and the area/volume of each grid square/cube. Supports arbitrarily high dimension. Ideally, in the future, this will support non-regular grids with any necessary quantities to do, for example, integration over the points on the grid."
  },
  {
    "objectID": "reference/create_grid.html",
    "href": "reference/create_grid.html",
    "title": "1 create_grid",
    "section": "",
    "text": "utilities.create_grid(bounds, ngrids)\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nArrayLike\nThe bounds for each dimension\nrequired\n\n\nngrids\nArrayLike\nThe number of columns/rows/hyper-column in each dimension\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGrid Object (NamedTuple) containing the coordinates, deltas, grid\n\n\n\n\nnumbers, areas, etc. See the Grid class."
  },
  {
    "objectID": "reference/create_grid.html#parameters",
    "href": "reference/create_grid.html#parameters",
    "title": "1 create_grid",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nArrayLike\nThe bounds for each dimension\nrequired\n\n\nngrids\nArrayLike\nThe number of columns/rows/hyper-column in each dimension\nrequired"
  },
  {
    "objectID": "reference/create_grid.html#returns",
    "href": "reference/create_grid.html#returns",
    "title": "1 create_grid",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nGrid Object (NamedTuple) containing the coordinates, deltas, grid\n\n\n\n\nnumbers, areas, etc. See the Grid class."
  },
  {
    "objectID": "reference/bisquare.html",
    "href": "reference/bisquare.html",
    "title": "1 bisquare",
    "section": "",
    "text": "1 bisquare\nutilities.bisquare(s, params)\nGeneric bisquare function"
  },
  {
    "objectID": "reference/st_data.html",
    "href": "reference/st_data.html",
    "title": "1 st_data",
    "section": "",
    "text": "utilities.st_data(self, x, y, t, z)\nFor storing spatio-temporal data and appropriate methods for plotting such data, and converting between long and wide formats.\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wide\nGives the data in wide format. Any missing data will be represented in\n\n\n\n\n\nutilities.st_data.as_wide()\nGives the data in wide format. Any missing data will be represented in the returned matris as NaN.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA dictionary containing the x coordinates and y coordinates as JAX\n\n\n\n\narrays, and a matrix corresponding to the value of the process at\n\n\n\n\neach time point (columns) and spatial point (rows)."
  },
  {
    "objectID": "reference/st_data.html#methods",
    "href": "reference/st_data.html#methods",
    "title": "1 st_data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wide\nGives the data in wide format. Any missing data will be represented in\n\n\n\n\n\nutilities.st_data.as_wide()\nGives the data in wide format. Any missing data will be represented in the returned matris as NaN.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA dictionary containing the x coordinates and y coordinates as JAX\n\n\n\n\narrays, and a matrix corresponding to the value of the process at\n\n\n\n\neach time point (columns) and spatial point (rows)."
  },
  {
    "objectID": "site/mathematics.html",
    "href": "site/mathematics.html",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "site/mathematics.html#process-noise",
    "href": "site/mathematics.html#process-noise",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.1 Process Noise",
    "text": "2.1 Process Noise\nWe still have to set out what the process noise, \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\), and it’s spectral couterpart, \\(\\boldsymbol{\\mathbf{\\eta}}_t\\), are. Dewar (Dewar, Scerri, and Kadirkamanathan 2008) fixes the variance of \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\) to be uniform and uncorrelated across space and time, with \\(\\omega_t(\\boldsymbol{\\mathbf{s}}) \\sim \\mathcal N(0,\\sigma^2)\\) It is then easily shown that \\(\\boldsymbol{\\mathbf{\\eta}}_t\\) is also normal, with \\(\\boldsymbol{\\mathbf{\\eta}}_t \\sim \\mathcal N(0, \\sigma^2\\Psi^{-1})\\).\nHowever, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of \\(\\boldsymbol{\\mathbf{\\eta}}_t\\), and compute the variance of \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\) if needed. This is exactly what the IDE package (Zammit-Mangion 2022) in R does, and, correspondingly, what this JAX project does.\n\nLemma 1 Let \\(\\boldsymbol{\\mathbf{\\eta}}_t \\sim \\mathcal N(0, \\Sigma_\\eta)\\), and \\(\\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\boldsymbol{\\mathbf{\\eta}}_t, \\boldsymbol{\\mathbf{\\eta}}_{t+\\tau}] =0\\), \\(\\forall \\tau&gt;0\\). Then \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\) has covariance\n\\[\\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\omega_t(\\boldsymbol{\\mathbf{s}}), \\omega_{t+\\tau}(\\boldsymbol{\\mathbf{r}})] = \\begin{cases}\n\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal \\Sigma_\\eta \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}) & \\text{if }\\tau=0\\\\\n0 & \\text{else}\\\\\n\\end{cases}\n\\]\n\n\nProof. Consider \\(\\Psi \\boldsymbol{\\mathbf{\\eta}}_t\\). It is clearly normal, with expectation zero and variance (using (Equation 5)),\n\\[\\begin{split}\n\\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\Psi \\boldsymbol{\\mathbf{\\eta}}_t] &= \\Psi \\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\boldsymbol{\\mathbf{\\omega}}_t] \\Psi^\\intercal = \\Psi\\Sigma_\\eta\\Psi^\\intercal,\\\\\n&= \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal d\\boldsymbol{\\mathbf{s}} \\  \\Sigma_\\eta \\ \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{r}}\\\\\n&=  \\int\\int_{\\mathcal D_s^2} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal \\  \\Sigma_\\eta \\  \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{r}} d\\boldsymbol{\\mathbf{s}}\\\\\n\\end{split}\n\\tag{6}\\]\nSince it has zero expectation, we also have\n\\[\\begin{split}\n\\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\Psi\\boldsymbol{\\mathbf{\\eta}}_t] &= \\mathbb E[(\\Psi\\boldsymbol{\\mathbf{\\eta}}_t) (\\Psi\\boldsymbol{\\mathbf{\\eta}}_t)^\\intercal] = \\mathbb E[\\Psi\\boldsymbol{\\mathbf{\\eta}}_t\\boldsymbol{\\mathbf{\\eta}}_t^\\intercal\\Psi^\\intercal]\\\\\n&= \\mathbb E \\left[ \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})\\omega_t(\\boldsymbol{\\mathbf{s}})d\\boldsymbol{\\mathbf{s}} \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal \\omega_t(\\boldsymbol{\\mathbf{r}}) d\\boldsymbol{\\mathbf{r}} \\right]\\\\\n&= \\int\\int_{\\mathcal D_s^2} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})\\  \\mathbb E[\\omega_t(\\boldsymbol{\\mathbf{s}})\\omega_t(\\boldsymbol{\\mathbf{r}})]\\  \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{s}} d \\boldsymbol{\\mathbf{r}}.\n\\end{split}\n\\tag{7}\\]\nWe can see that, comparing (Equation 6) and (Equation 7), we have\n\\[\\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\omega_t(\\boldsymbol{\\mathbf{s}}), \\omega_t(\\boldsymbol{\\mathbf{r}})] = \\mathbb E[\\omega_t(\\boldsymbol{\\mathbf{s}})\\omega_t(\\boldsymbol{\\mathbf{r}})]= \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal \\Sigma_\\eta \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}).\n\\]"
  },
  {
    "objectID": "site/mathematics.html#kernel-decomposition",
    "href": "site/mathematics.html#kernel-decomposition",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.2 Kernel Decomposition",
    "text": "2.2 Kernel Decomposition\nNext is the key part od the system, which defines the dynamics; the kernelf function, \\(\\kappa\\). There are a few ways to handle the kernel. One of the most obvious is to expand it out into a spectral decomposition as well;\n\\[\\kappa \\approx \\sum_i \\beta_i\\psi(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}).\n\\]\nThis can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on \\(\\mathbb R^2\\times \\mathbb R^2\\); to get a wide enough space of possible functions, we would likely need many terms of the basis expansion.\nA much simpler approach would be to simply parameterise the kernel function, to \\(\\kappa(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}, \\boldsymbol{\\mathbf{\\theta}}_\\kappa)\\). We then establish a simple shape for the kernel (e.g. Gaussian) and rely on very few parameters (for example, scale, shape, offsets). The example kernel used in the program is aGaussian kernel;\n\\[\\kappa(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}; \\boldsymbol{\\mathbf{m}}, a, b) = a \\exp \\left( -\\frac{1}{b} \\Vert \\boldsymbol{\\mathbf{s}}- \\boldsymbol{\\mathbf{r}} +\\boldsymbol{\\mathbf{m}}\\Vert^2 \\right)\n\\]\nOf course, this kernel lacks spatial dependance. We can add spatial variance back in in a nice way by adding dependance on \\(\\boldsymbol{\\mathbf{s}}\\) to the parameters, for example, variyng the offset term as \\(\\boldsymbol{\\mathbf{m}}(\\boldsymbol{\\mathbf{s}})\\). Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground (Cressie and Wikle 2015). The actual parameters of such a spatially-variant kernel are then the basis coefficients for the expansion of any spatially variant parameters, as well as any constant parameters."
  },
  {
    "objectID": "site/mathematics.html#idem-as-a-linear-dynamical-system",
    "href": "site/mathematics.html#idem-as-a-linear-dynamical-system",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.3 IDEM as a linear dynamical system",
    "text": "2.3 IDEM as a linear dynamical system\nTo recap, we have discretized space in such a way that the Integro-difference model is of a more traditional linear dynamical system form. All that is left is to include our observations in our system.\nLets assume that at each time \\(t\\) there are \\(n_t\\) observations at locations \\(\\boldsymbol{\\mathbf{s}}_{1,t},\\dots, \\boldsymbol{\\mathbf{s}}_{n_{t},t}\\). We write the vector of the process at these points as \\(\\boldsymbol{\\mathbf{Y}}(t) = (Y(s_{1,t};t), \\dots, Y(s_{n_{t},t};t))^\\intercal\\), and, in it’s expanded form \\(\\boldsymbol{\\mathbf{Y}}_t = \\Phi_t \\boldsymbol{\\mathbf{\\alpha}}_t\\), where \\(\\Phi \\in \\mathbb R^{r\\times n_{t}}\\) is\n\\[\\begin{split}\n\\{\\Phi_{t}\\}_{i, j} = \\phi_{i}(s_{j,t}).\n\\end{split}\n\\]\nFor the covariates, we write the matrix \\(X_t = (\\boldsymbol{\\mathbf{X}}(\\boldsymbol{\\mathbf{s}}_{1, t}), \\dots, \\boldsymbol{\\mathbf{X}}(\\boldsymbol{\\mathbf{s}}_{1=n_{t}, t})^\\intercal\\). We then have\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{Z}}_t &= \\Phi \\alpha_t + X_{t} \\boldsymbol{\\mathbf{\\beta }}+ \\boldsymbol{\\mathbf{\\epsilon}}_t, \\quad t=0,1,\\dots, T,\\\\\n\\boldsymbol{\\mathbf{\\alpha}}_{t+1} &= M\\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\eta}}_t,\\quad t = 1,2,\\dots, T,\\\\\nM &= \\int_{\\mathcal D_s}\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal d\\boldsymbol{\\mathbf{s}} \\int_{\\mathcal D_s^2}\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\kappa(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}; \\boldsymbol{\\mathbf{\\theta}}_\\kappa)\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{r}} d \\boldsymbol{\\mathbf{s}},\n\\end{split}\n\\]\nWriting \\(\\tilde{\\boldsymbol{\\mathbf{Z}}}_t = \\boldsymbol{\\mathbf{Z}}_t - X_t \\boldsymbol{\\mathbf{\\beta}}\\),\n\\[\\begin{split}\n\\tilde{\\boldsymbol{\\mathbf{Z}}}_t &= \\Phi_{t} \\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\epsilon}}_t,\\quad &t = 1,2,\\dots, T,\\\\\n\\boldsymbol{\\mathbf{\\alpha}}_{t+1} &= M \\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\eta}}_t,\\quad &t = 0,1, \\dots, T.\\\\\n\\end{split}\n\\tag{8}\\]\nWe should also initialise \\(\\boldsymbol{\\mathbf{\\alpha}}_0 \\sim \\mathcal N^{r}(\\boldsymbol{\\mathbf{m}}_{0}, \\Sigma_{0})\\), and fix simple distrubtions to the noise terms,\n\\[\\begin{split}\n\\epsilon_{t,i} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\sigma^2_\\epsilon),\\\\\n\\eta_{t,i} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\sigma^2_\\eta),\n\\end{split}\n\\]\nwhich are (also) independant in time.\nAs in, for example, (Wikle and Cressie 1999), this is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood. We can use these quantities in either an EM algorithm or a Bayesian approach. Firstly, we cover the EM algorithm applied to this system.\nAt most, the parameters to be estimated are\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{\\theta }}= \\left(\\boldsymbol{\\mathbf{\\theta}}_\\kappa^\\intercal, \\boldsymbol{\\mathbf{\\beta}}^\\intercal, \\boldsymbol{\\mathbf{m}}_0^\\intercal, \\sigma^{2}_{\\epsilon}, \\sigma^{2}_{\\eta}, \\mathrm{vec}[\\Sigma_0]\\right),\n\\end{split}\n\\]\nwhere the \\(\\mathrm{vec}[\\cdot]\\) operator gives the elements of the matrix in a column vector. Of course, in practice, some of these may be estimated outside of the algorithm, fixed, given a much simpler form (e.g. \\(\\Sigma_\\eta = \\sigma_\\eta^2 I_d\\)), etc.\nThere are two approaches we can make from here; directly maximising the marginal data likelihood using only the Kalman filter, or maximising the full likelihood with the EM algorithm.\nNow (Equation 8) is of the very familar linear dynamical system (LDS) type. This is a well-understood problem, and optimal state estimation can be done using the kalman filter and (RTS) smoother."
  },
  {
    "objectID": "site/mathematics.html#sec-kalmanfilter",
    "href": "site/mathematics.html#sec-kalmanfilter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.1 The Kalman Filter",
    "text": "3.1 The Kalman Filter\nFirstly, we should establish some notation. Write\n\\[\\begin{split}\nm_{r \\mid s} &= \\mathbb E[\\boldsymbol{\\mathbf{\\alpha}}_r \\mid \\{Z_t\\}_{t=0,\\dots,s}]\\\\\nP_{r \\mid s} &= \\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\boldsymbol{\\mathbf{\\alpha}}_r \\mid \\{Z_t\\}_{t=0,\\dots,s}]\\\\\nP_{r,q \\mid s} &= \\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\boldsymbol{\\mathbf{\\alpha}}_r, \\boldsymbol{\\mathbf{\\alpha}}_q \\mid \\{Z_t\\}_{t=0,\\dots,s}].\n\\end{split}\n\\]\nFor the initial terms, \\(m_{0\\mid0}=m_0\\) and \\(P_{0\\mid0}=\\Sigma_0\\). For convenience and generality, we write \\(\\Sigma_\\eta\\) and \\(\\Sigma_\\epsilon\\) for the variance matrices of the process and observations. Note that, if the number of observations change at each time point (for example, due to missing data), then \\(\\Sigma_\\epsilon\\) should be time variyng; we could either always keep it as uncorrelated so that \\(\\Sigma_\\epsilon = \\mathrm{diag} (\\sigma_\\epsilon^2)\\), or perhaps put some kind of distance-dependant covariance function to it.\nTo move the filter forward, that is, given \\(m_{r\\mid s}\\) and \\(P_{r\\mid s}\\), to get \\(m_{t+1\\mid t+1}\\) and \\(P_{t+1\\mid t+1}\\), we first predict\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{m}}_{t+1\\mid t} &= M \\boldsymbol{\\mathbf{m}}_{t\\mid t}\\\\\nP_{t+1\\mid t} &= M P_{t\\mid t} M^\\intercal + \\Sigma_\\eta,\n\\end{split}\n\\tag{9}\\]\nthen we add our new information, \\(z_{t}\\), adjusted for the Kalman gain;\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1} &= \\boldsymbol{\\mathbf{m}}_{t+1\\mid t} + K_{t+1} \\boldsymbol{\\mathbf{e}}_{t+1}\\\\\nP_{t+1\\mid t+1} &= [I- K_{t+1}\\Phi_{t+1}]P_{t+1\\mid t}\n\\end{split}\n\\tag{10}\\]\nwhere \\(K_{t+1}\\) is the Kalman gain;\n\\[\\begin{split}\nK_{t+1} = P_{t+1\\mid t}\\Phi_{t+1}^\\intercal [\\Phi_{t+1} P_{t+1\\mid t} \\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}, \\quad t=0,\\dots,T-1\n\\end{split}\n\\]\nand \\(\\boldsymbol{\\mathbf{e}}_{t+1}\\) are the prediction errors\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{e}}_{t+1} = \\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1}-\\Phi_{t+1} \\boldsymbol{\\mathbf{m}}_{t+1\\mid t}, \\quad t=1,\\dots,T\n\\end{split}\n\\]\nStarting with \\(m_{0\\mid0} = m_0\\) and \\(P_{0\\mid0} =\\Sigma_0\\), we can then iteratively move across the data to eventually compute \\(m_{T\\mid T}\\) and \\(P_{T\\mid T}\\).\nAssuming Gaussian all random variables here are Guassian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of linear operators.\nWe can compute the marginal data likelihood alongside the kalman fiter using the prediciton errors \\(\\boldsymbol{\\mathbf{e}}_t\\). These, under the assumptions we have made about \\(\\eta\\) and \\(\\epsilon\\) being normal, are also normal with zero mean and variance\n\\[\\begin{split}\n\\mathbb V\\mathrm{ar}[\\boldsymbol{\\mathbf{e}}_t]=\\Sigma_t= \\Phi_{t} P_{t\\mid t-1} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon.\n\\end{split}\n\\]\nTherefore, the log-likelihood at each time is\n\\[\\begin{split}\n\\mathcal L(Z\\mid\\boldsymbol{\\mathbf{\\theta}}) = -\\frac12\\sum \\log\\det(\\Sigma_t(\\boldsymbol{\\mathbf{\\theta}})) - \\frac12 \\sum\\boldsymbol{\\mathbf{e}}_t(\\boldsymbol{\\mathbf{\\theta}})^\\intercal\\Sigma_{t}(\\boldsymbol{\\mathbf{\\theta}})^{-1} \\boldsymbol{\\mathbf{e}}_t(\\boldsymbol{\\mathbf{\\theta}}) - \\frac{n_{t}}{2}\\log(2*\\pi).\n\\end{split}\n\\]\nSumming these across time, we get the log likelihood for all the data.\nA simplified example of the kalman filter function, written to be jax compatible, used in the package is this;\n\n\nCode\n# TODO: Replace this with a simpler 'naive' implementation\n@jax.jit\ndef kalman_filter(\n    m_0: ArrayLike,\n    P_0: ArrayLike,\n    M: ArrayLike,\n    PHI_obs: ArrayLike,\n    Sigma_eta: ArrayLike,\n    Sigma_eps: ArrayLike,\n    ztildes: ArrayLike,  # data matrix, with time across columns\n) -&gt; tuple:\n    nbasis = m_0.shape[0]\n    nobs = ztildes.shape[0]\n\n    @jax.jit\n    def step(carry, z_t):\n        m_tt, P_tt, _, _, ll, _ = carry\n\n        # predict\n        m_pred = M @ m_tt\n        P_pred = M @ P_tt @ M.T + Sigma_eta\n\n        # Update\n\n        # Prediction Errors\n        eps_t = z_t - PHI_obs @ m_pred\n\n        Sigma_t = PHI_obs @ P_pred @ PHI_obs.T + Sigma_eps\n\n        # Kalman Gain\n        K_t = (\n            jnp.linalg.solve(Sigma_t, PHI_obs)\n            @ P_pred.T\n        ).T\n\n        m_up = m_pred + K_t @ eps_t\n\n        P_up = (jnp.eye(nbasis) - K_t @ PHI_obs) @ P_pred\n\n        # likelihood of epsilon, using cholesky decomposition\n        chol_Sigma_t = jnp.linalg.cholesky(Sigma_t)\n        z = jax.scipy.linalg.solve_triangular(chol_Sigma_t, eps_t)\n        ll_new = ll - jnp.sum(jnp.log(jnp.diag(chol_Sigma_t))\n                              ) - 0.5 * jnp.dot(z, z)\n\n        return (m_up, P_up, m_pred, P_pred, ll_new, K_t), (\n            m_up,\n            P_up,\n            m_pred,\n            P_pred,\n            ll_new,\n            K_t,\n        )\n\n    carry, seq = jl.scan(\n        step,\n        (m_0, P_0, m_0, P_0, 0, jnp.zeros((nbasis, nobs))),\n        ztildes.T,\n    )\n\n    return (carry[4], seq[0], seq[1], seq[2][1:], seq[3][1:], seq[5][1:])\n\n\nFor the documentation of the method proveded by the package, see [WORK OUT HOW TO LINK DO PAGES]"
  },
  {
    "objectID": "site/mathematics.html#the-information-filter",
    "href": "site/mathematics.html#the-information-filter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.2 The Information Filter",
    "text": "3.2 The Information Filter\nIn some computational scenarios, it is beneficial to work with vectors of consistent dimension. In python jax, the efficient scan and map operations work only with such operations; JAX has no support for jagged arrays, and traditional for loops with have long compile times when jit-compiled. Although there are some tools in JAX to get around this problem (namely the jax.tree functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it’s core, a scan-type operation, this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.\nBut it is possible to re-write the kalman filter in a way which is compatible with this kind of data. the sometimes called ‘information filter’ involves transforming the data into a kind of ‘information form’, which will always have consistent dimension.\nThe information filter is simply the kalman filter re-written to use the Gaussian distribution’s canonical parameters, those being the information vector and the information matrix. If a Gaussian distribution has mean \\(\\boldsymbol{\\mathbf{\\mu}}\\) and variance matrix \\(\\Sigma\\), then the corresponding information vector and information matrix is \\(\\nu = \\Sigma^{-1}\\mu\\) and \\(Q = \\Sigma^{-1}\\), correspondingly.\n\nTheorem 2 (The Information Filter) The Kalman filter can be rewritten in information form as follows (for example, Khan 2005). Write\n\\[\\begin{split}\nQ_{i\\mid j} &= P_{i\\mid j}\\\\\n\\boldsymbol{\\mathbf{\\nu}}_{i\\mid j} &= Q_{i\\mid j} \\boldsymbol{\\mathbf{m}}_{i\\mid j}\n\\end{split}\n\\]\nand transform the observations into their ‘information form’, for \\(t=1,\\dots, T\\)\n\\[\\begin{split}\nI_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1}\\Phi_{t},\\\\\ni_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\mathbf{z}}_{t}.\n\\end{split}\n\\tag{11}\\]\nThe prediction step now becomes\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} &= (I-J_t) M^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}\\\\\nQ_{t+1\\mid t} &= (I-J_t) S_{t}\n\\end{split}\n\\]\nwhere \\(S_t = M^{-\\intercal} Q_{t\\mid t} M^{-1}\\) and \\(J_t = S_t [S_{t}+\\Sigma_{\\eta}^{-1}]^{-1}\\).\nUpdating is now as simple as adding the information-form observations;\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t+1} &= \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} + i_{t+1}\\\\\n  Q_{t+1\\mid t+1} &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\n\nProof in Appendix (Section 6.2.)\nWe can see that the information form of the observations (Equation 11) will always have the same dimension (that being the process dimension, previously labelled \\(r\\), the number of basis functions used in the expansion). For our purposes, this means that jax.lax.scan will work after we ‘informationify’ the data, which can be done using jax.tree.map. This is implemented in the functions information_filter and information_filter_indep (for uncorrelated errors).\nThere are other often cited advantages to filtering in this form. It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with \\([S_t + \\Sigma_\\eta]^{-1}\\) in the process dimesnion instead of \\([\\Phi_t P_{t+1\\mid t} \\Phi_t^\\intercal + \\Sigma_\\epsilon]^{-1}\\) in the observation dimension) (Assimakis, Adam, and Douladiris 2012).\nThe other often mentioned advantage is the ability to use a truly vague prior for \\(\\alpha_0\\); that is, we can set \\(Q_0\\) as the zero matrix, without worriying about an infinite variance matrix. While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see (Section 6.3).\nAs with the kalman filter, it is also possible to get the data likelihood in-line as well. Again, we would like to stick with things in the state dimension, so working direclty with the prediction errors \\(\\boldsymbol{\\mathbf{e}}_t\\) should be avoided. Luckily, by multipliying the errors by \\(\\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\), we can define the ‘information errors’ \\(\\boldsymbol{\\mathbf{\\iota}}_t\\);\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{\\iota}}_t &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\boldsymbol{\\mathbf{e}}_t = \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_t -\\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_t m_{t\\mid t-1}\\\\\n  &= i_t - I_tQ_{t\\mid t-1}^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t\\mid t-1}.\n\\end{split}\n\\]\nThe variance of this quantity is also easy to find;\n\\[\\begin{split}\n  \\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\boldsymbol{\\mathbf{\\iota}}_t] &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\boldsymbol{\\mathbf{e}}_t]\\Sigma_\\epsilon^{-1}\\Phi_t\\\\\n  &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} [\\Phi_{t} P_{t\\mid t-1} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon] \\Sigma_\\epsilon^{-1}\\Phi_t\\\\\n  &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_{t} Q_{t\\mid t-1}^{-1} \\Phi_{t}^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_t \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_t\\\\\n  &= I_t Q_{t\\mid t-1}^{-1} I_t^\\intercal + I_t =: \\Sigma_{\\iota, t}.\n\\end{split}\n\\]\nNoting that \\(\\boldsymbol{\\mathbf{\\iota}}\\) clearly still has mean zero, this allows us once again to compute the log likelihood, this time through \\(\\boldsymbol{\\mathbf{\\iota}}\\)\n\\[\\begin{split}\n\\mathcal L(z_t\\mid\\boldsymbol{\\mathbf{\\theta}}) = -\\frac12\\sum \\log\\det(\\Sigma_{\\iota, t}(\\boldsymbol{\\mathbf{\\theta}})) - \\frac12 \\sum\\boldsymbol{\\mathbf{\\iota}}_t(\\boldsymbol{\\mathbf{\\theta}})^\\intercal\\Sigma_{\\iota, t}(\\boldsymbol{\\mathbf{\\theta}})^{-1} \\boldsymbol{\\mathbf{\\iota}}_t(\\boldsymbol{\\mathbf{\\theta}}) - \\frac{r}{2}\\log(2*\\pi).\n\\end{split}\n\\]"
  },
  {
    "objectID": "site/mathematics.html#kalman-smoothers",
    "href": "site/mathematics.html#kalman-smoothers",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.3 Kalman Smoothers",
    "text": "3.3 Kalman Smoothers\nBeyond the Kalman filters, we can also do Kalman smoothers. That is, filters estimate \\(\\boldsymbol{\\mathbf{m}}_{T\\mid T}\\) and \\(P_{T\\mid T}\\), but there is use for estimating \\(\\boldsymbol{\\mathbf{m}}_t\\mid T\\) and \\(P_{t\\mid T}\\) for all \\(t=0,\\dots, T\\).\nWe can then work backwards from these values using what is known as the Rauch-Tung-Striebel (RTS) smoother;\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{m}}_{t-1\\mid T} &= \\boldsymbol{\\mathbf{m}}_{t-1\\mid t-1} + J_{t-1}(\\boldsymbol{\\mathbf{m}}_{t\\mid T} - \\boldsymbol{\\mathbf{m}}_{t\\mid t-1}),\\\\\nP_{t-1\\mid T} &= P_{t-1\\mid t-1} + J_{t-1}(P_{t\\mid T} - P_{t\\mid t-1})J_{t-1}^\\intercal,\n\\end{split}\n\\tag{12}\\]\nwhere,\n\\[\\begin{split}\nJ_{t-1} = P_{t-1\\mid t-1}M^\\intercal[P_{t\\mid t-1}]^{-1}.\n\\end{split}\n\\]\nWe can clearly see, then, that it is crucial to keep the values in Equation 9.\nWe can then also compute the lag-one cross-covariance matrices \\(P_{t,t-1\\mid T}\\) using the Lag-One Covariance Smoother (is this what they call the RTS smoother?) From\n\\[\\begin{split}\nP_{T,T-1\\mid T} = (I - K_T\\Phi_{T}) MP_{T-1\\mid T-1},\n\\end{split}\n\\]\nwe can compute the lag-one covariances\n\\[\\begin{split}\nP_{t, t-1\\mid T} = P_{t\\mid t}J_{t-1}^\\intercal + J_{t}[P_{t+1,t\\mid T} - MP_{t-1\\mid t-1}]J_{t-1}^\\intercal\n\\end{split}\n\\tag{13}\\]\nThese values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later."
  },
  {
    "objectID": "site/mathematics.html#woodburys-identity",
    "href": "site/mathematics.html#woodburys-identity",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "6.1 Woodbury’s identity",
    "text": "6.1 Woodbury’s identity\nThe following two sections will make heavy use of the Woodbury identity.\n\nLemma 2 (Woodbury’s Identity) We have, for conformable matrices \\(A, U, C, V\\),\n\\[\\begin{split}\n(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.\n\\end{split}\n\\tag{16}\\]\nAdditionally, we have the variant\n\\[\\begin{split}\n(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.\n\\end{split}\n\\tag{17}\\]\n\n\nProof. We only prove (Equation 17), since various proofs of (Equation 16) are well known (see, for example, the wikipedia page).\nSimply multipliying (Equation 16) by \\(CU\\), (similar to Khan 2005, although there is an error in their proof)\n\\[\\begin{split}\n(A+UCV)^{-1}UC &= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\\\\n&= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\\\\n&= A^{-1}U(C^{-1}+VA^{-1}U)\n\\end{split}\n\\]\nas needed."
  },
  {
    "objectID": "site/mathematics.html#sec-app1",
    "href": "site/mathematics.html#sec-app1",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "6.2 Proof of Theorem 2",
    "text": "6.2 Proof of Theorem 2\n\nProof. Firstly, for the prediction step, using \\(S_t = M^{-\\intercal}Q_{t\\mid t}M^{-1}\\) and \\(J_t = S_t(\\Sigma_\\eta^{-1} + S_t)^{-1}\\) and the identities Equation 16 and Equation 17,\n\\[\\begin{split}\n  Q_{t+1\\mid t} &= P_{t+1\\mid t}^{-1} = (MQ_{t\\mid t}^{-1}M^\\intercal + \\Sigma_\\eta)^{-1}\\\\\n  &= S_t - J_t S_t = (I-J_t)S_t,\n\\end{split}\n\\]\nwhere we used \\(A=MQ_{t\\mid t}^{-1}M^\\intercal\\), \\(C=\\Sigma_\\eta\\) and \\(U=C=I\\) in Equation 16. Thurthermore,\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} &= Q_{t+1\\mid t} \\boldsymbol{\\mathbf{m}}_{t+1\\mid t}\\\\\n  &= Q_{t+1\\mid t} M Q_{t\\mid t}^{-1} \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t} = Q_{t+1\\mid t} (M Q_{t\\mid t}^{-1}) \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal}Q_{t\\mid t}M^{-1} (M Q_{t\\mid t}^{-1}) \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal} \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}.\n\\end{split}\n\\]\nFor the update step,\n\\[\\begin{split}\n  Q_{t+1\\mid t+1} &= P_{t+1\\mid t+1}^{-1}\\\\\n  &= (Q_{t+1}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}\\Sigma_\\epsilon\\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})^{-1}\\\\\n  &= ((Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1})^{-1})^{-1} = Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1}\\\\\n  &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\nThen, writing \\(\\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1}\\) in terms of \\(Q_{t+1\\mid t}\\) and \\(\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t}\\)\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1} &= Q_{t+1\\mid t}^{-1} \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1} [\\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1} - \\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mit t}]\\\\\n  &= (Q_{t+1\\mid t}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} \\\\\n  &\\quad + Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1}\\\\\n  &= [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t}\\\\\n  &\\quad + [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\Phi_{t+1}\\Sigma_\\epsilon^{-1}\\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1},\n\\end{split}\n\\]\nand now noting that \\(\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t+1} = (Q_{t+1\\mid t} + I_{t+1}) \\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1}\\), we complete the proof."
  },
  {
    "objectID": "site/mathematics.html#sec-vagueprior",
    "href": "site/mathematics.html#sec-vagueprior",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "6.3 Truly Vague Prior with the Kalman Filter",
    "text": "6.3 Truly Vague Prior with the Kalman Filter\nIt has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior \\(Q_{0}=0\\). While this is true, it is actually possible to do this in the Kalman filter by ‘skipping’ the first step (contrary to some sources, such as the wikipedia page as of January 2025).\n\nTheorem 3 In the Kalman Filter (Section 3.1), if we allow \\(P_{0}^{-1} = 0\\), effectively setting infinite variance, and assuming the propegator matrix \\(M\\) is invertible, we have\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1} \\Phi_1 \\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_1,\\\\\n  P_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}.\n\\end{split}\n\\tag{18}\\]\nTherefore, starting with these values then continuing the filter as normal, we can perform the kalman filter with ‘infinite’ prior variance.\n[NOTE: The requirement that M be invertible should be droppable, see the proof below]\n\n\nProof. Unsuprisingly, the proof is effectively equivalent to proving the information filter and setting \\(Q_0 = P_0^{-1}=0\\).\nFor the first predict step (Equation 9),\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid0} &= M \\boldsymbol{\\mathbf{m}}_0,\\\\\n  P_{1\\mid0} &= M P_0 M^\\intercal + \\Sigma_\\eta.\n\\end{split}\n\\]\nBy (Equation 16),\n\\[\\begin{split}\n  P_{1\\mid0}^{-1} &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (P_0^{-1} + M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} = 0.\n\\end{split}\n\\]\nSo, moving to the update step (Equation 10),\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid1} = M \\boldsymbol{\\mathbf{m}}_0 + P_{1\\mid0}\\Phi_1 [\\Phi_1 P_{1\\mid0} \\Phi_1^\\intercal + \\Sigma_\\epsilon]^{-1}(\\tilde{\\boldsymbol{\\mathbf{z}}}_1 - \\Phi M \\boldsymbol{\\mathbf{m}}_0).\\\\\n\\end{split}\n\\]\nApplying (Equation 17) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1, V=\\Phi_1^\\intercal, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid1} &= M \\boldsymbol{\\mathbf{m}}_0 + (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}(\\tilde{\\boldsymbol{\\mathbf{z}}}_1 - \\Phi_1 M\\boldsymbol{\\mathbf{m}}_0)\\\\\n  &= M \\boldsymbol{\\mathbf{m}}_0 + (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_1 - (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1M\\boldsymbol{\\mathbf{m}}_0\\\\\n  &= (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_1.\n\\end{split}\n\\]\nFor the variance, we apply the (Equation 16) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1^\\intercal, V=\\Phi_1, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  P_{1\\mid1} &= (I - P_{1\\mid0}\\Phi_1^\\intercal[\\Sigma_\\epsilon + \\Phi_1^\\intercal P_{1\\mid0}\\Phi_1]^{-1}\\Phi_1)P_{1\\mid0}\\\\\n  &= (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\\\\n  &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1},\n\\end{split}\n\\]\nas needed.\n\nIt is worth noting that (Equation 18) seems to make a lot of sense; namely, we expect the estimate for \\(\\boldsymbol{\\mathbf{m}}_0\\) to look like a correlated least squares-type estimator like this."
  },
  {
    "objectID": "site/mathematics.html#footnotes",
    "href": "site/mathematics.html#footnotes",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nat least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see Liu, Yeo, and Lu (2022)↩︎"
  }
]