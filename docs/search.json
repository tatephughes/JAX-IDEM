[
  {
    "objectID": "site/mathematics.html",
    "href": "site/mathematics.html",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "site/mathematics.html#process-decomposition",
    "href": "site/mathematics.html#process-decomposition",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.1 Process decomposition",
    "text": "3.1 Process decomposition\nChoose a complete class of spatial spectral basis functions, \\(\\phi_i(\\bv s)\\), and decompose the process spatial field at each time;\n\\[\\begin{split}\nY_t(\\bv s) \\approx \\sum_{i=1}^{r} \\alpha_{i,t} \\phi_i(\\bv s), \\quad t=0,\\dots,T.\n\\end{split}\n\\tag{3}\\]\nwhere we truncate the expansion at some \\(r\\in\\mathbb N\\). Notice that we can write this in vector/matrix form; considering times \\(t=1,2,\\dots, T\\), we set\n\\[\\begin{split}\n\\bv \\phi(\\bv s) &= (\\phi_1(\\bv s), \\phi_2(\\bv s), \\dots, \\phi_r(\\bv s))^{\\intercal},\\\\\n\\bv \\alpha_t &= (\\alpha_{1,t}, \\alpha_{2,t}, \\dots, \\alpha_{r, t})^{\\intercal}.\n\\end{split}\n\\tag{4}\\]\nNow, (Equation 3) gives us\n\\[\\begin{split}\nY(\\bv s; t) \\approx \\bv \\phi^{\\intercal}(\\bv s)  \\alpha(t).\\\\\n\\end{split}\n\\tag{5}\\]\nWe can effectively now work exclusively with \\(\\bv alpha_t\\). To do so, we need to find the evolution equation of \\(\\bv alpha_t\\), as given below.\n\nTheorem 1 (Spectral form of the state evolution) Define the Gram matrix;\n\\[\\Psi \\coloneq \\int_{\\mathcal D_s} \\bv \\phi(\\bv s) \\bv \\phi(\\bv s)^\\intercal d\\bv s.\n\\tag{6}\\]\nThen, the basis coefficients evolve by the equation\n\\[\\bv \\alpha_{t+1} = M \\bv\\alpha_t + \\bv\\eta_t,\n\\tag{7}\\]\nwhere \\(M = \\Psi^{-1} \\int\\int \\bv\\phi(\\bv s) \\kappa(\\bv s, \\bv r)\\bv\\phi(\\bv r)^\\intercal d\\bv r d \\bv s\\) and \\(\\bv\\eta_t =\\Psi^{-1} \\int \\bv \\phi(\\bv s)\\omega_t(s)d\\bv s\\).\n\n\nProof. (Adapting from Dewar, Scerri, and Kadirkamanathan 2008), write out the process equation, (Equation 2), using the first equation of (Equation 5);\n\\[Y_{t+1}(\\bv s) = \\bv \\phi(\\bv s)^{\\intercal} \\alpha_{t+1} = \\int_{\\mathcal D_s} \\kappa(\\bv s, \\bv r) \\bv\\phi(\\bv r)^{\\intercal}\\bv \\alpha_t d\\bv r + \\omega_t(\\bv s),\n\\]\nWe then multiply both sides by \\(\\bv \\phi(s)\\) and integrate over \\(\\bv s\\)\n\\[\\begin{split}\n\\int_{\\mathcal D_s} \\bv\\phi(\\bv s)\\bv\\phi(\\bv s)^{\\intercal} d\\bv s \\bv\\alpha_{t+1} &= \\int\\bv\\phi(\\bv s)\\int \\kappa(\\bv s, \\bv r)\\bv\\phi(\\bv r)^\\intercal d\\bv r  d \\bv s\\ \\bv\\alpha_t + \\int \\bv \\phi(\\bv s)\\omega_t(s)d\\bv s\\\\\n\\Psi \\bv\\alpha_{t+1} &= \\int\\int \\bv\\phi(\\bv s)\\kappa(\\bv s, \\bv r) \\bv\\phi(\\bv r)^\\intercal d\\bv r d \\bv s\\ \\bv\\alpha_t + \\int \\bv \\phi(\\bv s)\\omega_t(s)d\\bv s.\n\\end{split}\n\\]\nSo, finally, pre-multipling by the inverse of the gram matrix, \\(\\Psi^{-1}\\) (Equation 6), we arrive at the result."
  },
  {
    "objectID": "site/mathematics.html#spectral-form-of-the-process-noise",
    "href": "site/mathematics.html#spectral-form-of-the-process-noise",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.2 Spectral form of the Process Noise",
    "text": "3.2 Spectral form of the Process Noise\nWe still have to set out what the process noise, \\(\\omega_t(\\bv s)\\), and it’s spectral couterpart, \\(\\bv \\eta_t\\), are. Dewar, Scerri, and Kadirkamanathan (2008) fix the variance of \\(\\omega_t(\\bv s)\\) to be uniform and uncorrelated across space and time, with \\(\\omega_t(\\bv s) \\sim \\mathcal N(0,\\sigma^2)\\) It is then easily shown that \\(\\bv\\eta_t\\) is also normal, with \\(\\bv\\eta_t \\sim \\mathcal N(0, \\sigma^2\\Psi^{-1})\\).\nHowever, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of \\(\\bv\\eta_t\\), and compute the variance of \\(\\omega_t(\\bv s)\\) if needed.\n\nLemma 1 Let \\(\\bv\\eta_t \\sim \\mathcal N(0, \\Sigma_\\eta)\\), and \\(\\cov[\\bv\\eta_t, \\bv \\eta_{t+\\tau}] =0\\), \\(\\forall \\tau&gt;0\\). Then \\(\\omega_t(\\bv s)\\) has covariance\n\\[\\cov [\\omega_t(\\bv s), \\omega_{t+\\tau}(\\bv r)] = \\begin{cases}\n\\bv\\phi(\\bv s)^\\intercal \\Sigma_\\eta \\bv\\phi(\\bv r) & \\text{if }\\tau=0\\\\\n0 & \\text{else}\\\\\n\\end{cases}\n\\]\n\n\nProof. Consider \\(\\Psi \\bv\\eta_t\\), and consider the case \\(\\tau=0\\). It is clearly normal, with zero expectation and variance (using Equation 6),\n\\[\\begin{split}\n\\var[\\Psi \\bv\\eta_t] &= \\Psi \\var[\\bv\\eta_t] \\Psi^\\intercal = \\Psi\\Sigma_\\eta\\Psi^\\intercal,\\\\\n&= \\int_{\\mathcal D_s} \\bv\\phi(\\bv s) \\bv\\phi(\\bv s)^\\intercal d\\bv s \\  \\Sigma_\\eta \\ \\int_{\\mathcal D_s} \\bv\\phi(\\bv r) \\bv\\phi(\\bv r)^\\intercal d\\bv r\\\\\n&=  \\int\\int_{\\mathcal D_s^2} \\bv\\phi(\\bv s) \\bv\\phi(\\bv s)^\\intercal \\  \\Sigma_\\eta \\  \\bv\\phi(\\bv r) \\bv\\phi(\\bv r)^\\intercal d\\bv r d\\bv s\\\\\n\\end{split}\n\\tag{8}\\]\nSince it has zero expectation, we also have\n\\[\\begin{split}\n\\var[\\Psi\\bv\\eta_t] &= \\mathbb E[(\\Psi\\bv\\eta_t) (\\Psi\\bv\\eta_t)^\\intercal] = \\mathbb E[\\Psi\\bv\\eta_t\\bv\\eta_t^\\intercal\\Psi^\\intercal]\\\\\n&= \\mathbb E \\left[ \\int_{\\mathcal D_s} \\bv\\phi(\\bv s)\\omega_t(\\bv s)d\\bv s \\int_{\\mathcal D_s} \\bv \\phi(\\bv r)^\\intercal \\omega_t(\\bv r) d\\bv r \\right]\\\\\n&= \\int\\int_{\\mathcal D_s^2} \\bv\\phi(\\bv s)\\  \\mathbb E[\\omega_t(\\bv s)\\omega_t(\\bv r)]\\  \\bv \\phi(\\bv r)^\\intercal d\\bv s d \\bv r.\n\\end{split}\n\\tag{9}\\]\nWe can see that, comparing (Equation 8) and (Equation 9), we have\n\\[\\cov [\\omega_t(\\bv s), \\omega_t(\\bv r)] = \\mathbb E[\\omega_t(\\bv s)\\omega_t(\\bv r)]= \\bv\\phi(\\bv s)^\\intercal \\Sigma_\\eta \\bv\\phi(\\bv r).\n\\]\nSince, once again, \\(\\mathbb E[\\bv\\omega_t(\\bv s)]=0\\).\nFor the \\(\\tau\\neq0\\) case, it is simple to show that the covariance is 0."
  },
  {
    "objectID": "site/mathematics.html#sec-kerneldecomp",
    "href": "site/mathematics.html#sec-kerneldecomp",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.3 Kernel Parameterisations",
    "text": "3.3 Kernel Parameterisations\nNext is the part of the system, which defines the dynamics; the kernelf function, \\(\\kappa\\). There are a few ways to handle the kernel. One of the most obvious is to expand it out into a spectral decomposition as well;\n\\[\\kappa \\approx \\sum_i \\beta_i\\psi(\\bv s, \\bv r).\n\\]\nThis can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on \\(\\mathbb R^2\\times \\mathbb R^2\\); to get a wide enough space of possible functions, we would likely need many terms in the spectral expansion.\nA much simpler approach would be to simply parameterise the kernel function, to \\(\\kappa(\\bv s, \\bv r, \\bv \\theta_\\kappa)\\). We then establish a simple shape for the kernel (e.g. Gaussian) and rely on very few parameters (for example, scale, shape, offsets). The example kernel used in the jaxidem is a Gaussian-shape kernel;\n\\[\\kappa(\\bv s, \\bv r; \\bv m, a, b) = a \\exp \\left( -\\frac{1}{b} \\vert \\bv s- \\bv r +\\bv m\\vert^2 \\right).\n\\]\nOf course, this kernel lacks spatial dependance. We can add spatial variance back by adding dependance on \\(\\bv s\\) to the parameters, for example, variyng the offset term as \\(\\bv m(\\bv s)\\). Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground (Cressie and Wikle 2015). The actual parameters of such a spatially-variant kernel are then the spectral coefficients for the expansion of any spatially variant parameters, as well as any constant parameters. This is precisely what is plotting in Figure 1, where the spectral coeficients are randomly sampled from a multivariate normal distribution;\n\\[\\begin{split}\n  \\bv m(\\bv s) = \\left(\\begin{matrix}\n    \\sum_{i=1}^{r_m} \\phi_{\\kappa,i}(\\bv s) m^{(x)}_i\\\\\n    \\sum_{i=1}^{r_m} \\phi_{\\kappa,i}(\\bv s) m^{(y)}_i\n  \\end{matrix}\\right),\n\\end{split}\n\\]\nwhere \\(m^{(x)}_i\\) and \\(m^{(y)}_i\\) are coefficients for the x and y coordinates respectively, and \\(\\phi_{\\kappa, i}(\\bv s)\\) are basis functions (e.g. bisquare functions in Figure 1)."
  },
  {
    "objectID": "site/mathematics.html#idem-as-a-linear-dynamical-system",
    "href": "site/mathematics.html#idem-as-a-linear-dynamical-system",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.4 IDEM as a linear dynamical system",
    "text": "3.4 IDEM as a linear dynamical system\nTo summarise, we have taken a truncated spectral decompostion to write the Integro-difference equation model as a more traditional linear dynamical system form (Equation 7). All that is left is to include our observations in our system.\nLets assume that at each time \\(t\\) there are \\(n_t\\) observations at locations \\(\\bv s_{1,t},\\dots, \\bv s_{n_{t},t}\\). We write the vector of the process at these points as \\(\\bv Y(t) = (Y(s_{1,t};t), \\dots, Y(s_{n_{t},t};t))^\\intercal\\), and, in it’s expanded form \\(\\bv Y_t = \\Phi_t \\bv\\alpha_t\\), where \\(\\Phi \\in \\mathbb R^{r\\times n_{t}}\\) is\n\\[\\begin{split}\n\\{\\Phi_{t}\\}_{i, j} = \\phi_{i}(s_{j,t}).\n\\end{split}\n\\]\nFor the covariates, we write the matrix \\(X_t = (\\bv X(\\bv s_{1, t}), \\dots, \\bv X(\\bv s_{1=n_{t}, t})^\\intercal\\). We then have\n\\[\\begin{split}\n\\bv Z_t &= \\Phi \\alpha_t + X_{t} \\bv \\beta + \\bv \\epsilon_t, \\quad t = 1,\\dots, T,\\\\\n\\bv \\alpha_{t+1} &= M\\bv \\alpha_t + \\bv\\eta_t,\\quad t = 0,2,\\dots, T-1,\\\\\nM &= \\int_{\\mathcal D_s}\\bv\\phi(\\bv s) \\bv\\phi(\\bv s)^\\intercal d\\bv s \\int_{\\mathcal D_s^2}\\bv\\phi(\\bv s) \\kappa(\\bv s, \\bv r; \\bv\\theta_\\kappa)\\bv\\phi(\\bv r)^\\intercal d\\bv r d \\bv s,\n\\end{split}\n\\]\nWriting \\(\\tilde{\\bv{Z}}_t = \\bv Z_t - X_t \\bv \\beta\\),\n\\[\\begin{split}\n\\tilde{\\bv Z}_t &= \\Phi_{t} \\bv \\alpha_t + \\bv \\epsilon_t,\\quad &t = 1,2,\\dots, T,\\\\\n\\bv \\alpha_{t+1} &= M \\bv \\alpha_t + \\bv\\eta_t,\\quad &t = 0,1, \\dots, T.\\\\\n\\end{split}\n\\tag{10}\\]\nWe should also initialise \\(\\bv \\alpha_0 \\sim \\mathcal N^{r}(\\bv m_{0}, \\Sigma_{0})\\), and fix simple distrubtions to the noise terms,\n\\[\\begin{split}\n\\epsilon_{t,i} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\sigma^2_\\epsilon),\\\\\n\\eta_{t,i} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\sigma^2_\\eta),\n\\end{split}\n\\]\nwhich are (also) independant in time.\nAs in, for example, (Wikle and Cressie 1999), Equation 10 is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood. We can use these quantities in either an EM algorithm or a Bayesian approach, or directly maximise the marginal data likelihood\nWe now move on to an example simulation of this kind of model using it’s spectral decomposition and jaxidem."
  },
  {
    "objectID": "site/mathematics.html#example-simulation",
    "href": "site/mathematics.html#example-simulation",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.5 Example Simulation",
    "text": "3.5 Example Simulation\nWe can now use the above to simulate easily from such models; once we have chosen the appropriate decompositions, we simply compute \\(M\\) and propegate \\(\\bv \\alpha_t\\) as we would when simulating any other linear dynamic system. We then use the spectral coefficients to generate \\(Y_t(\\bv s)\\) and \\(Z_t(\\bv s)\\) in the obvious way.\njaxidem implements this in the function sim_idem, or through the more user-friendly method idem.IDEM.simulate. An object of the idemModel class contains all the necesary information about basis decompositions, and the simulate methods calls simIDEM without comprimising its jit-ability (although just-in-time computation obviously isn’t as important for simulation, the jit-ed function could save compile time if someone want to simulate from many models).\nThe gen_example_idem method creates a simple IDEM object without many required parameters;\n\n\nCode\nkey = jax.random.PRNGKey(1)\nkeys = rand.split(key, 2)\n\nmodel = idem.gen_example_idem(keys[0], k_spat_inv=False)\n\nprocess_data, obs_data = model.simulate(keys[1], T=3, nobs=50)\n\n\nThe resulting objects are of class st_data, containing a couple of niceties for handling spatio-temporal data, while still storing all data as JAX arrays. For example, the show_plot, save_plot and save_gif methods provide easy plotting;\n\n\nCode\nprocess_data.save_plot('process_data_example.png')\nobs_data.save_plot('obs_data_example.png')\n\n\n\n\n\n\n\n\n\n\nProcess Simulation\n\n\n\n\n\n\n\n\n\nObservation Simulation\n\n\n\n\n\n\nFigure 2: Example simulations from an Integro-difference Equation Model. Kernel is generated with spatially varying flow terms, generated by bisquare basis functions with randomly generated coefficient. Not that some artifacts from the decomposition are visible, such as a faint checkerboard pattern in the process."
  },
  {
    "objectID": "site/mathematics.html#sec-kalmanfilter",
    "href": "site/mathematics.html#sec-kalmanfilter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.1 The Kalman Filter",
    "text": "4.1 The Kalman Filter\nFirstly, we should establish some notation. Write\n\\[\\begin{split}\nm_{i \\mid j} &= \\mathbb E[\\bv\\alpha_i \\mid \\{\\bv Z_t=\\bv z_t\\}_{t=0,\\dots,j}],\\\\\nP_{i \\mid j} &= \\var[\\bv\\alpha_i \\mid \\{\\bv Z_t=\\bv z_t\\}_{t=0,\\dots,j}],\\\\\nP_{i,j \\mid k} &= \\cov[\\bv\\alpha_i, \\bv\\alpha_k \\mid \\{\\bv Z_t=\\bv z_t\\}_{t=0,\\dots,k}].\n\\end{split}\n\\]\nFor the initial terms, we choose bayesian-like prior moments \\(m_{0\\mid0}=m_0\\) and \\(P_{0\\mid0}=\\Sigma_0\\). For convenience and generality, we write \\(\\Sigma_\\eta\\) and \\(\\Sigma_\\epsilon\\) for the variance matrices of the process and observations. Note that, if the number of observations change at each time point (for example, due to missing data), then \\(\\Sigma_\\epsilon\\) should be time variyng; we could either always keep it as uncorrelated so that \\(\\Sigma_\\epsilon = \\mathrm{diag} (\\sigma_\\epsilon^2)\\), or perhaps put some kind of distance-dependant covariance function to it.\nTo move the filter forward, that is, given \\(m_{t\\mid t}\\) and \\(P_{t\\mid t}\\), to get \\(m_{t+1\\mid t+1}\\) and \\(P_{t+1\\mid t+1}\\), we first predict\n\\[\\begin{split}\n\\bv m_{t+1\\mid t} &= M \\bv m_{t\\mid t},\\\\\nP_{t+1\\mid t} &= M P_{t\\mid t} M^\\intercal + \\Sigma_\\eta,\n\\end{split}\n\\tag{11}\\]\nthen we add our new information, \\(z_{t}\\);\n\\[\\begin{split}\n\\bv m_{t+1\\mid t+1} &= \\bv m_{t+1\\mid t} + K_{t+1} \\bv e_{t+1}\\\\\nP_{t+1\\mid t+1} &= [I- K_{t+1}\\Phi_{t+1}]P_{t+1\\mid t}\n\\end{split}\n\\tag{12}\\]\nwhere \\(K_{t+1}\\) is the Kalman gain;\n\\[\\begin{split}\nK_{t+1} = P_{t+1\\mid t}\\Phi_{t+1}^\\intercal [\\Phi_{t+1} P_{t+1\\mid t} \\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}, \\quad t=0,\\dots,T-1\n\\end{split}\n\\]\nand \\(\\bv e_{t+1}\\) are the prediction errors\n\\[\\begin{split}\n\\bv e_{t+1} = \\tilde{\\bv z}_{t+1}-\\Phi_{t+1} \\bv m_{t+1\\mid t}, \\quad t=1,\\dots,T\n\\end{split}\n\\]\nStarting with \\(m_0\\) and \\(P_0\\), we can then iteratively move across the data to eventually compute \\(m_{T\\mid T}\\) and \\(P_{T\\mid T}\\).\nAssuming Gaussian all random variables here are Guassian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of linear operators.\nWe can compute the marginal data likelihood alongside the kalman fiter using the prediciton errors \\(\\bv e_t\\). These, under the assumptions we have made about \\(\\bv \\eta_t\\) and \\(\\bv\\epsilon_t\\) being normal, are also normal with zero mean and variance\n\\[\\begin{split}\n\\mathbb V\\mathrm{ar}[\\bv e_t]=\\Sigma_t= \\Phi_{t} P_{t\\mid t-1} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon.\n\\end{split}\n\\]\nTherefore, the log-likelihood at each time is\n\\[\\begin{split}\n\\mathcal L(Z\\mid\\bv\\theta) = -\\frac12\\sum \\log\\det(\\Sigma_t(\\bv\\theta)) - \\frac12 \\sum\\bv e_t(\\bv\\theta)^\\intercal\\Sigma_{t}(\\bv\\theta)^{-1} \\bv e_t(\\bv\\theta) - \\frac{n_{t}}{2}\\log(2*\\pi).\n\\end{split}\n\\]\nSumming these across time, we get the log likelihood for all the data.\nA simplified example of the kalman filter function, written to be jax compatible, used in the package is this;\n\n\nCode\n@jax.jit\ndef kalman_filter(m_, P_0, M, PHI_obs, Sigma_eta, Sigma_eps, ztildes):\n    nbasis = m_0.shape[0]\n    nobs = ztildes.shape[0]\n\n    @jax.jit\n    def step(carry, z_t):\n        m_tt, P_tt, _, _, ll, _ = carry\n\n        # predict\n        m_pred = M @ m_tt\n        P_pred = M @ P_tt @ M.T + Sigma_eta\n\n        # Update\n        # Prediction Errors\n        eps_t = z_t - PHI_obs @ m_pred\n\n        Sigma_t = PHI_obs @ P_pred @ PHI_obs.T + Sigma_eps\n        # Kalman Gain\n        K_t = (jnp.linalg.solve(Sigma_t, PHI_obs)@ P_pred.T).T\n\n        m_up = m_pred + K_t @ eps_t\n        P_up = (jnp.eye(nbasis) - K_t @ PHI_obs) @ P_pred\n\n        # likelihood of epsilon, using cholesky decomposition\n        ll_new = ll - 0.5 * n * jnp.log(2*jnp.pi) - \\\n            0.5 * jnp.log(jnp.linalg.det(Sigma_t)) -\\\n            0.5 * e.T @ jnp.linalg.solve(Sigma_t, e)\n\n        return (m_up, P_up, m_pred, P_pred, ll_new, K_t), (m_up, P_up, m_pred, P_pred, ll_new, K_t,)\n\n    carry, seq = jl.scan(\n        step,\n        (m_0, P_0, m_0, P_0, 0, jnp.zeros((nbasis, nobs))),\n        ztildes.T,\n    )\n\n    return (carry[4], seq[0], seq[1], seq[2][1:], seq[3][1:], seq[5][1:])\n\n\nFor the documentation of the method proveded by the package, see [WORK OUT HOW TO LINK DO PAGES]"
  },
  {
    "objectID": "site/mathematics.html#the-information-filter",
    "href": "site/mathematics.html#the-information-filter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.2 The Information Filter",
    "text": "4.2 The Information Filter\nIn some computational scenarios, it is beneficial to work with vectors of consistent dimension. In Python JAX, the efficient scan method works only with such arrays; JAX has no support for jagged arrays, and traditional for loops will likely lead to long compile times when jit-compiled. Although there are some tools in JAX to get around this problem (namely the jax.tree functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it’s core, a scan-type operation (scanning over the data), this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.\nBut it is possible to re-write the Kalman filter in a way which is compatible with this kind of data. The ‘information filter’ (sometimes called inverse Kalman filter or other names) involves transforming the data into its ‘information form’, which will always have consistent dimension, allowing us to avoid jagged scans.\nThe information filter is simply the Kalman filter re-written to use the Gaussian distribution’s canonical parameters 3, those being the information vector and the information matrix. If a Gaussian distribution has mean \\(\\bv\\mu\\) and variance matrix \\(\\Sigma\\), then the corresponding information vector and information matrix is \\(\\nu = \\Sigma^{-1}\\mu\\) and \\(Q = \\Sigma^{-1}\\), correspondingly.\n\nTheorem 2 (The Information Filter) The Kalman filter can be rewritten in information form as follows (for example, Khan 2005). Write\n\\[\\begin{split}\nQ_{i\\mid j} &= P_{i\\mid j}\\\\\n\\bv\\nu_{i\\mid j} &= Q_{i\\mid j} \\bv m_{i\\mid j}\n\\end{split}\n\\]\nand transform the observations into their ‘information form’, for \\(t=1,\\dots, T\\)\n\\[\\begin{split}\nI_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1}\\Phi_{t},\\\\\ni_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1} \\bv z_{t}.\n\\end{split}\n\\tag{13}\\]\nThe prediction step now becomes\n\\[\\begin{split}\n\\bv\\nu_{t+1\\mid t} &= (I-J_t) M^{-1}\\bv\\nu_{t\\mid t}\\\\\nQ_{t+1\\mid t} &= (I-J_t) S_{t}\n\\end{split}\n\\]\nwhere \\(S_t = M^{-\\intercal} Q_{t\\mid t} M^{-1}\\) and \\(J_t = S_t [S_{t}+\\Sigma_{\\eta}^{-1}]^{-1}\\).\nUpdating is now as simple as adding the information-form observations;\n\\[\\begin{split}\n  \\bv\\nu_{t+1\\mid t+1} &= \\bv\\nu_{t+1\\mid t} + i_{t+1}\\\\\n  Q_{t+1\\mid t+1} &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\n\nProof in Appendix (Section 7.2.)\nWe can see that the information form of the observations (Equation 13) will always have the same dimension 4. For our purposes, this means that jax.lax.scan will work after we ‘informationify’ the data, which can be done using jax.tree.map. This is implemented in the functions information_filter and information_filter_indep (for uncorrelated errors).\nThere are other often cited advantages to filtering in this form. It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with \\([S_t + \\Sigma_\\eta]^{-1}\\) in the process dimension instead of \\([\\Phi_t P_{t+1\\mid t} \\Phi_t^\\intercal + \\Sigma_\\epsilon]^{-1}\\) in the observation dimension) (Assimakis, Adam, and Douladiris 2012).\nThe other often mentioned advantage is the ability to use a flat prior for \\(\\alpha_0\\); that is, we can set \\(Q_0\\) as the zero matrix, without worriying about an infinite variance matrix. While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see Section 7.3.\nAs with the kalman filter, it is also possible to get the data likelihood in-line as well. Again, we would like to stick with things in the state dimension, so working direclty with the prediction errors \\(\\bv e_t\\) should be avoided. Luckily, by multipliying the errors by \\(\\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\), we can define the ‘information errors’ \\(\\bv \\iota_t\\);\n\\[\\begin{split}\n  \\bv \\iota_t &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\bv e_t = \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\tilde{\\bv z}_t -\\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_t m_{t\\mid t-1}\\\\\n  &= i_t - I_tQ_{t\\mid t-1}^{-1}\\bv \\nu_{t\\mid t-1}.\n\\end{split}\n\\]\nThe variance of this quantity is also easy to find;\n\\[\\begin{split}\n  \\var[\\bv \\iota_t] &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\var[\\bv e_t]\\Sigma_\\epsilon^{-1}\\Phi_t\\\\\n  &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} [\\Phi_{t} P_{t\\mid t-1} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon] \\Sigma_\\epsilon^{-1}\\Phi_t\\\\\n  &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_{t} Q_{t\\mid t-1}^{-1} \\Phi_{t}^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_t \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_t\\\\\n  &= I_t Q_{t\\mid t-1}^{-1} I_t^\\intercal + I_t =: \\Sigma_{\\iota, t}.\n\\end{split}\n\\]\nNoting that \\(\\bv \\iota\\) clearly still has mean zero, this allows us once again to compute the log likelihood, this time through \\(\\bv\\iota\\)\n\\[\\begin{split}\n\\mathcal L(z_t\\mid\\bv\\theta) = -\\frac12\\sum \\log\\det(\\Sigma_{\\iota, t}(\\bv\\theta)) - \\frac12 \\sum\\bv \\iota_t(\\bv\\theta)^\\intercal\\Sigma_{\\iota, t}(\\bv\\theta)^{-1} \\bv \\iota_t(\\bv\\theta) - \\frac{r}{2}\\log(2*\\pi).\n\\end{split}\n\\]"
  },
  {
    "objectID": "site/mathematics.html#smoothing",
    "href": "site/mathematics.html#smoothing",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.3 Smoothing",
    "text": "4.3 Smoothing\nBeyond the filtering, another task is smoothing. That is, filters estimate \\(\\bv m_{T\\mid T}\\) and \\(P_{T\\mid T}\\), but there is use for estimating \\(\\bv m_{t\\mid T}\\) and \\(P_{t\\mid T}\\) for all \\(t=0,\\dots, T\\).\nWe simply work backwards from \\(\\bv m_{T\\mid T}\\) and \\(P_{T\\mid T}\\) values using what is known as the Rauch-Tung-Striebel (RTS) smoother;\n\\[\\begin{split}\n\\bv m_{t-1\\mid T} &= \\bv m_{t-1\\mid t-1} + J_{t-1}(\\bv m_{t\\mid T} - \\bv m_{t\\mid t-1}),\\\\\nP_{t-1\\mid T} &= P_{t-1\\mid t-1} + J_{t-1}(P_{t\\mid T} - P_{t\\mid t-1})J_{t-1}^\\intercal,\n\\end{split}\n\\tag{14}\\]\nwhere,\n\\[\\begin{split}\nJ_{t-1} = P_{t-1\\mid t-1}M^\\intercal[P_{t\\mid t-1}]^{-1}.\n\\end{split}\n\\]\nWe can clearly see, then, that it is crucial to keep the values in Equation 11.\nWe can then also compute the lag-one cross-covariance matrices \\(P_{t,t-1\\mid T}\\) using the Lag-One Covariance Smoother. This will b useful, for example, in the expectation-maximisation algorithm later. From\n\\[\\begin{split}\nP_{T,T-1\\mid T} = (I - K_T\\Phi_{T}) MP_{T-1\\mid T-1},\n\\end{split}\n\\]\nwe can compute the lag-one covariances\n\\[\\begin{split}\nP_{t, t-1\\mid T} = P_{t\\mid t}J_{t-1}^\\intercal + J_{t}[P_{t+1,t\\mid T} - MP_{t-1\\mid t-1}]J_{t-1}^\\intercal\n\\end{split}\n\\tag{15}\\]\nThese values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later."
  },
  {
    "objectID": "site/mathematics.html#woodburys-identity",
    "href": "site/mathematics.html#woodburys-identity",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "7.1 Woodbury’s identity",
    "text": "7.1 Woodbury’s identity\nThe following two sections will make heavy use of the Woodbury identity.\n\nLemma 2 (Woodbury’s Identity) We have, for conformable matrices \\(A, U, C, V\\),\n\\[\\begin{split}\n(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.\n\\end{split}\n\\tag{18}\\]\nAdditionally, we have the variant\n\\[\\begin{split}\n(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.\n\\end{split}\n\\tag{19}\\]\n\n\nProof. We only prove (Equation 19), since various proofs of (Equation 18) are well known (see, for example, the wikipedia page).\nSimply multipliying (Equation 18) by \\(CU\\), (similar to Khan 2005, although there is an error in their proof)\n\\[\\begin{split}\n(A+UCV)^{-1}UC &= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\\\\n&= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\\\\n&= A^{-1}U(C^{-1}+VA^{-1}U)\n\\end{split}\n\\]\nas needed."
  },
  {
    "objectID": "site/mathematics.html#sec-app1",
    "href": "site/mathematics.html#sec-app1",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "7.2 Proof of Theorem 2",
    "text": "7.2 Proof of Theorem 2\n\nProof. Firstly, for the prediction step, using \\(S_t = M^{-\\intercal}Q_{t\\mid t}M^{-1}\\) and \\(J_t = S_t(\\Sigma_\\eta^{-1} + S_t)^{-1}\\) and the identities Equation 18 and Equation 19,\n\\[\\begin{split}\n  Q_{t+1\\mid t} &= P_{t+1\\mid t}^{-1} = (MQ_{t\\mid t}^{-1}M^\\intercal + \\Sigma_\\eta)^{-1}\\\\\n  &= S_t - J_t S_t = (I-J_t)S_t,\n\\end{split}\n\\]\nwhere we used \\(A=MQ_{t\\mid t}^{-1}M^\\intercal\\), \\(C=\\Sigma_\\eta\\) and \\(U=C=I\\) in Equation 18. Thurthermore,\n\\[\\begin{split}\n  \\bv \\nu_{t+1\\mid t} &= Q_{t+1\\mid t} \\bv m_{t+1\\mid t}\\\\\n  &= Q_{t+1\\mid t} M Q_{t\\mid t}^{-1} \\bv \\nu_{t\\mid t} = Q_{t+1\\mid t} (M Q_{t\\mid t}^{-1}) \\bv \\nu_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal}Q_{t\\mid t}M^{-1} (M Q_{t\\mid t}^{-1}) \\bv \\nu_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal} \\bv \\nu_{t\\mid t}.\n\\end{split}\n\\]\nFor the update step,\n\\[\\begin{split}\n  Q_{t+1\\mid t+1} &= P_{t+1\\mid t+1}^{-1}\\\\\n  &= (Q_{t+1}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}\\Sigma_\\epsilon\\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})^{-1}\\\\\n  &= ((Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1})^{-1})^{-1} = Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1}\\\\\n  &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\nThen, writing \\(\\bv m_{t+1\\mid t+1}\\) in terms of \\(Q_{t+1\\mid t}\\) and \\(\\bv \\nu_{t+1\\mid t}\\)\n\\[\\begin{split}\n  \\bv m_{t+1\\mid t+1} &= Q_{t+1\\mid t}^{-1} \\bv \\nu_{t+1\\mid t} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1} [\\tilde{\\bv z}_{t+1} - \\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\bv \\nu_{t+1\\mit t}]\\\\\n  &= (Q_{t+1\\mid t}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})\\bv \\nu_{t+1\\mid t} \\\\\n  &\\quad + Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\tilde{\\bv z}_{t+1}\\\\\n  &= [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\bv \\nu_{t+1\\mid t}\\\\\n  &\\quad + [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\Phi_{t+1}\\Sigma_\\epsilon^{-1}\\tilde{\\bv z}_{t+1},\n\\end{split}\n\\]\nand now noting that \\(\\bv\\nu_{t+1\\mid t+1} = (Q_{t+1\\mid t} + I_{t+1}) \\bv m_{t+1\\mid t+1}\\), we complete the proof."
  },
  {
    "objectID": "site/mathematics.html#sec-vagueprior",
    "href": "site/mathematics.html#sec-vagueprior",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "7.3 Truly Vague Prior with the Kalman Filter",
    "text": "7.3 Truly Vague Prior with the Kalman Filter\nIt has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior \\(Q_{0}=0\\). While this is true, it is actually possible to do this in the Kalman filter by ‘skipping’ the first step (contrary to some sources, such as the wikipedia page as of January 2025).\n\nTheorem 3 In the Kalman Filter (Section 4.1), if we allow \\(P_{0}^{-1} = 0\\), effectively setting infinite variance, and assuming the propegator matrix \\(M\\) is invertible, we have\n\\[\\begin{split}\n  \\bv m_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1} \\Phi_1 \\Sigma_\\epsilon^{-1} \\tilde{\\bv z}_1,\\\\\n  P_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}.\n\\end{split}\n\\tag{20}\\]\nTherefore, starting with these values then continuing the filter as normal, we can perform the kalman filter with ‘infinite’ prior variance.\n[NOTE: The requirement that M be invertible should be droppable, see the proof below]\n\n\nProof. Unsuprisingly, the proof is effectively equivalent to proving the information filter and setting \\(Q_0 = P_0^{-1}=0\\).\nFor the first predict step (Equation 11),\n\\[\\begin{split}\n  \\bv m_{1\\mid0} &= M \\bv m_0,\\\\\n  P_{1\\mid0} &= M P_0 M^\\intercal + \\Sigma_\\eta.\n\\end{split}\n\\]\nBy (Equation 18),\n\\[\\begin{split}\n  P_{1\\mid0}^{-1} &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (P_0^{-1} + M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} = 0.\n\\end{split}\n\\]\nSo, moving to the update step (Equation 12),\n\\[\\begin{split}\n  \\bv m_{1\\mid1} = M \\bv m_0 + P_{1\\mid0}\\Phi_1 [\\Phi_1 P_{1\\mid0} \\Phi_1^\\intercal + \\Sigma_\\epsilon]^{-1}(\\tilde{\\bv{z}}_1 - \\Phi M \\bv m_0).\\\\\n\\end{split}\n\\]\nApplying (Equation 19) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1, V=\\Phi_1^\\intercal, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  \\bv m_{1\\mid1} &= M \\bv m_0 + (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}(\\tilde{\\bv{z}}_1 - \\Phi_1 M\\bv m_0)\\\\\n  &= M \\bv m_0 + (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\bv{z}}_1 - (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1M\\bv m_0\\\\\n  &= (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\bv{z}}_1.\n\\end{split}\n\\]\nFor the variance, we apply the (Equation 18) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1^\\intercal, V=\\Phi_1, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  P_{1\\mid1} &= (I - P_{1\\mid0}\\Phi_1^\\intercal[\\Sigma_\\epsilon + \\Phi_1^\\intercal P_{1\\mid0}\\Phi_1]^{-1}\\Phi_1)P_{1\\mid0}\\\\\n  &= (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\\\\n  &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1},\n\\end{split}\n\\]\nas needed. \n\nIt is worth noting that (Equation 20) seems to make a lot of sense; namely, we expect the estimate for \\(\\bv m_0\\) to look like a correlated least squares-type estimator like this."
  },
  {
    "objectID": "site/mathematics.html#footnotes",
    "href": "site/mathematics.html#footnotes",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHistorically, this has been abbrevited as IDE. However, with that abbreviation almost universally meaning ‘Integrated Development Environment’, here, we choose to include the ‘M’ in the abbreviation.↩︎\nat least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see Liu, Yeo, and Lu (2022)↩︎\nthat is, the parameters of the Gaussian distribution in it’s exponential family form↩︎\nthat being the process dimension, previously labelled \\(r\\), the number of basis functions used in the expansion of the process↩︎"
  },
  {
    "objectID": "site/filtering_and_smoothing.html",
    "href": "site/filtering_and_smoothing.html",
    "title": "Filtering in JAX-IDEM",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "site/filtering_and_smoothing.html#the-simple-model",
    "href": "site/filtering_and_smoothing.html#the-simple-model",
    "title": "Filtering in JAX-IDEM",
    "section": "1.1 The simple model",
    "text": "1.1 The simple model\nConsider the simple system, for \\(t=1,\\dots,T\\)\n\\[\\begin{split}\n\\vec\\alpha_{t+1} &= M\\vec\\alpha_t + \\vec\\eta_t,\\\\\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\alpha_0 = (1,1)^\\intercal\\) and\n\\[\\begin{split}\nM = \\left[\\begin{matrix}\n    \\cos(0.3) & -\\sin(0.3)\\\\\n    \\sin(0.3) & \\sin(0.3)\n\\end{matrix}\\right].\n\\end{split}\n\\tag{2}\\]\nThe error terms are mutually independant and have variances \\(\\sigma^{2}_\\epsilon=0.02\\) and \\(\\sigma^{2}_{\\eta}=0.03\\) and \\(\\vec z_t\\) are transformed linear ‘observations’ of \\(\\vec\\alpha\\)\n\\[\\begin{split}\n\\vec z_t &= \\Phi \\vec\\alpha_t + \\vec\\epsilon_t,\\\\\n\\Phi &= \\left[\\begin{matrix}\n1   & 0  \\\\\n0.6 & 0.4\\\\\n0.4 & 0.6\n\\end{matrix}\\right]\n\\end{split}.\n\\tag{3}\\]\nThe process, \\(\\alpha\\), simply spins in a circle with some noise. Lets simulate from this system;\n\n\nCode\nimport sys\nimport os\nsys.path.append(os.path.abspath('../src/jaxidem'))\n\nimport jax.random as rand\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nimport filter_smoother_functions as fsf\nimport testfuncs\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objs as go\nimport pandas as pd\n\njax.config.update('jax_enable_x64', True)\n\n\n\n\nCode\nkey = jax.random.PRNGKey(1)\n\n\nalpha_0 = jnp.ones(2)  # 2D, easily plottable\nM = jnp.array([[jnp.cos(0.3), -jnp.sin(0.3)],\n              [jnp.sin(0.3), jnp.cos(0.3)]])  # spinny\n\nalphas = [alpha_0]\nzs = []\n\nT = 50\nkeys = rand.split(key, T*2)\n\nsigma2_eta = 0.001\nSigma_eta = sigma2_eta*jnp.eye(2)\nchol_s_eta = jax.scipy.linalg.cholesky(Sigma_eta, lower=True)\nsigma2_eps = 0.01\nSigma_eps = sigma2_eps*jnp.eye(3)\nchol_s_eps = jax.scipy.linalg.cholesky(Sigma_eps, lower=True)\nPHI = jnp.array([[1, 0], [0.6, 0.4], [0.4, 0.6]])\n\nfor i in range(T):\n    alphas.append(M@alphas[i] + chol_s_eta @\n                  rand.normal(keys[2*i], shape=(2,)))\n    zs.append(PHI @ alphas[i+1] + chol_s_eps @\n              rand.normal(keys[2*i+1], shape=(3,)))\n\nalphas_df = pd.DataFrame(alphas, columns = [\"x\", \"y\"])\nzs_df = pd.DataFrame(zs, columns = [\"x\", \"y\", \"z\"])\n\n\nalphas = jnp.array(alphas)\nzs = jnp.array(zs)\n\n    \nfig1 = px.line(alphas_df, x='x', y='y', height=200)\nfig1.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n\n\n\n\n                                                \n\n\nFigure 1\n\n\n\n\n\n\nCode\nfig2 = go.Figure(data=[go.Scatter3d(\n    x=zs_df['x'],\n    y=zs_df['y'],\n    z=zs_df['z'],\n    mode='markers',\n    marker=dict(\n        symbol='cross',  # Change marker to cross\n        size=5           # Adjust marker size\n    )\n)])\n\nfig2.update_layout(height=200)\n    \nfig2.show()\n\n\n\n\n                                                \n\n\nFigure 2\n\n\n\n\nWe can see how the process is an odd random spiral, and the observations are skewed noisy observations of this in 3D space\nWith filtering, we aim to recover the process {fig-truth} from the observations {fig-obs}. We do this with two ‘forms’ of the filter, which should be equivalent."
  },
  {
    "objectID": "reference/simIDEM.html",
    "href": "reference/simIDEM.html",
    "title": "1 simIDEM",
    "section": "",
    "text": "IDEM.simIDEM(\n    key,\n    T,\n    M,\n    PHI_proc,\n    PHI_obs,\n    obs_locs,\n    beta,\n    alpha0,\n    sigma2_eta=0.01 ** 2,\n    sigma2_eps=0.01 ** 2,\n    process_grid=create_grid(bounds, ngrids),\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from a IDE model. For jit-ability, this only takes in certain parameters. For ease of use, use IDEM.simulate.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/simIDEM.html#parameters",
    "href": "reference/simIDEM.html#parameters",
    "title": "1 simIDEM",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)"
  },
  {
    "objectID": "reference/simIDEM.html#returns",
    "href": "reference/simIDEM.html#returns",
    "title": "1 simIDEM",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/param_exp_kernel.html",
    "href": "reference/param_exp_kernel.html",
    "title": "1 param_exp_kernel",
    "section": "",
    "text": "1 param_exp_kernel\nIDEM.param_exp_kernel(K_basis, k)\nCreates a kernel in the style of AZM’s R-IDE package"
  },
  {
    "objectID": "reference/kalman_smoother.html",
    "href": "reference/kalman_smoother.html",
    "title": "1 kalman_smoother",
    "section": "",
    "text": "1 kalman_smoother\nfilter_smoother_functions.kalman_smoother(ms, Ps, mpreds, Ppreds, M)\nNOT FULLY IMPLEMENTED"
  },
  {
    "objectID": "reference/kalman_filter.html",
    "href": "reference/kalman_filter.html",
    "title": "1 kalman_filter",
    "section": "",
    "text": "filter_smoother_functions.kalman_filter(\n    m_0,\n    P_0,\n    M,\n    PHI_obs,\n    Sigma_eta,\n    Sigma_eps,\n    ztildes,\n    full_likelihood=False,\n)\nApplies the Kalman Filter to a wide-format matrix of data. For jit-ability, this only allows full (no missing) data in a wide format. For changing data locations or changing data dimension, see information_filter_indep.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nSigma_eta\nArrayLike\nThe Covariance matrix of the process noise\nrequired\n\n\nSigma_eps\nArrayLike\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter.html#parameters",
    "href": "reference/kalman_filter.html#parameters",
    "title": "1 kalman_filter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nSigma_eta\nArrayLike\nThe Covariance matrix of the process noise\nrequired\n\n\nSigma_eps\nArrayLike\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired"
  },
  {
    "objectID": "reference/kalman_filter.html#returns",
    "href": "reference/kalman_filter.html#returns",
    "title": "1 kalman_filter",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/information_filter.html",
    "href": "reference/information_filter.html",
    "title": "1 information_filter",
    "section": "",
    "text": "filter_smoother_functions.information_filter(\n    nu_0,\n    Q_0,\n    M,\n    PHI_obs_tuple,\n    Sigma_eta,\n    Sigma_eps_tuple,\n    ztildes,\n    full_likelihood=False,\n)\nApplies the Information Filter to a PyTree of data.\n\n\nnu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nSigma_eta: ArrayLike (r,r)\n    The Covariance matrix of the process noise\nsigma2_eps_tuple: Pytree[ArrayLike (n_t,n_t)]\n    The Covariance matrix of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation\n\n\n\nA tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/information_filter.html#parameters",
    "href": "reference/information_filter.html#parameters",
    "title": "1 information_filter",
    "section": "",
    "text": "nu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nSigma_eta: ArrayLike (r,r)\n    The Covariance matrix of the process noise\nsigma2_eps_tuple: Pytree[ArrayLike (n_t,n_t)]\n    The Covariance matrix of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation"
  },
  {
    "objectID": "reference/information_filter.html#returns",
    "href": "reference/information_filter.html#returns",
    "title": "1 information_filter",
    "section": "",
    "text": "A tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/gen_example_idem.html",
    "href": "reference/gen_example_idem.html",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "IDEM.gen_example_idem(\n    key,\n    k_spat_inv=True,\n    ngrid=jnp.array([41, 41]),\n    nints=jnp.array([100, 100]),\n    nobs=50,\n    m_0=None,\n    sigma2_0=None,\n    process_basis=None,\n    sigma2_eta=0.5 ** 2,\n    sigma2_eps=0.1 ** 2,\n)\nCreates an example IDE model, with randomly generated kernel on the domain [0,1]x[0,1]. Intial value of the process is simply some of the coefficients for the process basis are set to 1. The kernel has a Gaussian shape, with parameters defined as basis expansions in order to allow for spatial variance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nk_spat_inv\nbool\nWhether or not the generated kernel should be spatially invariant.\nTrue\n\n\nngrid\nArrayLike\nThe resolution of the grid at which the process is computed. Should have shape (2,).\njnp.array([41, 41])\n\n\nnints\nArrayLike\nThe resolution of the grid at which Riemann integrals are computed. Should have shape (2,)\njnp.array([100, 100])\n\n\nnobs\nint\nThe number of observations per time interval.\n50\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA model of type IDEM."
  },
  {
    "objectID": "reference/gen_example_idem.html#parameters",
    "href": "reference/gen_example_idem.html#parameters",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nk_spat_inv\nbool\nWhether or not the generated kernel should be spatially invariant.\nTrue\n\n\nngrid\nArrayLike\nThe resolution of the grid at which the process is computed. Should have shape (2,).\njnp.array([41, 41])\n\n\nnints\nArrayLike\nThe resolution of the grid at which Riemann integrals are computed. Should have shape (2,)\njnp.array([100, 100])\n\n\nnobs\nint\nThe number of observations per time interval.\n50"
  },
  {
    "objectID": "reference/gen_example_idem.html#returns",
    "href": "reference/gen_example_idem.html#returns",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA model of type IDEM."
  },
  {
    "objectID": "reference/bisquare.html",
    "href": "reference/bisquare.html",
    "title": "1 bisquare",
    "section": "",
    "text": "1 bisquare\nutilities.bisquare(s, params)\nGeneric bisquare function"
  },
  {
    "objectID": "reference/Kernel.html",
    "href": "reference/Kernel.html",
    "title": "1 Kernel",
    "section": "",
    "text": "IDEM.Kernel(self, function, basis=None, params=None, form='expansion')\nGeneric class defining a kernel, or a basis expansion of a kernel with its parameters.\n\n\n\n\n\nName\nDescription\n\n\n\n\nsave_fig\nSaves a plot of the direction of the kernel.\n\n\nshow_plot\nShows a plot of the direction of the kernel.\n\n\n\n\n\nIDEM.Kernel.save_fig(name='kernel.png', width=5, height=4, dpi=300)\nSaves a plot of the direction of the kernel.\n\n\n\nIDEM.Kernel.show_plot(width=5, height=4)\nShows a plot of the direction of the kernel."
  },
  {
    "objectID": "reference/Kernel.html#methods",
    "href": "reference/Kernel.html#methods",
    "title": "1 Kernel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsave_fig\nSaves a plot of the direction of the kernel.\n\n\nshow_plot\nShows a plot of the direction of the kernel.\n\n\n\n\n\nIDEM.Kernel.save_fig(name='kernel.png', width=5, height=4, dpi=300)\nSaves a plot of the direction of the kernel.\n\n\n\nIDEM.Kernel.show_plot(width=5, height=4)\nShows a plot of the direction of the kernel."
  },
  {
    "objectID": "reference/Grid.html",
    "href": "reference/Grid.html",
    "title": "1 Grid",
    "section": "",
    "text": "1 Grid\nutilities.Grid()\nA simple grid class to store (currently exclusively regular) grids, along with some key quantities such as the lenth between grid points, the number of grid points and the area/volume of each grid square/cube. Supports arbitrarily high dimension. Ideally, in the future, this will support non-regular grids with any necessary quantities to do, for example, integration over the points on the grid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "",
    "text": "Code\nimport sys\nimport os\nsys.path.append(os.path.abspath('src/jaxidem'))\n\nimport jax\nimport importlib\nimport utilities\nimport idem\n\nimportlib.reload(utilities)\nimportlib.reload(idem)\n\nfrom utilities import *\nfrom idem import *\nimport warnings\n\nimport matplotlib.pyplot as plt\n\nseed = 4\nkey = jax.random.PRNGKey(seed)\nkeys = rand.split(key, 2)\n\nmodel = gen_example_idem(keys[0], k_spat_inv=False, ngrid=jnp.array([40, 40]))\n\n# Simulation\nT = 35\nprocess_data, obs_data = model.simulate(key, T=T, nobs=50)\n\ndpi = 200\nwidth = 576 / dpi\nheight = 480 / dpi\n\n# plot the objects\ngif_st_grid(process_data, \"site/process.gif\", width=width, height=height)\ngif_st_pts(obs_data, \"site/obs.gif\", width=width, height=height)\nmodel.kernel.save_plot(\"site/kernel.png\", width=width, height=height)\n\nfrom PIL import Image, ImageSequence\n\ngif1 = Image.open('site/process.gif')\ngif2 = Image.open('site/tardis.gif')\n\nwidth, height = gif1.size\n\nframes = []\nnum_frames_gif1 = len(list(ImageSequence.Iterator(gif1)))\nnum_frames_gif2 = len(list(ImageSequence.Iterator(gif2)))\nmax_frames = max(num_frames_gif1, num_frames_gif2)\n\nfor i in range(max_frames):\n    frame1 = ImageSequence.Iterator(gif1)[i % num_frames_gif1].convert(\"RGBA\")\n    frame2 = ImageSequence.Iterator(gif2)[i % num_frames_gif2].convert(\"RGBA\")\n\n    frame2 = frame2.resize((width, height), Image.LANCZOS)\n    \n    combined = Image.alpha_composite(frame1, frame2)\n    frames.append(combined)\n\n\nframes[0].save('site/process.gif', save_all=True, append_images=frames[1:], duration=gif1.info['duration'], loop=0)"
  },
  {
    "objectID": "index.html#the-technicalities",
    "href": "index.html#the-technicalities",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "1 The Technicalities",
    "text": "1 The Technicalities\nFor a rundown of the mathematics underpinning this model and implementation, see here."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "2 Documentation",
    "text": "2 Documentation\nDocumentation for the package is available here."
  },
  {
    "objectID": "reference/Basis.html",
    "href": "reference/Basis.html",
    "title": "1 Basis",
    "section": "",
    "text": "utilities.Basis()\nA simple class for spatial basis expansions.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvfun\nArrayLike (ndim,) -&gt; ArrayLike (nbasis,)\nApplying to a single spatial location, evaluates all the basis functions on the single location and returns the result as a vector.\n\n\nmfun\nArrayLike (ndim, n) -&gt; ArrayLike (nbasis, n)\nApplying to a array of spatial points, evaluates all the basis functions on each point and returns the results in a matrix.\n\n\nparams\nArrayLike(nparams, nbasis)\nThe parameters defining each basis function. For example, for bisquare basis functions, the parameters are the locations of the centers of each function, and the function’s scale.\n\n\nnbasis\nint\nThe number of basis functions in the expansion."
  },
  {
    "objectID": "reference/Basis.html#attributes",
    "href": "reference/Basis.html#attributes",
    "title": "1 Basis",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nvfun\nArrayLike (ndim,) -&gt; ArrayLike (nbasis,)\nApplying to a single spatial location, evaluates all the basis functions on the single location and returns the result as a vector.\n\n\nmfun\nArrayLike (ndim, n) -&gt; ArrayLike (nbasis, n)\nApplying to a array of spatial points, evaluates all the basis functions on each point and returns the results in a matrix.\n\n\nparams\nArrayLike(nparams, nbasis)\nThe parameters defining each basis function. For example, for bisquare basis functions, the parameters are the locations of the centers of each function, and the function’s scale.\n\n\nnbasis\nint\nThe number of basis functions in the expansion."
  },
  {
    "objectID": "reference/IDEM_Model.html",
    "href": "reference/IDEM_Model.html",
    "title": "1 IDEM_Model",
    "section": "",
    "text": "IDEM.IDEM_Model(\n    self,\n    process_basis,\n    kernel,\n    process_grid,\n    sigma2_eta,\n    sigma2_eps,\n    beta,\n    int_grid=create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n    m_0=None,\n    sigma2_0=None,\n)\nThe Integro-differential Equation Model.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\ndata_mle_fit\nMAY BE OUT OF DATE\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfilter_information\nNOT IMPLEMENTED\n\n\nfit_information_filter\nNOT FULLY IMPLEMENTED\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function simIDEM.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\n\n\n\nIDEM.IDEM_Model.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nIDEM.IDEM_Model.data_mle_fit(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nMAY BE OUT OF DATE\n\n\n\nIDEM.IDEM_Model.filter(obs_data_wide, X_obs)\nRuns the Kalman filter on the inputted data.\n\n\n\nIDEM.IDEM_Model.filter_information(obs_data, X_obs, nu_0=None, Q_0=None)\nNOT IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.fit_information_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nNOT FULLY IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nIDEM.IDEM_Model.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from the model, using the jit-able function simIDEM.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the ST_Data_Long type (see utilities)\n\n\n\n\n\n\n\nIDEM.IDEM_Model.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the"
  },
  {
    "objectID": "reference/IDEM_Model.html#methods",
    "href": "reference/IDEM_Model.html#methods",
    "title": "1 IDEM_Model",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\ndata_mle_fit\nMAY BE OUT OF DATE\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfilter_information\nNOT IMPLEMENTED\n\n\nfit_information_filter\nNOT FULLY IMPLEMENTED\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function simIDEM.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\n\n\n\nIDEM.IDEM_Model.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nIDEM.IDEM_Model.data_mle_fit(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nMAY BE OUT OF DATE\n\n\n\nIDEM.IDEM_Model.filter(obs_data_wide, X_obs)\nRuns the Kalman filter on the inputted data.\n\n\n\nIDEM.IDEM_Model.filter_information(obs_data, X_obs, nu_0=None, Q_0=None)\nNOT IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.fit_information_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nNOT FULLY IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nIDEM.IDEM_Model.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from the model, using the jit-able function simIDEM.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the ST_Data_Long type (see utilities)\n\n\n\n\n\n\n\nIDEM.IDEM_Model.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the"
  },
  {
    "objectID": "reference/basis_params_to_st_data.html",
    "href": "reference/basis_params_to_st_data.html",
    "title": "1 basis_params_to_st_data",
    "section": "",
    "text": "IDEM.basis_params_to_st_data(alphas, process_basis, process_grid, times=None)\nConverts the process expansion coefficients back into the original process \\(Y_t(s)\\) on the inputted process grid.\n\n\nalphas: ArrayLike (T, r) The basis coefficients of the process process_basis: Basis The basis to use in the expansion process_grid: Grid The grid points on which to evaluate \\(Y\\) times: ArrayLike (T,) (optional) The array of times which the processes correspond to"
  },
  {
    "objectID": "reference/basis_params_to_st_data.html#params",
    "href": "reference/basis_params_to_st_data.html#params",
    "title": "1 basis_params_to_st_data",
    "section": "",
    "text": "alphas: ArrayLike (T, r) The basis coefficients of the process process_basis: Basis The basis to use in the expansion process_grid: Grid The grid points on which to evaluate \\(Y\\) times: ArrayLike (T,) (optional) The array of times which the processes correspond to"
  },
  {
    "objectID": "reference/create_grid.html",
    "href": "reference/create_grid.html",
    "title": "1 create_grid",
    "section": "",
    "text": "utilities.create_grid(bounds, ngrids)\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nArrayLike\nThe bounds for each dimension\nrequired\n\n\nngrids\nArrayLike\nThe number of columns/rows/hyper-column in each dimension\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGrid Object (NamedTuple) containing the coordinates, deltas, grid\n\n\n\n\nnumbers, areas, etc. See the Grid class."
  },
  {
    "objectID": "reference/create_grid.html#parameters",
    "href": "reference/create_grid.html#parameters",
    "title": "1 create_grid",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nArrayLike\nThe bounds for each dimension\nrequired\n\n\nngrids\nArrayLike\nThe number of columns/rows/hyper-column in each dimension\nrequired"
  },
  {
    "objectID": "reference/create_grid.html#returns",
    "href": "reference/create_grid.html#returns",
    "title": "1 create_grid",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nGrid Object (NamedTuple) containing the coordinates, deltas, grid\n\n\n\n\nnumbers, areas, etc. See the Grid class."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "1 Function reference",
    "section": "",
    "text": "Functions to apply Kalman/information filters, smoothers and so on.\n\n\n\nkalman_filter\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\nkalman_filter_indep\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\ninformation_filter\nApplies the Information Filter to a PyTree of data.\n\n\ninformation_filter_indep\nApplies the Information Filter to a PyTree of data.\n\n\nkalman_smoother\nNOT FULLY IMPLEMENTED\n\n\n\n\n\n\nClasses and functions to perform simulation, fitting, filtering and prediction on IDEMs.\n\n\n\nKernel\nGeneric class defining a kernel, or a basis expansion of a kernel with\n\n\nIDEM_Model\nThe Integro-differential Equation Model.\n\n\nparam_exp_kernel\nCreates a kernel in the style of AZM’s R-IDE package\n\n\nsimIDEM\nSimulates from a IDE model.\n\n\ngen_example_idem\nCreates an example IDE model, with randomly generated kernel on the\n\n\nbasis_params_to_st_data\nConverts the process expansion coefficients back into the original process\n\n\n\n\n\n\nGeneral classes and functions used to supplement the main package\n\n\n\nGrid\nA simple grid class to store (currently exclusively regular) grids, along\n\n\nBasis\nA simple class for spatial basis expansions.\n\n\ncreate_grid\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\nouter_op\nComputes the outer operation of two vectors, a generalisation of the outer\n\n\nbisquare\nGeneric bisquare function\n\n\nplace_basis\nDistributes knots (centroids) and scales for basis functions over a\n\n\nst_data\nFor storing spatio-temporal data and appropriate methods for plotting such"
  },
  {
    "objectID": "reference/index.html#filtering-and-smoothing-functions",
    "href": "reference/index.html#filtering-and-smoothing-functions",
    "title": "1 Function reference",
    "section": "",
    "text": "Functions to apply Kalman/information filters, smoothers and so on.\n\n\n\nkalman_filter\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\nkalman_filter_indep\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\ninformation_filter\nApplies the Information Filter to a PyTree of data.\n\n\ninformation_filter_indep\nApplies the Information Filter to a PyTree of data.\n\n\nkalman_smoother\nNOT FULLY IMPLEMENTED"
  },
  {
    "objectID": "reference/index.html#integro-difference-models-in-jax",
    "href": "reference/index.html#integro-difference-models-in-jax",
    "title": "1 Function reference",
    "section": "",
    "text": "Classes and functions to perform simulation, fitting, filtering and prediction on IDEMs.\n\n\n\nKernel\nGeneric class defining a kernel, or a basis expansion of a kernel with\n\n\nIDEM_Model\nThe Integro-differential Equation Model.\n\n\nparam_exp_kernel\nCreates a kernel in the style of AZM’s R-IDE package\n\n\nsimIDEM\nSimulates from a IDE model.\n\n\ngen_example_idem\nCreates an example IDE model, with randomly generated kernel on the\n\n\nbasis_params_to_st_data\nConverts the process expansion coefficients back into the original process"
  },
  {
    "objectID": "reference/index.html#utilties",
    "href": "reference/index.html#utilties",
    "title": "1 Function reference",
    "section": "",
    "text": "General classes and functions used to supplement the main package\n\n\n\nGrid\nA simple grid class to store (currently exclusively regular) grids, along\n\n\nBasis\nA simple class for spatial basis expansions.\n\n\ncreate_grid\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\nouter_op\nComputes the outer operation of two vectors, a generalisation of the outer\n\n\nbisquare\nGeneric bisquare function\n\n\nplace_basis\nDistributes knots (centroids) and scales for basis functions over a\n\n\nst_data\nFor storing spatio-temporal data and appropriate methods for plotting such"
  },
  {
    "objectID": "reference/information_filter_indep.html",
    "href": "reference/information_filter_indep.html",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "filter_smoother_functions.information_filter_indep(\n    nu_0,\n    Q_0,\n    M,\n    PHI_obs_tuple,\n    sigma2_eta,\n    sigma2_eps,\n    ztildes,\n    full_likelihood=True,\n)\nApplies the Information Filter to a PyTree of data.\nIncludes some optimisation for uncorrelated errors.\n\n\nnu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nsigma2_eta: float\n    The variance of the process noise\nsigma2_eps: float\n    The variance of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation\n\n\n\nA tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/information_filter_indep.html#parameters",
    "href": "reference/information_filter_indep.html#parameters",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "nu_0: ArrayLike (r,)\n    The initial information of the process vector\nQ_0: ArrayLike (r,r)\n    The initial information matrix of the process vector\nM: ArrayLike (r,r)\n    The transition matrix of the process\nPHI_obs_tuple: Pytree[ArrayLike (r,n)]\n    The process-to-data matrix\nsigma2_eta: float\n    The variance of the process noise\nsigma2_eps: float\n    The variance of the observation noise\nztildes: ArrayLike\n    The observed data to be filtered, in matrix format\nfull_likelihood: bool\n    Whether to include constant terms in the likelihood computation"
  },
  {
    "objectID": "reference/information_filter_indep.html#returns",
    "href": "reference/information_filter_indep.html#returns",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "A tuple containing:\n    ll: The log (data) likelihood of the data\n    nus: (T,r) The posterior information vectors $\nu_{t t}$ of the process given the data 1:t Qs: (T,r,r) The posterior information matrices \\(Q_{t \\mid t}\\) of the process given the data 1:t"
  },
  {
    "objectID": "reference/kalman_filter_indep.html",
    "href": "reference/kalman_filter_indep.html",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "filter_smoother_functions.kalman_filter_indep(\n    m_0,\n    P_0,\n    M,\n    PHI_obs,\n    sigma2_eta,\n    sigma2_eps,\n    ztildes,\n    full_likelihood=False,\n)\nApplies the Kalman Filter to a wide-format matrix of data. Includes some optimisation for uncorrelated errors. For jit-ability, this only allows full (no missing) data in a wide format. For changing data locations or changing data dimension, see information_filter_indep.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nsigma2_eta\nfloat\nThe Covariance matrix of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter_indep.html#parameters",
    "href": "reference/kalman_filter_indep.html#parameters",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_obs\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nsigma2_eta\nfloat\nThe Covariance matrix of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe Covariance matrix of the observation noise\nrequired\n\n\nztildes\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired"
  },
  {
    "objectID": "reference/kalman_filter_indep.html#returns",
    "href": "reference/kalman_filter_indep.html#returns",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing:\nll: The log (data) likelihood of the data ms: (T,r) The posterior means \\(m_{t \\mid t}\\) of the process given\n\n\n\nthe data 1:t\nPs: (T,r,r) The posterior covariance matrices \\(P_{t \\mid t}\\) of\n\n\n\nthe process given the data 1:t\nmpreds: (T-1,r) The predicted next-step means \\(m_{t \\mid t-1}\\)\n\n\n\nof the process given the data 1:t-1\nPpreds: (T-1,r,r) The predicted next-step covariances\n\n\n\n\\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\nKs: (n,r) The Kalman Gains at each time step"
  },
  {
    "objectID": "reference/outer_op.html",
    "href": "reference/outer_op.html",
    "title": "1 outer_op",
    "section": "",
    "text": "utilities.outer_op(a, b, op=lambda x, y: x * y)\nComputes the outer operation of two vectors, a generalisation of the outer product.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nArrayLike\nArray of the first vector\nrequired\n\n\nb\nArrayLike\nArray of the second vector\nrequired\n\n\nop\nCallable\nA jit-function acting on an element of vec1 and an element of vec2. By default, this is the outer product.\nlambda x, y: x * y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nArrayLike[C] (n, m):\nThe matrix of the result of applying operation to every pair of elements from the two vectors."
  },
  {
    "objectID": "reference/outer_op.html#parameters",
    "href": "reference/outer_op.html#parameters",
    "title": "1 outer_op",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\na\nArrayLike\nArray of the first vector\nrequired\n\n\nb\nArrayLike\nArray of the second vector\nrequired\n\n\nop\nCallable\nA jit-function acting on an element of vec1 and an element of vec2. By default, this is the outer product.\nlambda x, y: x * y"
  },
  {
    "objectID": "reference/outer_op.html#returns",
    "href": "reference/outer_op.html#returns",
    "title": "1 outer_op",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nArrayLike[C] (n, m):\nThe matrix of the result of applying operation to every pair of elements from the two vectors."
  },
  {
    "objectID": "reference/place_basis.html",
    "href": "reference/place_basis.html",
    "title": "1 place_basis",
    "section": "",
    "text": "utilities.place_basis(\n    data=jnp.array([[0, 0], [1, 1]]),\n    nres=2,\n    aperture=1.25,\n    min_knot_num=3,\n    basis_fun=bisquare,\n)\nDistributes knots (centroids) and scales for basis functions over a number of resolutions,similar to auto_basis from the R package FRK. This function must be run outside of a jit loop, since it involves varying the length of arrays.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n\nArray of 2D points defining the space on which to put the basis functions\njnp.array([[0, 0], [1, 1]])\n\n\nnres\n\nThe number of resolutions at which to place basis functions\n2\n\n\naperture\n\nScaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)\n1.25\n\n\nmin_knot_num\n\nThe number of basis functions to place in each dimension at the coursest resolution\n3\n\n\nbasis_fun\n\nThe basis functions being used. The basis function’s second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.\nbisquare\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA Basis object (NamedTuple) with the vector and matrix functions, and the\n\n\n\n\nparameters associated to the basis functions."
  },
  {
    "objectID": "reference/place_basis.html#parameters",
    "href": "reference/place_basis.html#parameters",
    "title": "1 place_basis",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n\nArray of 2D points defining the space on which to put the basis functions\njnp.array([[0, 0], [1, 1]])\n\n\nnres\n\nThe number of resolutions at which to place basis functions\n2\n\n\naperture\n\nScaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)\n1.25\n\n\nmin_knot_num\n\nThe number of basis functions to place in each dimension at the coursest resolution\n3\n\n\nbasis_fun\n\nThe basis functions being used. The basis function’s second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.\nbisquare"
  },
  {
    "objectID": "reference/place_basis.html#returns",
    "href": "reference/place_basis.html#returns",
    "title": "1 place_basis",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA Basis object (NamedTuple) with the vector and matrix functions, and the\n\n\n\n\nparameters associated to the basis functions."
  },
  {
    "objectID": "reference/st_data.html",
    "href": "reference/st_data.html",
    "title": "1 st_data",
    "section": "",
    "text": "utilities.st_data(self, x, y, t, z)\nFor storing spatio-temporal data and appropriate methods for plotting such data, and converting between long and wide formats.\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wide\nGives the data in wide format. Any missing data will be represented in\n\n\n\n\n\nutilities.st_data.as_wide()\nGives the data in wide format. Any missing data will be represented in the returned matris as NaN.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA dictionary containing the x coordinates and y coordinates as JAX\n\n\n\n\narrays, and a matrix corresponding to the value of the process at\n\n\n\n\neach time point (columns) and spatial point (rows)."
  },
  {
    "objectID": "reference/st_data.html#methods",
    "href": "reference/st_data.html#methods",
    "title": "1 st_data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wide\nGives the data in wide format. Any missing data will be represented in\n\n\n\n\n\nutilities.st_data.as_wide()\nGives the data in wide format. Any missing data will be represented in the returned matris as NaN.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA dictionary containing the x coordinates and y coordinates as JAX\n\n\n\n\narrays, and a matrix corresponding to the value of the process at\n\n\n\n\neach time point (columns) and spatial point (rows)."
  },
  {
    "objectID": "site/fit_example.html",
    "href": "site/fit_example.html",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "",
    "text": ": : : {.content-visible unless-format = “pdf”} Index : : :"
  },
  {
    "objectID": "site/fit_example.html#fitting",
    "href": "site/fit_example.html#fitting",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.1 Fitting",
    "text": "2.1 Fitting\nNow, after we initialise with a ‘guess’ baseline model, we can use idem.IDEM_model.fit_kalman_filter (recomended for fixed data observation locations) or fit_information_filter to fit the model to the synthetic data.\n\n\nCode\nK_basis = truemodel.kernel.basis\n\nk = (\n            jnp.array([100]),\n            jnp.array([0.001]),\n            jnp.zeros(truemodel.kernel.basis[2].nbasis),\n            jnp.zeros(truemodel.kernel.basis[2].nbasis),\n)\n\n# This is the kind of kernel used by ```gen_example_idem```\nkernel = idem.param_exp_kernel(K_basis, k)\n\nmodel0 = idem.IDEM(process_basis = process_basis,\n                   kernel=kernel,\n                   process_grid = utilities.create_grid(jnp.array([[0, 1], [0, 1]]),\n                                                        jnp.array([41, 41])),\n                   sigma2_eta = 0.01**2,\n                   sigma2_eps = 0.01**2,\n                   beta = jnp.array([0.0, 0.0, 0.0]),)\n\n\nFor context, the true values of the kernel parameters are\n\n\nCode\nprint(truemodel.kernel.params)\n\n\n(Array([150.], dtype=float64), Array([0.002], dtype=float64), Array([-0.1], dtype=float64), Array([0.1], dtype=float64))\n\n\nSo we’ve chosen a model with high prior variance and no flow, with inaccurate guesses for the spread, diffusion, and variances.\nThe fitting functions output new IDEM_Model objects, generated using OPTAX to optimise for the likelihood.\n\n\nCode\nobs_data_wide = obs_data.as_wide()\nX_obs = jnp.column_stack([jnp.ones(obs_data_wide['x'].shape[0]), obs_data_wide['x'], obs_data_wide['y']])\nX_obs_tuple = [X_obs for _ in range(len(obs_data.z))]\n\nimport optax \n\nmodel1, params = model0.fit_kalman_filter(obs_data=obs_data,\n                                       X_obs=X_obs, \n                                       optimizer=optax.adamax(1e-1),\n                                       debug=False,\n                                       max_its=1000,\n                                       eps = 1e-5)\nprint(model1.kernel.params)\nprint(truemodel.kernel.params)\n\n\nInitial Parameters:\n\nKernel Parameters: \n     shape:[100.00000000000004]\n     scale: [0.0010000000000000002]\n     offsets [0.0], [0.0]\nVariance Parameters: 0.00010000000000000009, 0.00010000000000000009\nCoefficient Parameters: [0.0, 0.0, 0.0]\n\n\n\n\n\n\nLikelihood stopped improving. Stopping early...\nThe log likelihood (up to a constant) of the initial model is\n               -455073.02788914\nThe final log likelihood (up to a constant) of the fit model is\n               1845.3149093149214\n\nthe final offset parameters are [-0.10004509] and\n               [0.0997501]\n\n\n(Array([165.56467586], dtype=float64), Array([0.00178456], dtype=float64), Array([-0.10004509], dtype=float64), Array([0.0997501], dtype=float64))\n(Array([150.], dtype=float64), Array([0.002], dtype=float64), Array([-0.1], dtype=float64), Array([0.1], dtype=float64))\n\n\n\n\nCode\nll, ms, Ps, _, _ = model1.filter(obs_data, X_obs=X_obs)\n#ms = jnp.linalg.solve(Qs, nus[..., None]).squeeze(-1)\n\ndata = idem.basis_params_to_st_data(ms, truemodel.process_basis, truemodel.process_grid)\n\ndata.show_plot()\nprint(ll)\n\n\n/home/tate/Projects/JAX-IDEM/src/jaxidem/utilities.py:390: UserWarning:\n\nMatplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n\n\n\n1018.2702294307157"
  },
  {
    "objectID": "index.html#other-sections",
    "href": "index.html#other-sections",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "3 Other sections",
    "text": "3 Other sections\nIDEM fit example\nFiltering example"
  },
  {
    "objectID": "site/comparison.html",
    "href": "site/comparison.html",
    "title": "Testing against R-IDE",
    "section": "",
    "text": "Index\n\n1 Loading the data from R-IDE\nI have run code from Andrew ZM’s IDE package, available in the file R-IDErun.RData. Here are the results of that run on my machine;\n\n\nCode\nlibrary(\"plyr\")\nlibrary(\"dplyr\")\nlibrary(\"IDE\")\nlibrary(\"FRK\")\nlibrary(\"ggplot2\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"readr\")\n\n\n\n\nCode\nload(\"../data/R-IDErun.RData\")\n\n# Extract the values\nsigma2_eta &lt;- fit_results_sim1$IDEmodel$get(\"sigma2_eta\")[\"par6\"]\nsigma2_eps &lt;- fit_results_sim1$IDEmodel$get(\"sigma2_eps\")[\"par5\"]\n\n# Combine them into a data frame\nvariances_df &lt;- data.frame(par5 = sigma2_eps, par6 = sigma2_eta)\n\nprint(fit_results_sim1$IDEmodel$get(\"k\") %&gt;% unlist())\n\n\n         par1          par2          par3          par4 \n152.836345912   0.001977115  -0.101601099   0.100368743 \n\n\nCode\nprint(variances_df)\n\n\n             par5        par6\npar5 0.0001057245 0.001322482\n\n\nCode\nprint(coef(fit_results_sim1$IDEmodel)) \n\n\nIntercept        s1        s2 \n0.2073442 0.1966224 0.1907062 \n\n\nCode\nprint(fit_results_sim1$IDEmodel$negloglik())\n\n\n[1] -3217.945\n\n\nCode\nprint(time.taken)\n\n\nTime difference of 18.3 mins\n\n\n\n\n2 Re-creating the environment in jax_idem\nWe will fit to the same data, using as much the same as possible, fitting to target the same likelihood as a stopping rule, where we can then compare the time and parameters between the two implementations.\n\n\nCode\n# | output: false\n\nimport jax\njax.config.update('jax_platform_name', 'gpu')\njax.config.update('jax_enable_x64', True)\nimport jax.random as rand\nimport jax.numpy as jnp\n\n                             \nimport matplotlib.pyplot as plt\n\nimport sys\nimport os\nsys.path.append(os.path.abspath('../src/jaxidem'))\nimport idem\nimport utilities\nimport filter_smoother_functions as fsf\nimport csv\nimport pandas as pd\n\n\ndf = pd.read_csv('../data/obs_data_r-ide.csv')\ndf['time'] = pd.to_datetime(df['time'])\nreference_date = pd.to_datetime('2017-12-01')\ndf['t'] = (df['time'] - reference_date).dt.days+1\n\nobs_data = utilities.st_data(x = jnp.array(df['s1']),\n                             y = jnp.array(df['s2']),\n                             t = jnp.array(df['t']),\n                             z = jnp.array(df['z']))\n\nobs_locs = jnp.column_stack((obs_data.x, obs_data.y))\nX_obs = jnp.column_stack([jnp.ones(obs_locs.shape[0]), obs_locs[:, -2:]])\n\nbetahat = jnp.linalg.solve(X_obs.T @ X_obs, X_obs.T) @ obs_data.z\n\nztilde = obs_data.z - X_obs @ betahat\n\nconst_basis = utilities.place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1)\n                                                \nK_basis = (\n            const_basis,\n            const_basis,\n            const_basis,\n            const_basis,\n        )\nk = (\n            jnp.array([150.0]),\n            jnp.array([0.002]),\n            jnp.array([0.0]),\n            jnp.array([0.0]),\n        )\n\nkernel = idem.param_exp_kernel(K_basis, k)\n                                               \nprocess_basis = utilities.place_basis(nres=2, min_knot_num=3)\n\n\n\nmodel0 = idem.IDEM(\n                    process_basis = process_basis,\n                    kernel=kernel,\n                    process_grid = utilities.create_grid(jnp.array([[0, 1], [0, 1]]),\n                                               jnp.array([41, 41])),\n                    sigma2_eta = jnp.var(ztilde),\n                    sigma2_eps = jnp.var(ztilde),\n                    beta = betahat,)\n\n\n\n\nCode\nimport optax\nimport time\n\nobs_data_wide = obs_data.as_wide()\nX_obs = jnp.column_stack([jnp.ones(obs_data_wide['x'].shape[0]), obs_data_wide['x'], obs_data_wide['y']])\nstart_time = time.time()\nmodel1, params = model0.fit_kalman_filter(obs_data=obs_data,\n                                       X_obs=X_obs, \n                                       optimizer=optax.adamax(1e-2),\n                                       debug=False,\n                                       max_its = 1000,\n                                       fixed_ind = ['ks1', 'ks2'],\n                                       target_ll = jnp.array(3217.945),\n                                       likelihood = 'partial',\n                                       loading_bar=False)\n\n\nInitial Parameters:\n\nKernel Parameters: \n     shape:[149.99999999999997]\n     scale: [0.0020000000000000005]\n     offsets [0.0], [0.0]\nVariance Parameters: 0.009626094370839137, 0.009626094370839137\nCoefficient Parameters: [0.23190200783831588, 0.18107646486427417, 0.1950488254200777]\n\nAchieved target likelihood. Stopping early...\nThe log likelihood (up to a constant) of the initial model is\n               1650.0392919085284\nThe final log likelihood (up to a constant) of the fit model is\n               3218.1511166829446\n\nthe final offset parameters are [-0.09861604] and\n               [0.10126115]\n\n\nCode\nend_time = time.time()\nprint(f\"\\nTime Elapsed is {end_time - start_time}\")\n\n\n\nTime Elapsed is 31.753804683685303\n\n\nCode\nprint(\"\\nFitted parameters are: \\n\", idem.format_params(params))\n\n\n\nFitted parameters are: \n Kernel Parameters: \n     shape:[149.99999999999997]\n     scale: [0.0020000000000000005]\n     offsets [-0.09861603834112512], [0.10126114812950107]\nVariance Parameters: 0.00021915690586025107, 0.00020488287556980953\nCoefficient Parameters: [0.2043215049172408, 0.18227354744326724, 0.20139286382963814]\n\n\nCode\nprint(f\"with ll {model1.filter(obs_data, X_obs=X_obs, likelihood = 'partial')[0].tolist()}\")\n\n\nwith ll 3218.1511677200033"
  },
  {
    "objectID": "site/filtering_and_smoothing.html#square-root-kalman-filter",
    "href": "site/filtering_and_smoothing.html#square-root-kalman-filter",
    "title": "Filtering in JAX-IDEM",
    "section": "3.1 Square Root Kalman filter",
    "text": "3.1 Square Root Kalman filter\n\n\nCode\nm_0 = jnp.zeros(2)\nU_0 = 10*jnp.eye(2)\n\n# Since we have independant errors, we can use the faster sqrt_filter_indep.\n    \nll3, ms3, Us, _, _, _ = fsf.sqrt_filter_indep(m_0,\n                                              U_0,\n                                              M,\n                                              PHI,\n                                              sigma2_eta,\n                                              sigma2_eps,\n                                              zs.T,  \n                                              likelihood='full')\n    \nms3_df = pd.DataFrame(list(ms3), columns = [\"x\", \"y\"])\n\ncombined3_df = pd.concat([alphas_df.assign(line='True Process'), ms3_df.assign(line='Filtered Process Means')])\n\n# Creating the line plot with custom colors\nfig5 = px.line(combined3_df, x='x', y='y', color='line',\n               color_discrete_sequence=['blue', 'red'], height=200)  # Specify colors here\n\nfig5.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n# Show the plot\nfig5.show()\n\n\n\n\n                                                \n\n\nFigure 5\n\n\n\n\n\n\nCode\nprint(ll)\nprint(ll2)\nprint(ll3)\n\n\n117.81118025923834\n89.47515340536991\nnan"
  }
]