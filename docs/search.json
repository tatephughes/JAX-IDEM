[
  {
    "objectID": "site/Sydney_Radar.html",
    "href": "site/Sydney_Radar.html",
    "title": "Sydney Radar Data",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "site/Sydney_Radar.html#maximum-likelihood-using-optax",
    "href": "site/Sydney_Radar.html#maximum-likelihood-using-optax",
    "title": "Sydney Radar Data",
    "section": "2.1 Maximum Likelihood using Optax",
    "text": "2.1 Maximum Likelihood using Optax\nFirstly, we can easily plug this into the optax package to use some optimiser to do maximum likelhood estimation. For example, using the adam optimiser;\n\nimport optax\n\n# create a function to find the value and gradient of the negative log likelihood\nnll_val_grad = jax.value_and_grad(lambda par: -log_marginal(par))\n\ninit_nll, _ = nll_val_grad(model.params)\n\noptimizer = optax.adam(1e-1)\nopt_state = optimizer.init(model.params)\n\nparams = model.params\n\nfor i in range(200):\n    nll, grad = nll_val_grad(params)\n    updates, opt_state = optimizer.update(grad, opt_state, params=params)\n    params = optax.apply_updates(params, updates)\n    print(i, nll)\n\nmle_params = params\n\nThe resulting parameters are then\n\nidem.print_params(mle_params)\n\nParameters:\n  sigma2_eps: 5.723726272583008\n  sigma2_eta: 28.266475677490234\n  Kernel Parameters:\n    Scale: [0.08538345247507095]\n    Shape: [3.7510576248168945]\n    Offset X: [-5.437947750091553]\n    Offset Y: [-1.7626336812973022]\n  beta: [0.42389795184135437]\n\n\nOnce we have new parameters, we can update the model with them using idem.Model.update;\n\nfit_model_mle = model.update(mle_params)"
  },
  {
    "objectID": "site/Sydney_Radar.html#simple-random-walk-mcmc",
    "href": "site/Sydney_Radar.html#simple-random-walk-mcmc",
    "title": "Sydney Radar Data",
    "section": "2.2 Simple Random-Walk MCMC",
    "text": "2.2 Simple Random-Walk MCMC\nOf course, for Bayesian analysis, we furthermore want to be able to sample from the posterior. Now, we will use a basic random walk RMH (Rosenbluth-Metropolis-Hastings, often just called Metropolis-Hastings) to sample from the models posterior.\nFirstly, in order to esaily handle everything, we will flatten the parameters into a single 1D JAX array. The functions jaxidem.utils.flatten and jaxidem.utils.unflatten do this easily;\n\nfparams, unflat = utils.flatten_and_unflatten(model.params)\nprint(fparams)\nidem.print_params(unflat(fparams))\n\n[ 3.9097307  3.9097307  5.0106354 -6.214608   0.         0.\n  0.       ]\nParameters:\n  sigma2_eps: 49.88551330566406\n  sigma2_eta: 49.88551330566406\n  Kernel Parameters:\n    Scale: [150.00001525878906]\n    Shape: [0.001999999862164259]\n    Offset X: [0.0]\n    Offset Y: [0.0]\n  beta: [0.0]\n\n\nNow we can initialise a chain with variance 1 for each parameter\n\ninit_mean = fparams\n\n# initial run gave the following for estimated optimal tuning\nprop_var = jnp.array([0.16133152, 0.00453646, 0.01214727, 0.392362, 0.789936, 0.41011548, 0.14044523])\n\nNow sampling from the proposal Gaussian distribution is as simple as\n\nrng_key = jax.random.PRNGKey(1)\nparshape = init_mean.shape\nnpars = parshape[0]\nprint(init_mean + jax.random.normal(rng_key, shape=parshape) * jnp.sqrt(prop_var))\n\n[ 3.8476994   3.915436    4.9956484  -6.311721    1.125793    0.09497016\n  0.80257165]\n\n\nAnd finally, we can sample a chain with RMH as follows;\n\nback_key, sample_key = jax.random.split(rng_key, 2)\n\nn = 100\n\nsample_keys = jax.random.split(sample_key, n)\n\ncurrent_state = init_mean        \nrmh_sample = [current_state]\naccepted = 0\n\nfor i in tqdm(range(n), desc=\"Sampling... \"):\n    current_state = rmh_sample[-1]\n    prop_key, acc_key = jax.random.split(sample_keys[i], 2)\n\n    proposal = current_state + jax.random.normal(prop_key, shape=parshape) * jnp.sqrt(init_vars)\n    r = log_marginal(unflat(proposal)) - log_marginal(unflat(current_state))\n    log_acc_prob = min((jnp.array(0.0), r))\n    if jnp.log(jax.random.uniform(acc_key)) &gt; log_acc_prob:\n        rmh_sample.append(current_state)\n    else:\n        accepted = accepted + 1\n        rmh_sample.append(proposal)\n\nacc_ratio = accepted/n\n\n\nprint(acc_ratio)\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):]), axis=0)\npost_params_mean = unflat(post_mean)\nidem.print_params(post_params_mean)\nprint(log_marginal(post_params_mean)) \n\n0.06055\nParameters:\n  sigma2_eps: 5.692798614501953\n  sigma2_eta: 28.267518997192383\n  Kernel Parameters:\n    Scale: [0.09685702621936798]\n    Shape: [3.2974460124969482]\n    Offset X: [-5.449050426483154]\n    Offset Y: [-1.7590761184692383]\n  beta: [0.4384661912918091]\nnan\n\n\n\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\n\nsamples = jnp.array(rmh_sample[1000:])\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()"
  },
  {
    "objectID": "site/Sydney_Radar.html#mala",
    "href": "site/Sydney_Radar.html#mala",
    "title": "Sydney Radar Data",
    "section": "2.3 MALA",
    "text": "2.3 MALA\nEven when tuned, the mixing of these chains leaves much to be desired. Since jaxidem supports JAX’s autodifferentiation, we can easily incorporate the gradient into a MCMC chain using MALA.\n\nprop_sd = 0.008\n\naccepted = 0\nlmvn = jax.scipy.stats.multivariate_normal.logpdf\n\nback_key, sample_key = jax.random.split(back_key, 2)\n\nsample_keys = jax.random.split(sample_key, mala_n)\n\n\nll_val_grad = jax.value_and_grad(lambda par: log_marginal(par))\n\n# start from the end of the last chain\nmala_sample = init_mean]\n\nfor i in tqdm(range(mala_n), desc=\"Sampling... \"):\n    current_state = mala_sample[-1]\n    prop_key, acc_key = jax.random.split(sample_keys[i], 2)\n\n    val, grad = ll_val_grad(unflat(current_state))\n    grad, _ = utils.flatten_and_unflatten(grad)\n\n    mean = 0.5* prop_sd**2 * grad + current_state\n\n    proposal = (mean + prop_sd * jax.random.normal(prop_key, shape=parshape))\n\n    r = (log_marginal(unflat(proposal)) - val\n         + lmvn(current_state, mean, prop_sd*jnp.eye(7)) - lmvn(proposal, mean, prop_sd*jnp.eye(7)))\n    log_acc_prob = min((jnp.array(0.0), r))\n        \n    if jnp.log(jax.random.uniform(acc_key)) &gt; log_acc_prob:\n        mala_sample.append(current_state)\n    else:\n        accepted = accepted + 1\n        mala_sample.append(proposal)\n\nacc_ratio = accepted/mala_n\n\n\nprint(acc_ratio)\npost_mean = jnp.mean(jnp.array(mala_sample[int(len(mala_sample)/3):]), axis=0)\npost_params_mean = unflat(post_mean)\nidem.print_params(post_params_mean)\nprint(log_marginal(post_params_mean)) \n\n0.02675\nParameters:\n  sigma2_eps: 5.694046974182129\n  sigma2_eta: 28.345043182373047\n  Kernel Parameters:\n    Scale: [0.0813666507601738]\n    Shape: [4.000741958618164]\n    Offset X: [-5.447969436645508]\n    Offset Y: [-1.7537591457366943]\n  beta: [0.1484980583190918]\nnan\n\n\n\nimport matplotlib.pyplot as plt\n\nsamples = jnp.array(mala_sample[1000:])\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()"
  },
  {
    "objectID": "site/Sydney_Radar.html#using-blackjax",
    "href": "site/Sydney_Radar.html#using-blackjax",
    "title": "Sydney Radar Data",
    "section": "2.4 Using Blackjax",
    "text": "2.4 Using Blackjax\nFrom there, it is easy to sample from the posterior\n\nkey = jax.random.PRNGKey(1) # PRNG key\ninverse_mass_matrix = jnp.ones(model.nparams)\nnum_integration_steps = 10\nstep_size = 1e-5\nsample, _ = model.sample_posterior(key,\n                                   n=10,\n                                   burnin=0,\n                                   obs_data=radar_data,\n                                   X_obs=[X_obs for _ in range(T)],\n                                   inverse_mass_matrix=inverse_mass_matrix,\n                                   num_integration_steps=num_integration_steps,\n                                   step_size = step_size,\n                                   likelihood_method=\"sqinf\",)\n\nThis initial run will likely mix poorly and have a low acceptance rate. Taking this initial sample, we can fit new (gaussian) priors on the paramterers and use a new mass matrix for the sampling based on the initial sample’s variance.\n\npost_mean = jax.tree.map(lambda x: jnp.mean(x, axis=0), sample.position)\npost_var = jax.tree.map(lambda x: jnp.var(x, axis=0), sample.position)\n\ndef log_prior_density(param):\n\n    (\n        log_sigma2_eta,\n        log_sigma2_eps,\n        ks,\n        beta,\n    ) = param\n\n    logdens_log_sigma2_eta = jax.scipy.stats.norm.logpdf(log_sigma2_eta, loc = post_mean[0], scale=post_var[0])\n    logdens_log_sigma2_eps = jax.scipy.stats.norm.logpdf(log_sigma2_eps, loc = post_mean[1], scale=post_var[1])\n\n    logdens_ks1 = jax.scipy.stats.norm.logpdf(ks[0], post_mean[2][0], post_var[2][0])\n    logdens_ks2 = jax.scipy.stats.norm.logpdf(ks[1], post_mean[2][1], post_var[2][1])\n    logdens_ks3 = jax.scipy.stats.multivariate_normal.logpdf(ks[2], post_mean[2][2], jnp.diag(post_var[2][2]))\n    logdens_ks4 = jax.scipy.stats.multivariate_normal.logpdf(ks[3], post_mean[2][3], jnp.diag(post_var[2][3]))\n\n    logdens_beta = jax.scipy.stats.multivariate_normal.logpdf(beta, post_mean[3], jnp.diag(post_var[3]))\n    return logdens_log_sigma2_eta+logdens_log_sigma2_eps+logdens_ks1+logdens_ks2+logdens_ks3+logdens_ks4+logdens_beta\n\n\ninverse_mass_matrix = jnp.array(jax.tree.flatten(post_var)[0])\n\n\n\nParameters:\n  sigma2_eps: 47.1029052734375\n  sigma2_eta: 32.85206604003906\n  Kernel Parameters:\n    Scale: 0.021949168294668198\n    Shape: 13.138100624084473\n    Offset X: -0.09490437060594559\n    Offset Y: -0.020940084010362625\n  beta: 0.008254376240074635\n\n\n\nimport matplotlib.pyplot as plt\n\nsamples = jnp.array(hmc_sample_array[1000:])\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()"
  },
  {
    "objectID": "site/fit_example.html",
    "href": "site/fit_example.html",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "",
    "text": ": : : {.content-visible unless-format = “pdf”} Index : : :"
  },
  {
    "objectID": "site/fit_example.html#fitting",
    "href": "site/fit_example.html#fitting",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.1 Fitting",
    "text": "2.1 Fitting\nNow, after we initialise with a ‘guess’ baseline model, we can use idem.IDEM_model.fit_kalman_filter (recomended for fixed data observation locations) or fit_information_filter to fit the model to the synthetic data.\n\n\nCode\nK_basis = truemodel.kernel.basis\n\nk = (\n            jnp.array([100]),\n            jnp.array([0.001]),\n            jnp.zeros(truemodel.kernel.basis[2].nbasis),\n            jnp.zeros(truemodel.kernel.basis[2].nbasis),\n)\n\n# This is the kind of kernel used by ```gen_example_idem```\nkernel = idem.param_exp_kernel(K_basis, k)\n\nprocess_basis0 = utils.place_cosine_basis(N=9)\n\nmodel0 = idem.Model(process_basis = process_basis0,\n                   kernel=kernel,\n                   process_grid = utils.create_grid(jnp.array([[0, 1], [0, 1]]),\n                                                        jnp.array([41, 41])),\n                   sigma2_eta = 0.01**2,\n                   sigma2_eps = 0.01**2,\n                   beta = jnp.array([0.0, 0.0, 0.0]),)\n\n\nFor context, the true values of the kernel parameters are\n\n\nCode\nprint(truemodel.kernel.params)\n\n\nSo we’ve chosen a model with high prior variance and no flow, with inaccurate guesses for the spread, diffusion, and variances.\nThe fitting functions output new IDEM_Model objects, generated using OPTAX to optimise for the likelihood.\n\n\nCode\n# OUT OF DATE\n\nobs_data_wide = obs_data.as_wide()\nX_obs = jnp.column_stack([jnp.ones(obs_data_wide['x'].shape[0]), obs_data_wide['x'], obs_data_wide['y']])\nX_obs_tuple = [X_obs for _ in range(len(obs_data.z))]\n\nimport optax \n\nmodel1, params = model0.fit_kalman_filter(obs_data=obs_data,\n                                       X_obs=X_obs, \n                                       optimizer=optax.adamax(1e-1),\n                                       debug=False,\n                                       max_its=200,\n                                       eps = 1e-5)\nprint(model1.kernel.params)\nprint(truemodel.kernel.params)\n\n\n\n\nCode\nll, ms, Ps, _, _ = model1.kalman_filter(obs_data, X_obs=X_obs)\n#ms = jnp.linalg.solve(Qs, nus[..., None]).squeeze(-1)\n\ndata = idem.basis_params_to_st_data(ms, model0.process_basis, model0.process_grid)\n\ndata.show_plot()\nprint(ll)\nprint(model1.beta)"
  },
  {
    "objectID": "site/comparison.html",
    "href": "site/comparison.html",
    "title": "Testing against R-IDE",
    "section": "",
    "text": "Index\n\n1 Loading the data from R-IDE\nI have run code from Andrew ZM’s IDE package, available in the file R-IDErun.RData. Here are the results of that run on my machine;\n#| eval: false\n#| output: false\n\nlibrary(\"plyr\")\nlibrary(\"dplyr\")\nlibrary(\"IDE\")\nlibrary(\"FRK\")\nlibrary(\"ggplot2\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"readr\")\n#| eval: false\nload(\"../data/R-IDErun.RData\")\n\n# Extract the values\nsigma2_eta &lt;- fit_results_sim1$IDEmodel$get(\"sigma2_eta\")[\"par6\"]\nsigma2_eps &lt;- fit_results_sim1$IDEmodel$get(\"sigma2_eps\")[\"par5\"]\n\n# Combine them into a data frame\nvariances_df &lt;- data.frame(par5 = sigma2_eps, par6 = sigma2_eta)\n\nprint(fit_results_sim1$IDEmodel$get(\"k\") %&gt;% unlist())\nprint(variances_df)\nprint(coef(fit_results_sim1$IDEmodel)) \nprint(fit_results_sim1$IDEmodel$negloglik())\n\nprint(time.taken)\n\n\n2 Re-creating the environment in jax_idem\nWe will fit to the same data, using as much the same as possible, fitting to target the same likelihood as a stopping rule, where we can then compare the time and parameters between the two implementations.\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n                             \nimport matplotlib.pyplot as plt\n\nimport jaxidem.idem as idem\nimport jaxidem.utils as utils\nimport jaxidem.filters as filt\n\nimport csv\nimport pandas as pd\n\n\ndf = pd.read_csv('../data/obs_data_r-ide.csv')\ndf['t'] = pd.to_datetime(df['t'])\nreference_date = pd.to_datetime('2017-12-01')\ndf['t'] = (df['t'] - reference_date).dt.days+1\n\nobs_data = utils.st_data(x = jnp.array(df['s1']),\n                             y = jnp.array(df['s2']),\n                             t = jnp.array(df['t']),\n                             z = jnp.array(df['z']))\n\nobs_locs = jnp.column_stack((obs_data.x, obs_data.y))\nX_obs = jnp.column_stack([jnp.ones(obs_locs.shape[0]), obs_locs[:, -2:]])\n\nbetahat = jnp.linalg.solve(X_obs.T @ X_obs, X_obs.T) @ obs_data.z\n\nztilde = obs_data.z - X_obs @ betahat\n\nconst_basis = utils.place_basis(nres=1, min_knot_num=1, basis_fun=lambda s, r: 1)\nK_basis = (const_basis,\n           const_basis,\n           const_basis,\n           const_basis,)\nk = (jnp.array([150.0]),\n     jnp.array([0.002]),\n     jnp.array([0.0]),\n     jnp.array([0.0]),)\n\nkernel = idem.param_exp_kernel(K_basis, k)\n                                               \nprocess_basis = utils.place_basis(nres=2, min_knot_num=3)\n\nmodel = idem.Model(process_basis = process_basis,\n                    kernel=kernel,\n                    process_grid = utils.create_grid(jnp.array([[0, 1], [0, 1]]),\n                                                     jnp.array([41, 41])),\n                    sigma2_eta = jnp.var(ztilde),\n                    sigma2_eps = jnp.var(ztilde),\n                    beta = betahat,)\n\n\n\n\nCode\nimport optax\nimport time\n\nobs_data_wide = obs_data.as_wide()\nX_obs = jnp.column_stack([jnp.ones(obs_data_wide['x'].shape[0]), obs_data_wide['x'], obs_data_wide['y']])\nstart_time = time.time()\n\nfirmodel1, params = model.fit_mle(obs_data=obs_data,\n                                       X_obs=X_obs, \n                                       optimizer=optax.adamax(1e-2),\n                                       debug=False,\n                                       max_its = 10,\n                                       fixed_ind = ['ks1', 'ks2'],\n                                       target_ll = jnp.array(3217.945),\n                                       likelihood = 'partial',\n                                       loading_bar=False)\n\nend_time = time.time()\n\nprint(f\"\\nTime Elapsed is {end_time - start_time}\")\n\nprint(\"\\nFitted parameters are: \\n\", idem.format_params(params))\nprint(f\"with ll {model1.kalman_filter(obs_data, X_obs=X_obs, likelihood = 'partial')[0].tolist()}\")"
  },
  {
    "objectID": "reference/sim_idem.html",
    "href": "reference/sim_idem.html",
    "title": "1 sim_idem",
    "section": "",
    "text": "idem.sim_idem(\n    key,\n    T,\n    M,\n    PHI_proc,\n    PHI_obs,\n    obs_locs,\n    beta,\n    alpha_0,\n    sigma2_eta=0.01 ** 2,\n    sigma2_eps=0.01 ** 2,\n    process_grid=create_grid(bounds, ngrids),\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from a IDE model. For jit-ability, this only takes in certain parameters. For ease of use, use IDEM.simulate.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha_0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/sim_idem.html#parameters",
    "href": "reference/sim_idem.html#parameters",
    "title": "1 sim_idem",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha_0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)"
  },
  {
    "objectID": "reference/sim_idem.html#returns",
    "href": "reference/sim_idem.html#returns",
    "title": "1 sim_idem",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/place_basis.html",
    "href": "reference/place_basis.html",
    "title": "1 place_basis",
    "section": "",
    "text": "utilities.place_basis(\n    data=jnp.array([[0, 0], [1, 1]]),\n    nres=2,\n    aperture=1.25,\n    min_knot_num=3,\n    basis_fun=bisquare,\n)\nDistributes knots (centroids) and scales for basis functions over a number of resolutions,similar to auto_basis from the R package FRK. This function must be run outside of a jit loop, since it involves varying the length of arrays.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n\nArray of 2D points defining the space on which to put the basis functions\njnp.array([[0, 0], [1, 1]])\n\n\nnres\n\nThe number of resolutions at which to place basis functions\n2\n\n\naperture\n\nScaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)\n1.25\n\n\nmin_knot_num\n\nThe number of basis functions to place in each dimension at the coursest resolution\n3\n\n\nbasis_fun\n\nThe basis functions being used. The basis function’s second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.\nbisquare\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA Basis object (NamedTuple) with the vector and matrix functions, and the\n\n\n\n\nparameters associated to the basis functions."
  },
  {
    "objectID": "reference/place_basis.html#parameters",
    "href": "reference/place_basis.html#parameters",
    "title": "1 place_basis",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\n\nArray of 2D points defining the space on which to put the basis functions\njnp.array([[0, 0], [1, 1]])\n\n\nnres\n\nThe number of resolutions at which to place basis functions\n2\n\n\naperture\n\nScaling factor for the scale parameter (scale parameter will be w=aperture * d, where d is the minimum distance between any two of the knots)\n1.25\n\n\nmin_knot_num\n\nThe number of basis functions to place in each dimension at the coursest resolution\n3\n\n\nbasis_fun\n\nThe basis functions being used. The basis function’s second argument must be an array with three doubles; the first coordinate for the centre, the second coordinate for the centre, and the scale/aperture of the function.\nbisquare"
  },
  {
    "objectID": "reference/place_basis.html#returns",
    "href": "reference/place_basis.html#returns",
    "title": "1 place_basis",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA Basis object (NamedTuple) with the vector and matrix functions, and the\n\n\n\n\nparameters associated to the basis functions."
  },
  {
    "objectID": "reference/outer_op.html",
    "href": "reference/outer_op.html",
    "title": "1 outer_op",
    "section": "",
    "text": "utilities.outer_op(a, b, op=lambda x, y: x * y)\nComputes the outer operation of two vectors, a generalisation of the outer product.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nArrayLike\nArray of the first vector\nrequired\n\n\nb\nArrayLike\nArray of the second vector\nrequired\n\n\nop\nCallable\nA jit-function acting on an element of vec1 and an element of vec2. By default, this is the outer product.\nlambda x, y: x * y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nArrayLike[C] (n, m):\nThe matrix of the result of applying operation to every pair of elements from the two vectors."
  },
  {
    "objectID": "reference/outer_op.html#parameters",
    "href": "reference/outer_op.html#parameters",
    "title": "1 outer_op",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\na\nArrayLike\nArray of the first vector\nrequired\n\n\nb\nArrayLike\nArray of the second vector\nrequired\n\n\nop\nCallable\nA jit-function acting on an element of vec1 and an element of vec2. By default, this is the outer product.\nlambda x, y: x * y"
  },
  {
    "objectID": "reference/outer_op.html#returns",
    "href": "reference/outer_op.html#returns",
    "title": "1 outer_op",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nArrayLike[C] (n, m):\nThe matrix of the result of applying operation to every pair of elements from the two vectors."
  },
  {
    "objectID": "reference/kalman_filter_indep.html",
    "href": "reference/kalman_filter_indep.html",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "filter_smoother_functions.kalman_filter_indep(\n    m_0,\n    P_0,\n    M,\n    PHI,\n    sigma2_eta,\n    sigma2_eps,\n    zs,\n    likelihood='partial',\n)\nApplies the Kalman Filter to a wide-format matrix of data. Additionally assumes that both the error terms \\[\\boldsymbol \\epsilon_t\\] and the noise terms \\[\\boldsymbol \\eta_t\\] both have uncorrelated componants. For jit-ability, this only allows full (no missing) data in a wide format. For changing data locations or changing data dimension, see information_filter_inder. For the Kalman filter with correlated errors, see kalman_filter. For the square-root Kalman filter, sqrt_filter_indep.\nComputes posterior means and variances for a system \\[\\begin{split}\n    \\mathbf Z_t &= \\Phi \\boldsymbol\\alpha_t + \\boldsymbol \\epsilon_t, \\quad t = 1,\\dots, T,\\\\\n    \\boldsymbol \\alpha_{t+1} &= M\\boldsymbol \\alpha_t + \\boldsymbol\\eta_t,\\quad t = 0,2,\\dots, T-1,\\\\\n\\end{split}\n\\] with initial ‘priors’ \\[\\begin{split}\n    \\boldsymbol \\alpha_{0} \\sim \\mathcal N(\\mathbf m_0, \\mathbf P_0),\\\\\n\\end{split}\n\\] where \\[\\begin{split}\n    \\boldsymbol \\epsilon_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0, \\sigma^2_\\epsilon I),\n    \\boldsymbol \\eta_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0, \\sigma^2_\\eta I).\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\nrequired\n\n\nzs\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nms\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nPs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nmpreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nPpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter_indep.html#parameters",
    "href": "reference/kalman_filter_indep.html#parameters",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\nrequired\n\n\nzs\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'"
  },
  {
    "objectID": "reference/kalman_filter_indep.html#returns",
    "href": "reference/kalman_filter_indep.html#returns",
    "title": "1 kalman_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nms\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nPs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nmpreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nPpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/information_filter_indep.html",
    "href": "reference/information_filter_indep.html",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "filter_smoother_functions.information_filter_indep(\n    nu_0,\n    Q_0,\n    M,\n    PHI_tree,\n    sigma2_eta,\n    sigma2_eps,\n    zs_tree,\n    likelihood='partial',\n)\nApplies the information Filter (inverse-Kalman filter) to a PyTree of data points at a number of times. Additionally assumes that both the error terms \\[\\boldsymbol \\epsilon_t\\] and the noise terms \\[\\boldsymbol \\eta_t\\] both have uncorrelated componants. Unlike the Kalman filters, this allows for missing data and data changing shape, by taking a PyTree (most likely a list) of observations at each time (which can be jagged). For the standard Kalman filter with uncorrelated errors, see kalman_filter. For the square-root Kalman filter, sqrt_filter_indep.\nComputes posterior information vectors and information matrices for a system \\[\\begin{split}\n    \\mathbf Z_t &= \\Phi \\boldsymbol\\alpha_t + \\boldsymbol \\epsilon_t, \\quad t = 1,\\dots, T,\\\\\n    \\boldsymbol \\alpha_{t+1} &= M\\boldsymbol \\alpha_t + \\boldsymbol\\eta_t,\\quad t = 0,2,\\dots, T-1,\\\\\n\\end{split}\n\\] with initial ‘priors’ \\[\\begin{split}\n    \\boldsymbol \\alpha_{0} \\sim \\mathcal N(\\mathbf m_0, \\mathbf P_0),\\\\\n\\end{split}\n\\] where \\[\\begin{split}\n    \\boldsymbol \\epsilon_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0, \\sigma^2_\\epsilon I),\n    \\boldsymbol \\eta_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0, \\sigma^2_\\eta I).\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnu_0\nArrayLike\nThe initial information vector of the process vector\nrequired\n\n\nQ_0\nArrayLike\nThe initial information matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_tree\ntuple\nThe process-to-data matrices at each time\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe variance the observation noise\nrequired\n\n\nzs_tree\ntuple\nThe observed data to be filtered\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nnus\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nQs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nnupreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nQpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/information_filter_indep.html#parameters",
    "href": "reference/information_filter_indep.html#parameters",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnu_0\nArrayLike\nThe initial information vector of the process vector\nrequired\n\n\nQ_0\nArrayLike\nThe initial information matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_tree\ntuple\nThe process-to-data matrices at each time\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise\nrequired\n\n\nsigma2_eps\nfloat\nThe variance the observation noise\nrequired\n\n\nzs_tree\ntuple\nThe observed data to be filtered\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'"
  },
  {
    "objectID": "reference/information_filter_indep.html#returns",
    "href": "reference/information_filter_indep.html#returns",
    "title": "1 information_filter_indep",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nnus\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nQs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nnupreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nQpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "1 Function reference",
    "section": "",
    "text": "Functions to apply Kalman/information filters, smoothers and so on.\n\n\n\nkalman_filter\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\nkalman_filter_indep\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\ninformation_filter\nWARNING: CURRENTLY NOT WORKING CORRECTLY\n\n\ninformation_filter_indep\nApplies the information Filter (inverse-Kalman filter) to a PyTree of data points at a number of times.\n\n\nkalman_smoother\nNOT FULLY IMPLEMENTED\n\n\n\n\n\n\nClasses and functions to perform simulation, fitting, filtering and prediction on IDEMs.\n\n\n\nKernel\nGeneric class defining a kernel, or a basis expansion of a kernel with\n\n\nIDEM\nThe Integro-differential Equation Model.\n\n\nparam_exp_kernel\nCreates a kernel in the style of AZM’s R-IDE package\n\n\nsim_idem\nSimulates from a IDE model.\n\n\ngen_example_idem\nCreates an example IDE model, with randomly generated kernel on the\n\n\nbasis_params_to_st_data\nConverts the process expansion coefficients back into the original process\n\n\n\n\n\n\nGeneral classes and functions used to supplement the main package\n\n\n\nGrid\nA simple grid class to store (currently exclusively regular) grids, along\n\n\nBasis\nA simple class for spatial basis expansions.\n\n\ncreate_grid\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\nouter_op\nComputes the outer operation of two vectors, a generalisation of the outer\n\n\nbisquare\nGeneric bisquare function\n\n\nplace_basis\nDistributes knots (centroids) and scales for basis functions over a\n\n\nst_data\nFor storing spatio-temporal data and appropriate methods for plotting such"
  },
  {
    "objectID": "reference/index.html#filtering-and-smoothing-functions",
    "href": "reference/index.html#filtering-and-smoothing-functions",
    "title": "1 Function reference",
    "section": "",
    "text": "Functions to apply Kalman/information filters, smoothers and so on.\n\n\n\nkalman_filter\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\nkalman_filter_indep\nApplies the Kalman Filter to a wide-format matrix of data.\n\n\ninformation_filter\nWARNING: CURRENTLY NOT WORKING CORRECTLY\n\n\ninformation_filter_indep\nApplies the information Filter (inverse-Kalman filter) to a PyTree of data points at a number of times.\n\n\nkalman_smoother\nNOT FULLY IMPLEMENTED"
  },
  {
    "objectID": "reference/index.html#integro-difference-models-in-jax",
    "href": "reference/index.html#integro-difference-models-in-jax",
    "title": "1 Function reference",
    "section": "",
    "text": "Classes and functions to perform simulation, fitting, filtering and prediction on IDEMs.\n\n\n\nKernel\nGeneric class defining a kernel, or a basis expansion of a kernel with\n\n\nIDEM\nThe Integro-differential Equation Model.\n\n\nparam_exp_kernel\nCreates a kernel in the style of AZM’s R-IDE package\n\n\nsim_idem\nSimulates from a IDE model.\n\n\ngen_example_idem\nCreates an example IDE model, with randomly generated kernel on the\n\n\nbasis_params_to_st_data\nConverts the process expansion coefficients back into the original process"
  },
  {
    "objectID": "reference/index.html#utilties",
    "href": "reference/index.html#utilties",
    "title": "1 Function reference",
    "section": "",
    "text": "General classes and functions used to supplement the main package\n\n\n\nGrid\nA simple grid class to store (currently exclusively regular) grids, along\n\n\nBasis\nA simple class for spatial basis expansions.\n\n\ncreate_grid\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\nouter_op\nComputes the outer operation of two vectors, a generalisation of the outer\n\n\nbisquare\nGeneric bisquare function\n\n\nplace_basis\nDistributes knots (centroids) and scales for basis functions over a\n\n\nst_data\nFor storing spatio-temporal data and appropriate methods for plotting such"
  },
  {
    "objectID": "reference/create_grid.html",
    "href": "reference/create_grid.html",
    "title": "1 create_grid",
    "section": "",
    "text": "utilities.create_grid(bounds, ngrids)\nCreates an n-dimensional grid based on the given bounds and deltas.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nArrayLike\nThe bounds for each dimension\nrequired\n\n\nngrids\nArrayLike\nThe number of columns/rows/hyper-column in each dimension\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGrid Object (NamedTuple) containing the coordinates, deltas, grid\n\n\n\n\nnumbers, areas, etc. See the Grid class."
  },
  {
    "objectID": "reference/create_grid.html#parameters",
    "href": "reference/create_grid.html#parameters",
    "title": "1 create_grid",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nArrayLike\nThe bounds for each dimension\nrequired\n\n\nngrids\nArrayLike\nThe number of columns/rows/hyper-column in each dimension\nrequired"
  },
  {
    "objectID": "reference/create_grid.html#returns",
    "href": "reference/create_grid.html#returns",
    "title": "1 create_grid",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nGrid Object (NamedTuple) containing the coordinates, deltas, grid\n\n\n\n\nnumbers, areas, etc. See the Grid class."
  },
  {
    "objectID": "reference/basis_params_to_st_data.html",
    "href": "reference/basis_params_to_st_data.html",
    "title": "1 basis_params_to_st_data",
    "section": "",
    "text": "idem.basis_params_to_st_data(alphas, process_basis, process_grid, times=None)\nConverts the process expansion coefficients back into the original process \\(Y_t(s)\\) on the inputted process grid.\n\n\nalphas: ArrayLike (T, r) The basis coefficients of the process process_basis: Basis The basis to use in the expansion process_grid: Grid The grid points on which to evaluate \\(Y\\) times: ArrayLike (T,) (optional) The array of times which the processes correspond to"
  },
  {
    "objectID": "reference/basis_params_to_st_data.html#params",
    "href": "reference/basis_params_to_st_data.html#params",
    "title": "1 basis_params_to_st_data",
    "section": "",
    "text": "alphas: ArrayLike (T, r) The basis coefficients of the process process_basis: Basis The basis to use in the expansion process_grid: Grid The grid points on which to evaluate \\(Y\\) times: ArrayLike (T,) (optional) The array of times which the processes correspond to"
  },
  {
    "objectID": "reference/IDEM_Model.html",
    "href": "reference/IDEM_Model.html",
    "title": "1 IDEM_Model",
    "section": "",
    "text": "IDEM.IDEM_Model(\n    self,\n    process_basis,\n    kernel,\n    process_grid,\n    sigma2_eta,\n    sigma2_eps,\n    beta,\n    int_grid=create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n    m_0=None,\n    sigma2_0=None,\n)\nThe Integro-differential Equation Model.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\ndata_mle_fit\nMAY BE OUT OF DATE\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfilter_information\nNOT IMPLEMENTED\n\n\nfit_information_filter\nNOT FULLY IMPLEMENTED\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function simIDEM.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\n\n\n\nIDEM.IDEM_Model.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nIDEM.IDEM_Model.data_mle_fit(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nMAY BE OUT OF DATE\n\n\n\nIDEM.IDEM_Model.filter(obs_data_wide, X_obs)\nRuns the Kalman filter on the inputted data.\n\n\n\nIDEM.IDEM_Model.filter_information(obs_data, X_obs, nu_0=None, Q_0=None)\nNOT IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.fit_information_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nNOT FULLY IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nIDEM.IDEM_Model.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from the model, using the jit-able function simIDEM.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the ST_Data_Long type (see utilities)\n\n\n\n\n\n\n\nIDEM.IDEM_Model.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the"
  },
  {
    "objectID": "reference/IDEM_Model.html#methods",
    "href": "reference/IDEM_Model.html#methods",
    "title": "1 IDEM_Model",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\ndata_mle_fit\nMAY BE OUT OF DATE\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfilter_information\nNOT IMPLEMENTED\n\n\nfit_information_filter\nNOT FULLY IMPLEMENTED\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function simIDEM.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\n\n\n\nIDEM.IDEM_Model.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nIDEM.IDEM_Model.data_mle_fit(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nMAY BE OUT OF DATE\n\n\n\nIDEM.IDEM_Model.filter(obs_data_wide, X_obs)\nRuns the Kalman filter on the inputted data.\n\n\n\nIDEM.IDEM_Model.filter_information(obs_data, X_obs, nu_0=None, Q_0=None)\nNOT IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.fit_information_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nits=10,\n)\nNOT FULLY IMPLEMENTED\n\n\n\nIDEM.IDEM_Model.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nIDEM.IDEM_Model.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from the model, using the jit-able function simIDEM.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the ST_Data_Long type (see utilities)\n\n\n\n\n\n\n\nIDEM.IDEM_Model.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the"
  },
  {
    "objectID": "reference/Grid.html",
    "href": "reference/Grid.html",
    "title": "1 Grid",
    "section": "",
    "text": "1 Grid\nutilities.Grid()\nA simple grid class to store (currently exclusively regular) grids, along with some key quantities such as the lenth between grid points, the number of grid points and the area/volume of each grid square/cube. Supports arbitrarily high dimension. Ideally, in the future, this will support non-regular grids with any necessary quantities to do, for example, integration over the points on the grid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "",
<<<<<<< HEAD
    "text": "Code\nimport jax\nimport os\nimport jax.numpy as jnp\nimport jaxidem.utils as utils\nimport jaxidem.idem as idem\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageSequence\n\nseed = 4\nkey = jax.random.PRNGKey(seed)\nkeys = jax.random.split(key, 10)\n\nprocess_basis = utils.place_cosine_basis(N = 10)\n\n#Sigma_eta = jnp.diag(0.1*jnp.arange(process_basis.nbasis, dtype=\"float32\"))\n#Sigma_eta = jnp.diag(rand.normal(key, shape=(process_basis.nbasis,))**2)\nsigma2_eta = jnp.diag((0.01*jnp.ones(process_basis.nbasis)).at[1].set(40.0).at[30].set(80.0).at[31].set(60.0))\n\nmodel = idem.gen_example_idem(keys[0], k_spat_inv=False, ngrid=jnp.array([40, 40]), process_basis = process_basis, sigma2_eta = sigma2_eta)\n\n# Simulation\nT = 35\nnobs = 50\n\nobs_locs = jax.random.uniform(\n                keys[0],\n                shape=(nobs, 2),\n                minval=0,\n                maxval=1,\n            )\n\nobs_locs_tree = [obs_locs for _ in range(T)]\n\nX_obs_tree = [jnp.column_stack([jnp.ones(nobs), obs_locs]) for _ in range(T)]\n\nalphas = model.simulate_basis(keys[1], T)\nprocess_data = idem.basis_params_to_st_data(alphas, self.process_basis, self.process_grid)\n\nprocess_data, obs_data = model.simulate(keys[1], obs_locs_tree = obs_locs_tree, X_obs_tree = X_obs_tree)\n\ndpi = 200\nwidth = 576 / dpi\nheight = 480 / dpi\n\n# plot the objects\nutils.gif_st_grid(process_data, \"site/figure/process.gif\", width=width, height=height)\nutils.gif_st_pts(obs_data, \"site/figure/obs.gif\", width=width, height=height)\nmodel.kernel.save_plot(\"site/figure/kernel.png\", width=width, height=height)\n \ngif1 = Image.open('site/figure/process.gif')\ngif2 = Image.open('site/figure/tardis.gif')\n\nwidth, height = gif1.size\n\nframes = []\nnum_frames_gif1 = len(list(ImageSequence.Iterator(gif1)))\nnum_frames_gif2 = len(list(ImageSequence.Iterator(gif2)))\nmax_frames = max(num_frames_gif1, num_frames_gif2)\n\nfor i in range(max_frames):\n    frame1 = ImageSequence.Iterator(gif1)[i % num_frames_gif1].convert(\"RGBA\")\n    frame2 = ImageSequence.Iterator(gif2)[i % num_frames_gif2].convert(\"RGBA\")\n\n    frame2 = frame2.resize((width, height), Image.LANCZOS)\n    \n    combined = Image.alpha_composite(frame1, frame2)\n    frames.append(combined)\n\n\nframes[0].save('site/figure/process.gif', save_all=True, append_images=frames[1:], duration=gif1.info['duration'], loop=0)"
=======
    "text": "Code\nimport jax\nimport os\nimport jax.numpy as jnp\nimport jaxidem.utils as utils\nimport jaxidem.idem as idem\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageSequence\n\nseed = 4\nkey = jax.random.PRNGKey(seed)\nkeys = jax.random.split(key, 10)\n\nprocess_basis = utils.place_cosine_basis(N = 10)\n\n#Sigma_eta = jnp.diag(0.1*jnp.arange(process_basis.nbasis, dtype=\"float32\"))\n#Sigma_eta = jnp.diag(rand.normal(key, shape=(process_basis.nbasis,))**2)\nsigma2_eta = jnp.diag((0.01*jnp.ones(process_basis.nbasis)).at[1].set(40.0).at[30].set(80.0).at[31].set(60.0))\n\nmodel = idem.gen_example_idem(keys[0], k_spat_inv=False, ngrid=jnp.array([40, 40]), process_basis = process_basis, sigma2_eta = sigma2_eta)\n\n# Simulation\nT = 35\nnobs = 50\n\ncoords = jax.random.uniform(\n                keys[0],\n                shape=(nobs, 2),\n                minval=0,\n                maxval=1,\n            )\n\ntimes = jnp.repeat(jnp.arange(1, T + 1), coords.shape[0])\nrep_coords = jnp.tile(coords, (T, 1))\nx = rep_coords[:,0]\ny = rep_coords[:,1]\n\nprocess_data, obs_data = model.simulate(keys[1], x, y, times,\n                                        covariates = jnp.column_stack([x,y]),\n                                        covariate_labels=['Intercept', 'x', 'y'])\n\ndpi = 200\nwidth = 576 / dpi\nheight = 480 / dpi\n\n# plot the objects\nutils.gif_st_grid(process_data, \"site/figure/process.gif\", width=width, height=height)\nutils.gif_st_pts(obs_data, \"site/figure/obs.gif\", width=width, height=height)\nmodel.kernel.save_plot(\"site/figure/kernel.png\", width=width, height=height)\n \ngif1 = Image.open('site/figure/process.gif')\ngif2 = Image.open('site/figure/tardis.gif')\n\nwidth, height = gif1.size\n\nframes = []\nnum_frames_gif1 = len(list(ImageSequence.Iterator(gif1)))\nnum_frames_gif2 = len(list(ImageSequence.Iterator(gif2)))\nmax_frames = max(num_frames_gif1, num_frames_gif2)\n\nfor i in range(max_frames):\n    frame1 = ImageSequence.Iterator(gif1)[i % num_frames_gif1].convert(\"RGBA\")\n    frame2 = ImageSequence.Iterator(gif2)[i % num_frames_gif2].convert(\"RGBA\")\n\n    frame2 = frame2.resize((width, height), Image.LANCZOS)\n    \n    combined = Image.alpha_composite(frame1, frame2)\n    frames.append(combined)\n\n\nframes[0].save('site/figure/process.gif', save_all=True, append_images=frames[1:], duration=gif1.info['duration'], loop=0)"
>>>>>>> 9deb318 (Testing and re-writing simulation and st_data)
  },
  {
    "objectID": "index.html#the-technicalities",
    "href": "index.html#the-technicalities",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "1 The Technicalities",
    "text": "1 The Technicalities\nFor a rundown of the mathematics underpinning this model and implementation, see here."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "2 Documentation",
    "text": "2 Documentation\nDocumentation for the package is available here."
  },
  {
    "objectID": "index.html#other-sections",
    "href": "index.html#other-sections",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "3 Other sections",
    "text": "3 Other sections\nIDEM fit example\nFiltering example\nSydney Radar example"
  },
  {
    "objectID": "reference/Basis.html",
    "href": "reference/Basis.html",
    "title": "1 Basis",
    "section": "",
    "text": "utilities.Basis()\nA simple class for spatial basis expansions.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvfun\nArrayLike (ndim,) -&gt; ArrayLike (nbasis,)\nApplying to a single spatial location, evaluates all the basis functions on the single location and returns the result as a vector.\n\n\nmfun\nArrayLike (ndim, n) -&gt; ArrayLike (nbasis, n)\nApplying to a array of spatial points, evaluates all the basis functions on each point and returns the results in a matrix.\n\n\nparams\nArrayLike(nparams, nbasis)\nThe parameters defining each basis function. For example, for bisquare basis functions, the parameters are the locations of the centers of each function, and the function’s scale.\n\n\nnbasis\nint\nThe number of basis functions in the expansion."
  },
  {
    "objectID": "reference/Basis.html#attributes",
    "href": "reference/Basis.html#attributes",
    "title": "1 Basis",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nvfun\nArrayLike (ndim,) -&gt; ArrayLike (nbasis,)\nApplying to a single spatial location, evaluates all the basis functions on the single location and returns the result as a vector.\n\n\nmfun\nArrayLike (ndim, n) -&gt; ArrayLike (nbasis, n)\nApplying to a array of spatial points, evaluates all the basis functions on each point and returns the results in a matrix.\n\n\nparams\nArrayLike(nparams, nbasis)\nThe parameters defining each basis function. For example, for bisquare basis functions, the parameters are the locations of the centers of each function, and the function’s scale.\n\n\nnbasis\nint\nThe number of basis functions in the expansion."
  },
  {
    "objectID": "reference/IDEM.html",
    "href": "reference/IDEM.html",
    "title": "1 IDEM",
    "section": "",
    "text": "idem.IDEM(\n    self,\n    process_basis,\n    kernel,\n    process_grid,\n    sigma2_eta,\n    sigma2_eps,\n    beta,\n    int_grid=create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n)\nThe Integro-differential Equation Model. I’m really going back and forth on what to name this clas\n\n\n\n\n\nName\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfit_information_filter\nFits a new model by maximum likelihood estimation, maximizing the\n\n\nfit_kalman_filter\nFits a new model by maximum likelihood estimation, maximizing the\n\n\nfit_sqrt_filter\nFits a new model by maximum likelihood estimation, maximizing the\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function sim_idem.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\nsqrt_filter\nRuns the Kalman filter on the inputted data.\n\n\n\n\n\nidem.IDEM.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nidem.IDEM.filter(obs_data, X_obs, m_0=None, P_0=None, likelihood='full')\nRuns the Kalman filter on the inputted data.\n\n\n\nidem.IDEM.fit_information_filter(\n    obs_data,\n    X_obs_tuple,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nu_0=None,\n    Q_0=None,\n    debug=False,\n    max_its=100,\n    target_ll=jnp.inf,\n    likelihood='partial',\n    eps=None,\n    loading_bar=True,\n)\nFits a new model by maximum likelihood estimation, maximizing the data likelihood, computed by the information filter (inverse Kalman filter), using a given OPTAX optimiser.\n\n\nobs_data: st_data The observed data, as an st_data object containing the data to be fit to. X_obs_tuple: tuple Tuple of matrices of covariate data, where p is the number of covariates (including a column of 1s) fixed_ind: list = [] List of strings representing the variables to keep fixed at the value in self. Possible values; “sigma2_eps”, “sigma2_eta”, “ks1”, “ks2”, “ks3”, “ks4”, “beta”. lower: tuple = None Lower bounds on the parameters upper:tuple = None Upper bounds on the parameters optimizer: Callable = optax.adam(1e-3) Optimiser to use (see here for available options) nu_0: ArrayLike = None (r,) Initial information vector for information filter Q_0: ArrayLike = None (r,r) Initial Information matrix for information filter debug: bool = False Whether to print diagnostics during the fitting max_its: int = 100 Maximum number of iterations to perform (if other stopping rules don’t stop the loop early) target_ll: ArrayLike = jnp.inf Target log likelihood which, once reached, the main loop will stop early likelihood: str = ‘partial’ Type of likelihood for computation (‘full’ or ‘partial’). eps: float = None How close two loops should be before the loop is stopped early (None removes this stopping rule loading_bar:bool = True Displays a tqdm bar during the main loop.\n\n\n\nA tuple containing a new, fitted idem.IDEM object and the corresponding parameters.\n\n\n\n\nidem.IDEM.fit_kalman_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    m_0=None,\n    P_0=None,\n    debug=False,\n    max_its=100,\n    target_ll=jnp.inf,\n    likelihood='partial',\n    eps=None,\n    loading_bar=True,\n)\nFits a new model by maximum likelihood estimation, maximizing the data likelihood, computed by the standard Kalman filter, using a given OPTAX optimiser.\n\n\nobs_data: st_data The observed data, as an st_data object containing the data to be fit to. X_obs: ArrayLike (nobs, p) Matrix of covariate data, where p is the number of covariates (including a column of 1s) fixed_ind: list = [] List of strings representing the variables to keep fixed at the value in self. Possible values; “sigma2_eps”, “sigma2_eta”, “ks1”, “ks2”, “ks3”, “ks4”, “beta”. lower: tuple = None Lower bounds on the parameters upper:tuple = None Upper bounds on the parameters optimizer: Callable = optax.adam(1e-3) Optimiser to use (see here for available options) m_0: ArrayLike = None (r,) Initial mean vector for Kalman filter P_0: ArrayLike = None (r,r) Initial Variance matrix for Kalman filter debug: bool = False Whether to print diagnostics during the fitting max_its: int = 100 Maximum number of iterations to perform (if other stopping rules don’t stop the loop early) target_ll: ArrayLike = jnp.inf Target log likelihood which, once reached, the main loop will stop early likelihood: str = ‘partial’ Type of likelihood for computation (‘full’ or ‘partial’). eps: float = None How close two loops should be before the loop is stopped early (None removes this stopping rule loading_bar:bool = True Displays a tqdm bar during the main loop.\n\n\n\nA tuple containing a new, fitted idem.IDEM object and the corresponding parameters.\n\n\n\n\nidem.IDEM.fit_sqrt_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    m_0=None,\n    U_0=None,\n    debug=False,\n    max_its=100,\n    target_ll=jnp.inf,\n    likelihood='partial',\n    eps=None,\n    loading_bar=True,\n)\nFits a new model by maximum likelihood estimation, maximizing the data likelihood, computed by the square-root Kalman filter, using a given OPTAX optimiser.\nThis can be more stable than the standard Kalman, and in some situations can be run in Single-precision mode\n\n\nobs_data: st_data The observed data, as an st_data object containing the data to be fit to. X_obs: ArrayLike (nobs, p) Matrix of covariate data, where p is the number of covariates (including a column of 1s) fixed_ind: list = [] List of strings representing the variables to keep fixed at the value in self. Possible values; “sigma2_eps”, “sigma2_eta”, “ks1”, “ks2”, “ks3”, “ks4”, “beta”. lower: tuple = None Lower bounds on the parameters upper:tuple = None Upper bounds on the parameters optimizer: Callable = optax.adam(1e-3) Optimiser to use (see here for available options) m_0: ArrayLike = None (r,) Initial mean vector for Kalman filter U_0: ArrayLike = None (r,r) Initial square-root Variance matrix for square root filter debug: bool = False Whether to print diagnostics during the fitting max_its: int = 100 Maximum number of iterations to perform (if other stopping rules don’t stop the loop early) target_ll: ArrayLike = jnp.inf Target log likelihood which, once reached, the main loop will stop early likelihood: str = ‘partial’ Type of likelihood for computation (‘full’ or ‘partial’). eps: float = None How close two loops should be before the loop is stopped early (None removes this stopping rule loading_bar:bool = True Displays a tqdm bar during the main loop.\n\n\n\nA tuple containing a new, fitted idem.IDEM object and the corresponding parameters.\n\n\n\n\nidem.IDEM.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nidem.IDEM.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n    alpha_0=None,\n)\nSimulates from the model, using the jit-able function sim_idem.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the st_data type (see utilities)\n\n\n\n\n\n\n\nidem.IDEM.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the\n\n\n\nidem.IDEM.sqrt_filter(obs_data, X_obs, m_0=None, U_0=None, likelihood='full')\nRuns the Kalman filter on the inputted data."
  },
  {
    "objectID": "reference/IDEM.html#methods",
    "href": "reference/IDEM.html#methods",
    "title": "1 IDEM",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncon_M\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nfilter\nRuns the Kalman filter on the inputted data.\n\n\nfit_information_filter\nFits a new model by maximum likelihood estimation, maximizing the\n\n\nfit_kalman_filter\nFits a new model by maximum likelihood estimation, maximizing the\n\n\nfit_sqrt_filter\nFits a new model by maximum likelihood estimation, maximizing the\n\n\nlag1smooth\nNOT FULLY IMPLEMENTED OR TESTED\n\n\nsimulate\nSimulates from the model, using the jit-able function sim_idem.\n\n\nsmooth\nRuns the Kalman smoother on the\n\n\nsqrt_filter\nRuns the Kalman filter on the inputted data.\n\n\n\n\n\nidem.IDEM.con_M(ks)\nCreates the propegation matrix, M, with a given set of kernel parameters.\n\n\nks: PyTree(ArrayLike) The kernel parameters used to construct the matrix (must match the structure of self.kernel.params).\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nM\nArrayLike(r, r)\nThe propegation matrix M.\n\n\n\n\n\n\n\nidem.IDEM.filter(obs_data, X_obs, m_0=None, P_0=None, likelihood='full')\nRuns the Kalman filter on the inputted data.\n\n\n\nidem.IDEM.fit_information_filter(\n    obs_data,\n    X_obs_tuple,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    nu_0=None,\n    Q_0=None,\n    debug=False,\n    max_its=100,\n    target_ll=jnp.inf,\n    likelihood='partial',\n    eps=None,\n    loading_bar=True,\n)\nFits a new model by maximum likelihood estimation, maximizing the data likelihood, computed by the information filter (inverse Kalman filter), using a given OPTAX optimiser.\n\n\nobs_data: st_data The observed data, as an st_data object containing the data to be fit to. X_obs_tuple: tuple Tuple of matrices of covariate data, where p is the number of covariates (including a column of 1s) fixed_ind: list = [] List of strings representing the variables to keep fixed at the value in self. Possible values; “sigma2_eps”, “sigma2_eta”, “ks1”, “ks2”, “ks3”, “ks4”, “beta”. lower: tuple = None Lower bounds on the parameters upper:tuple = None Upper bounds on the parameters optimizer: Callable = optax.adam(1e-3) Optimiser to use (see here for available options) nu_0: ArrayLike = None (r,) Initial information vector for information filter Q_0: ArrayLike = None (r,r) Initial Information matrix for information filter debug: bool = False Whether to print diagnostics during the fitting max_its: int = 100 Maximum number of iterations to perform (if other stopping rules don’t stop the loop early) target_ll: ArrayLike = jnp.inf Target log likelihood which, once reached, the main loop will stop early likelihood: str = ‘partial’ Type of likelihood for computation (‘full’ or ‘partial’). eps: float = None How close two loops should be before the loop is stopped early (None removes this stopping rule loading_bar:bool = True Displays a tqdm bar during the main loop.\n\n\n\nA tuple containing a new, fitted idem.IDEM object and the corresponding parameters.\n\n\n\n\nidem.IDEM.fit_kalman_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    m_0=None,\n    P_0=None,\n    debug=False,\n    max_its=100,\n    target_ll=jnp.inf,\n    likelihood='partial',\n    eps=None,\n    loading_bar=True,\n)\nFits a new model by maximum likelihood estimation, maximizing the data likelihood, computed by the standard Kalman filter, using a given OPTAX optimiser.\n\n\nobs_data: st_data The observed data, as an st_data object containing the data to be fit to. X_obs: ArrayLike (nobs, p) Matrix of covariate data, where p is the number of covariates (including a column of 1s) fixed_ind: list = [] List of strings representing the variables to keep fixed at the value in self. Possible values; “sigma2_eps”, “sigma2_eta”, “ks1”, “ks2”, “ks3”, “ks4”, “beta”. lower: tuple = None Lower bounds on the parameters upper:tuple = None Upper bounds on the parameters optimizer: Callable = optax.adam(1e-3) Optimiser to use (see here for available options) m_0: ArrayLike = None (r,) Initial mean vector for Kalman filter P_0: ArrayLike = None (r,r) Initial Variance matrix for Kalman filter debug: bool = False Whether to print diagnostics during the fitting max_its: int = 100 Maximum number of iterations to perform (if other stopping rules don’t stop the loop early) target_ll: ArrayLike = jnp.inf Target log likelihood which, once reached, the main loop will stop early likelihood: str = ‘partial’ Type of likelihood for computation (‘full’ or ‘partial’). eps: float = None How close two loops should be before the loop is stopped early (None removes this stopping rule loading_bar:bool = True Displays a tqdm bar during the main loop.\n\n\n\nA tuple containing a new, fitted idem.IDEM object and the corresponding parameters.\n\n\n\n\nidem.IDEM.fit_sqrt_filter(\n    obs_data,\n    X_obs,\n    fixed_ind=[],\n    lower=None,\n    upper=None,\n    optimizer=optax.adam(0.001),\n    m_0=None,\n    U_0=None,\n    debug=False,\n    max_its=100,\n    target_ll=jnp.inf,\n    likelihood='partial',\n    eps=None,\n    loading_bar=True,\n)\nFits a new model by maximum likelihood estimation, maximizing the data likelihood, computed by the square-root Kalman filter, using a given OPTAX optimiser.\nThis can be more stable than the standard Kalman, and in some situations can be run in Single-precision mode\n\n\nobs_data: st_data The observed data, as an st_data object containing the data to be fit to. X_obs: ArrayLike (nobs, p) Matrix of covariate data, where p is the number of covariates (including a column of 1s) fixed_ind: list = [] List of strings representing the variables to keep fixed at the value in self. Possible values; “sigma2_eps”, “sigma2_eta”, “ks1”, “ks2”, “ks3”, “ks4”, “beta”. lower: tuple = None Lower bounds on the parameters upper:tuple = None Upper bounds on the parameters optimizer: Callable = optax.adam(1e-3) Optimiser to use (see here for available options) m_0: ArrayLike = None (r,) Initial mean vector for Kalman filter U_0: ArrayLike = None (r,r) Initial square-root Variance matrix for square root filter debug: bool = False Whether to print diagnostics during the fitting max_its: int = 100 Maximum number of iterations to perform (if other stopping rules don’t stop the loop early) target_ll: ArrayLike = jnp.inf Target log likelihood which, once reached, the main loop will stop early likelihood: str = ‘partial’ Type of likelihood for computation (‘full’ or ‘partial’). eps: float = None How close two loops should be before the loop is stopped early (None removes this stopping rule loading_bar:bool = True Displays a tqdm bar during the main loop.\n\n\n\nA tuple containing a new, fitted idem.IDEM object and the corresponding parameters.\n\n\n\n\nidem.IDEM.lag1smooth(Ps, Js, K_T, PHI_obs)\nNOT FULLY IMPLEMENTED OR TESTED\n\n\n\nidem.IDEM.simulate(\n    key,\n    obs_locs=None,\n    fixed_data=True,\n    nobs=100,\n    T=9,\n    int_grid=create_grid(bounds, ngrids),\n    alpha_0=None,\n)\nSimulates from the model, using the jit-able function sim_idem.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n\nPRNG key\nrequired\n\n\nobs_locs\n\nthe observation locations in long format. This should be a (3, n) array where the first column corresponds to time, and the last two to spatial coordinates. If this is not provided, 50 random points per time are chosen in the domain of interest.d\nNone\n\n\nint_grid\nGrid\nThe grid over which to compute the Riemann integral.\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nA tuple containing the Process data and the Observed data, both in long format in the st_data type (see utilities)\n\n\n\n\n\n\n\nidem.IDEM.smooth(ms, Ps, mpreds, Ppreds)\nRuns the Kalman smoother on the\n\n\n\nidem.IDEM.sqrt_filter(obs_data, X_obs, m_0=None, U_0=None, likelihood='full')\nRuns the Kalman filter on the inputted data."
  },
  {
    "objectID": "reference/Kernel.html",
    "href": "reference/Kernel.html",
    "title": "1 Kernel",
    "section": "",
    "text": "idem.Kernel(self, function, basis=None, params=None, form='expansion')\nGeneric class defining a kernel, or a basis expansion of a kernel with its parameters.\n\n\n\n\n\nName\nDescription\n\n\n\n\nsave_plot\nSaves a plot of the direction of the kernel.\n\n\nshow_plot\nShows a plot of the direction of the kernel.\n\n\n\n\n\nidem.Kernel.save_plot(filename, width=6, height=4, dpi=300, title=None)\nSaves a plot of the direction of the kernel.\n\n\n\nidem.Kernel.show_plot(width=5, height=4)\nShows a plot of the direction of the kernel."
  },
  {
    "objectID": "reference/Kernel.html#methods",
    "href": "reference/Kernel.html#methods",
    "title": "1 Kernel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsave_plot\nSaves a plot of the direction of the kernel.\n\n\nshow_plot\nShows a plot of the direction of the kernel.\n\n\n\n\n\nidem.Kernel.save_plot(filename, width=6, height=4, dpi=300, title=None)\nSaves a plot of the direction of the kernel.\n\n\n\nidem.Kernel.show_plot(width=5, height=4)\nShows a plot of the direction of the kernel."
  },
  {
    "objectID": "reference/bisquare.html",
    "href": "reference/bisquare.html",
    "title": "1 bisquare",
    "section": "",
    "text": "1 bisquare\nutilities.bisquare(s, params)\nGeneric bisquare function"
  },
  {
    "objectID": "reference/gen_example_idem.html",
    "href": "reference/gen_example_idem.html",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "idem.gen_example_idem(\n    key,\n    k_spat_inv=True,\n    ngrid=jnp.array([41, 41]),\n    nints=jnp.array([100, 100]),\n    nobs=50,\n    process_basis=None,\n    sigma2_eta=0.5 ** 2,\n    sigma2_eps=0.1 ** 2,\n    beta=jnp.array([0.0, 0.0, 0.0]),\n)\nCreates an example IDE model, with randomly generated kernel on the domain [0,1]x[0,1]. Intial value of the process is simply some of the coefficients for the process basis are set to 1. The kernel has a Gaussian shape, with parameters defined as basis expansions in order to allow for spatial variance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nk_spat_inv\nbool\nWhether or not the generated kernel should be spatially invariant.\nTrue\n\n\nngrid\nArrayLike\nThe resolution of the grid at which the process is computed. Should have shape (2,).\njnp.array([41, 41])\n\n\nnints\nArrayLike\nThe resolution of the grid at which Riemann integrals are computed. Should have shape (2,)\njnp.array([100, 100])\n\n\nnobs\nint\nThe number of observations per time interval.\n50\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA model of type IDEM."
  },
  {
    "objectID": "reference/gen_example_idem.html#parameters",
    "href": "reference/gen_example_idem.html#parameters",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nk_spat_inv\nbool\nWhether or not the generated kernel should be spatially invariant.\nTrue\n\n\nngrid\nArrayLike\nThe resolution of the grid at which the process is computed. Should have shape (2,).\njnp.array([41, 41])\n\n\nnints\nArrayLike\nThe resolution of the grid at which Riemann integrals are computed. Should have shape (2,)\njnp.array([100, 100])\n\n\nnobs\nint\nThe number of observations per time interval.\n50"
  },
  {
    "objectID": "reference/gen_example_idem.html#returns",
    "href": "reference/gen_example_idem.html#returns",
    "title": "1 gen_example_idem",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA model of type IDEM."
  },
  {
    "objectID": "reference/information_filter.html",
    "href": "reference/information_filter.html",
    "title": "1 information_filter",
    "section": "",
    "text": "filter_smoother_functions.information_filter(\n    nu_0,\n    Q_0,\n    M,\n    PHI_tree,\n    Sigma_eta,\n    Sigma_eps_tree,\n    zs_tree,\n    likelihood='partial',\n)\nWARNING: CURRENTLY NOT WORKING CORRECTLY\nApplies the information Filter (inverse-Kalman filter) to a PyTree of data points at a number of times. Unlike the Kalman filters, this allows for missing data and data changing shape, by taking a PyTree (most likely a list) of observations at each time (which can be jagged). For the standard Kalman filter with uncorrelated errors, see kalman_filter. For the square-root Kalman filter, sqrt_filter_indep.\nComputes posterior information vectors and information matrices for a system \\[\\begin{split}\n    \\mathbf Z_t &= \\Phi \\boldsymbol\\alpha_t + \\boldsymbol \\epsilon_t, \\quad t = 1,\\dots, T,\\\\\n    \\boldsymbol \\alpha_{t+1} &= M\\boldsymbol \\alpha_t + \\boldsymbol\\eta_t,\\quad t = 0,2,\\dots, T-1,\\\\\n\\end{split}\n\\] with initial ‘priors’ \\[\\begin{split}\n    \\boldsymbol \\alpha_{0} \\sim \\mathcal N(\\mathbf m_0, \\mathbf P_0),\\\\\n\\end{split}\n\\] where \\[\\begin{split}\n    \\boldsymbol \\epsilon_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0, \\Sigma_\\epsilon),\n    \\boldsymbol \\eta_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0, \\Sigma_\\eta).\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnu_0\nArrayLike\nThe initial information vector of the process vector\nrequired\n\n\nQ_0\nArrayLike\nThe initial information matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_tree\ntuple\nThe process-to-data matrices at each time\nrequired\n\n\nSigma_eta\nArrayLike\nThe variance of the process noise\nrequired\n\n\nSigma_eps_tree\ntuple\nThe variance matrices of the observation noise\nrequired\n\n\nzs_tree\ntuple\nThe observed data to be filtered\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nnus\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nQs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nnupreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nQpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/information_filter.html#parameters",
    "href": "reference/information_filter.html#parameters",
    "title": "1 information_filter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnu_0\nArrayLike\nThe initial information vector of the process vector\nrequired\n\n\nQ_0\nArrayLike\nThe initial information matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI_tree\ntuple\nThe process-to-data matrices at each time\nrequired\n\n\nSigma_eta\nArrayLike\nThe variance of the process noise\nrequired\n\n\nSigma_eps_tree\ntuple\nThe variance matrices of the observation noise\nrequired\n\n\nzs_tree\ntuple\nThe observed data to be filtered\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'"
  },
  {
    "objectID": "reference/information_filter.html#returns",
    "href": "reference/information_filter.html#returns",
    "title": "1 information_filter",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nnus\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nQs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nnupreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nQpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter.html",
    "href": "reference/kalman_filter.html",
    "title": "1 kalman_filter",
    "section": "",
    "text": "filter_smoother_functions.kalman_filter(\n    m_0,\n    P_0,\n    M,\n    PHI,\n    Sigma_eta,\n    Sigma_eps,\n    zs,\n    likelihood='partial',\n)\nApplies the Kalman Filter to a wide-format matrix of data. For jit-ability, this only allows full (no missing) data in a wide format. For changing data locations or changing data dimension, see information_filter. For the Kalman filter with uncorrelated errors, see kalman_filter_indep. For the square-root Kalman filter, sqrt_filter_indep.\nComputes posterior means and variances for a system \\[\\begin{split}\n    \\mathbf Z_t &= \\Phi \\boldsymbol\\alpha_t + \\boldsymbol \\epsilon_t, \\quad t = 1,\\dots, T,\\\\\n    \\boldsymbol \\alpha_{t+1} &= M\\boldsymbol \\alpha_t + \\boldsymbol\\eta_t,\\quad t = 0,2,\\dots, T-1,\\\\\n\\end{split}\n\\] with initial ‘priors’ \\[\\begin{split}\n    \\boldsymbol \\alpha_{0} \\sim \\mathcal N(\\mathbf m_0, \\mathbf P_0),\\\\\n\\end{split}\n\\] where \\[\\begin{split}\n    \\boldsymbol \\epsilon_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\Sigma_\\epsilon),\n    \\boldsymbol \\eta_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\Sigma_\\eta).\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nSigma_eta\nArrayLike\nThe Covariance matrix of the process noise\nrequired\n\n\nSigma_eps\nArrayLike\nThe Covariance matrix of the observation noise\nrequired\n\n\nzs\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nms\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nPs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nmpreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nPpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_filter.html#parameters",
    "href": "reference/kalman_filter.html#parameters",
    "title": "1 kalman_filter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm_0\nArrayLike\nThe initial means of the process vector\nrequired\n\n\nP_0\nArrayLike\nThe initial Covariance matrix of the process vector\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the process\nrequired\n\n\nPHI\nArrayLike\nThe process-to-data matrix\nrequired\n\n\nSigma_eta\nArrayLike\nThe Covariance matrix of the process noise\nrequired\n\n\nSigma_eps\nArrayLike\nThe Covariance matrix of the observation noise\nrequired\n\n\nzs\nArrayLike\nThe observed data to be filtered, in matrix format\nrequired\n\n\nlikelihood\nstr\n(STATIC) The mode to compute the likelihood (‘full’ with constant terms, ‘partial’ without constant terms, or ‘none’.)\n'partial'"
  },
  {
    "objectID": "reference/kalman_filter.html#returns",
    "href": "reference/kalman_filter.html#returns",
    "title": "1 kalman_filter",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nll\nArrayLike(1)\nThe log (data) likelihood of the data\n\n\nms\nArrayLike(T, r)\nThe posterior means \\(m_{t \\mid t}\\) of the process given the data 1:t\n\n\nPs\nArrayLike(T, r, r)\nThe posterior covariance matrices \\(P_{t \\mid t}\\) of the process given the data 1:t\n\n\nmpreds\nArrayLike(T - 1, r)\nThe predicted next-step means \\(m_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nPpreds\nArrayLike(T - 1, r, r)\nThe predicted next-step covariances \\(P_{t \\mid t-1}\\) of the process given the data 1:t-1\n\n\nKs\nArrayLike(T, n, r)\nThe Kalman Gains at each time step"
  },
  {
    "objectID": "reference/kalman_smoother.html",
    "href": "reference/kalman_smoother.html",
    "title": "1 kalman_smoother",
    "section": "",
    "text": "1 kalman_smoother\nfilter_smoother_functions.kalman_smoother(ms, Ps, mpreds, Ppreds, M)\nNOT FULLY IMPLEMENTED"
  },
  {
    "objectID": "reference/param_exp_kernel.html",
    "href": "reference/param_exp_kernel.html",
    "title": "1 param_exp_kernel",
    "section": "",
    "text": "1 param_exp_kernel\nidem.param_exp_kernel(K_basis, k)\nCreates a kernel in the style of AZM’s R-IDE package"
  },
  {
    "objectID": "reference/simIDEM.html",
    "href": "reference/simIDEM.html",
    "title": "1 simIDEM",
    "section": "",
    "text": "IDEM.simIDEM(\n    key,\n    T,\n    M,\n    PHI_proc,\n    PHI_obs,\n    obs_locs,\n    beta,\n    alpha0,\n    sigma2_eta=0.01 ** 2,\n    sigma2_eps=0.01 ** 2,\n    process_grid=create_grid(bounds, ngrids),\n    int_grid=create_grid(bounds, ngrids),\n)\nSimulates from a IDE model. For jit-ability, this only takes in certain parameters. For ease of use, use IDEM.simulate.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/simIDEM.html#parameters",
    "href": "reference/simIDEM.html#parameters",
    "title": "1 simIDEM",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkey\nArrayLike\nPRNG key\nrequired\n\n\nT\nint\nThe number of time points to simulate\nrequired\n\n\nM\nArrayLike\nThe transition matrix of the proces\nrequired\n\n\nPHI_proc\nArrayLike\nThe process basis coefficient matrix of the points on the process grid\nrequired\n\n\nPHI_obs\nArrayLike\nThe process basis coefficient matrices of the observation points, in block-diagonal form\nrequired\n\n\nbeta\nArrayLike\nThe covariate coefficients for the data\nrequired\n\n\nsigma2_eta\nfloat\nThe variance of the process noise (currently iid, will be a covariance matrix in the future)\n0.01 ** 2\n\n\nsigma2_eps\nfloat\nThe variance of the observation noise\n0.01 ** 2\n\n\nalpha0\nArrayLike\nThe initial value for the process basis coefficients\nrequired\n\n\nprocess_grid\nGrid\nThe grid at which to expand the process basis coefficients to the process function\ncreate_grid(bounds, ngrids)\n\n\nint_grid\nGrid\nThe grid to compute the Riemann integrals over (will be replaced with a better method soon)\ncreate_grid(bounds, ngrids)"
  },
  {
    "objectID": "reference/simIDEM.html#returns",
    "href": "reference/simIDEM.html#returns",
    "title": "1 simIDEM",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nA tuple containing the values of the process and the values of the\n\n\n\n\nobservation."
  },
  {
    "objectID": "reference/st_data.html",
    "href": "reference/st_data.html",
    "title": "1 st_data",
    "section": "",
    "text": "utilities.st_data(self, x, y, t, z)\nFor storing spatio-temporal data and appropriate methods for plotting such data, and converting between long and wide formats.\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_wide\nGives the data in wide format. Any missing data will be represented in\n\n\nsave_gif\nUNIMPLEMENTED\n\n\n\n\n\nutilities.st_data.as_wide()\nGives the data in wide format. Any missing data will be represented in the returned matris as NaN.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA dictionary containing the x coordinates and y coordinates as JAX\n\n\n\n\narrays, and a matrix corresponding to the value of the process at\n\n\n\n\neach time point (columns) and spatial point (rows).\n\n\n\n\n\n\n\n\nutilities.st_data.save_gif()\nUNIMPLEMENTED"
  },
  {
    "objectID": "reference/st_data.html#methods",
    "href": "reference/st_data.html#methods",
    "title": "1 st_data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nas_wide\nGives the data in wide format. Any missing data will be represented in\n\n\nsave_gif\nUNIMPLEMENTED\n\n\n\n\n\nutilities.st_data.as_wide()\nGives the data in wide format. Any missing data will be represented in the returned matris as NaN.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA dictionary containing the x coordinates and y coordinates as JAX\n\n\n\n\narrays, and a matrix corresponding to the value of the process at\n\n\n\n\neach time point (columns) and spatial point (rows).\n\n\n\n\n\n\n\n\nutilities.st_data.save_gif()\nUNIMPLEMENTED"
  },
  {
    "objectID": "site/filtering_and_smoothing.html",
    "href": "site/filtering_and_smoothing.html",
    "title": "Filtering in JAX-IDEM",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "site/filtering_and_smoothing.html#the-simple-model",
    "href": "site/filtering_and_smoothing.html#the-simple-model",
    "title": "Filtering in JAX-IDEM",
    "section": "1.1 The simple model",
<<<<<<< HEAD
    "text": "1.1 The simple model\nConsider the simple system, for \\(t=1,\\dots,T\\)\n\\[\\begin{split}\n\\vec\\alpha_{t+1} &= M\\vec\\alpha_t + \\vec\\eta_t,\\\\\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\alpha_0 = (1,1)^\\intercal\\) and\n\\[\\begin{split}\nM = \\left[\\begin{matrix}\n    \\cos(0.3) & -\\sin(0.3)\\\\\n    \\sin(0.3) & \\sin(0.3)\n\\end{matrix}\\right].\n\\end{split}\n\\tag{2}\\]\nThe error terms are mutually independant and have variances \\(\\sigma^{2}_\\epsilon=0.02\\) and \\(\\sigma^{2}_{\\eta}=0.03\\) and \\(\\vec z_t\\) are transformed linear ‘observations’ of \\(\\vec\\alpha\\)\n\\[\\begin{split}\n\\vec z_t &= \\Phi \\vec\\alpha_t + \\vec\\epsilon_t,\\\\\n\\Phi &= \\left[\\begin{matrix}\n1   & 0  \\\\\n0.6 & 0.4\\\\\n0.4 & 0.6\n\\end{matrix}\\right]\n\\end{split}.\n\\tag{3}\\]\nThe process, \\(\\alpha\\), simply spins in a circle with some noise. Lets simulate from this system;\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n\nimport jaxidem.filters as filt\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objs as go\n\nimport pandas as pd\n\n\n\n\n\n\nCode\nkey = jax.random.PRNGKey(1)\n\nalpha_0 = jnp.ones(2)  # 2D, easily plottable\nM = jnp.array([[jnp.cos(0.3), -jnp.sin(0.3)],\n              [jnp.sin(0.3), jnp.cos(0.3)]])  # spinny\n\nalphas = [alpha_0]\nzs = []\n\nT = 50\nkeys = jax.random.split(key, T*2)\n\nsigma2_eta = jnp.array(0.001)\nsigma2_eps = jnp.array(0.01)\n\nPHI = jnp.array([[1, 0], [0.6, 0.4], [0.4, 0.6]])\n\nfor i in range(T):\n    alphas.append(M @ alphas[i] + jnp.sqrt(sigma2_eta)*jax.random.normal(keys[2*i], shape=(2,)))\n    zs.append(PHI @ alphas[i+1] + jnp.sqrt(sigma2_eps)*jax.random.normal(keys[2*i+1], shape=(3,)))\n\nalphas_df = pd.DataFrame(alphas, columns = [\"x\", \"y\"])\nzs_df = pd.DataFrame(zs, columns = [\"x\", \"y\", \"z\"])\n\n\nalphas = jnp.array(alphas)\nzs = jnp.array(zs)\n\n    \nfig1 = px.line(alphas_df, x='x', y='y', height=200)\nfig1.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n\n\n\nFigure 1\n\n\n\n\n\nCode\nfig2 = go.Figure(data=[go.Scatter3d(\n    x=zs_df['x'],\n    y=zs_df['y'],\n    z=zs_df['z'],\n    mode='markers',\n    marker=dict(\n        symbol='cross',  # Change marker to cross\n        size=5           # Adjust marker size\n    )\n)])\n\nfig2.update_layout(height=200)\n    \nfig2.show()\n\n\n\n\n                            \n                                            \n\n\nFigure 2\n\n\n\n\nWe can see how the process is an odd random spiral, and the observations are skewed noisy observations of this in 3D space\nWith filtering, we aim to recover the process {fig-truth} from the observations {fig-obs}. We do this with two ‘forms’ of the filter, which should be equivalent."
=======
    "text": "1.1 The simple model\nConsider the simple system, for \\(t=1,\\dots,T\\)\n\\[\\begin{split}\n\\vec\\alpha_{t+1} &= M\\vec\\alpha_t + \\vec\\eta_t,\\\\\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\alpha_0 = (1,1)^\\intercal\\) and\n\\[\\begin{split}\nM = \\left[\\begin{matrix}\n    \\cos(0.3) & -\\sin(0.3)\\\\\n    \\sin(0.3) & \\sin(0.3)\n\\end{matrix}\\right].\n\\end{split}\n\\tag{2}\\]\nThe error terms are mutually independant and have variances \\(\\sigma^{2}_\\epsilon=0.02\\) and \\(\\sigma^{2}_{\\eta}=0.03\\) and \\(\\vec z_t\\) are transformed linear ‘observations’ of \\(\\vec\\alpha\\)\n\\[\\begin{split}\n\\vec z_t &= \\Phi \\vec\\alpha_t + \\vec\\epsilon_t,\\\\\n\\Phi &= \\left[\\begin{matrix}\n1   & 0  \\\\\n0.6 & 0.4\\\\\n0.4 & 0.6\n\\end{matrix}\\right]\n\\end{split}.\n\\tag{3}\\]\nThe process, \\(\\alpha\\), simply spins in a circle with some noise. Lets simulate from this system;\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n\nimport jaxidem.filters as filt\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objs as go\n\nimport pandas as pd\n\n\n\n\n\n\nCode\nkey = jax.random.PRNGKey(1)\n\nalpha_0 = jnp.ones(2)  # 2D, easily plottable\nM = jnp.array([[jnp.cos(0.3), -jnp.sin(0.3)],\n              [jnp.sin(0.3), jnp.cos(0.3)]])  # spinny\n\nalphas = [alpha_0]\nzs = []\n\nT = 50\nkeys = jax.random.split(key, T*2)\n\nsigma2_eta = jnp.array(0.001)\nsigma2_eps = jnp.array(0.01)\n\nPHI = jnp.array([[1, 0], [0.6, 0.4], [0.4, 0.6]])\n\nfor i in range(T):\n    alphas.append(M @ alphas[i] + jnp.sqrt(sigma2_eta)*jax.random.normal(keys[2*i], shape=(2,)))\n    zs.append(PHI @ alphas[i+1] + jnp.sqrt(sigma2_eps)*jax.random.normal(keys[2*i+1], shape=(3,)))\n\nalphas_df = pd.DataFrame(alphas, columns = [\"x\", \"y\"])\nzs_df = pd.DataFrame(zs, columns = [\"x\", \"y\", \"z\"])\n\n\nalphas = jnp.array(alphas)\nzs_tree = zs\n\n    \nfig1 = px.line(alphas_df, x='x', y='y', height=200)\nfig1.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n\n\n\nFigure 1\n\n\n\n\n\nCode\nfig2 = go.Figure(data=[go.Scatter3d(\n    x=zs_df['x'],\n    y=zs_df['y'],\n    z=zs_df['z'],\n    mode='markers',\n    marker=dict(\n        symbol='cross',  # Change marker to cross\n        size=5           # Adjust marker size\n    )\n)])\n\nfig2.update_layout(height=200)\n    \nfig2.show()\n\n\n\n\n                            \n                                            \n\n\nFigure 2\n\n\n\n\nWe can see how the process is an odd random spiral, and the observations are skewed noisy observations of this in 3D space\nWith filtering, we aim to recover the process {fig-truth} from the observations {fig-obs}. We do this with two ‘forms’ of the filter, which should be equivalent."
>>>>>>> 9deb318 (Testing and re-writing simulation and st_data)
  },
  {
    "objectID": "site/mathematics.html",
    "href": "site/mathematics.html",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "",
    "text": "The Integro-Difference equation model (here abbreviated as IDEM 1) is dynamics-based spatio-temporal aiming to model diffusion and convection by making the value of a process a weighted average of it’s previous time, plus noise.\n[NOTE: I intend to create a more thorough background for the introduction here.]"
  },
  {
    "objectID": "site/mathematics.html#process-decomposition",
    "href": "site/mathematics.html#process-decomposition",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.1 Process decomposition",
    "text": "3.1 Process decomposition\nChoose a complete class of spatial spectral basis functions, \\(\\{\\phi_i(\\cdot): \\mathcal D\\to \\mathbb R\\}_{i=1,\\dots}\\), and decompose the process spatial field at each time;\n\\[\\begin{split}\nY_t(\\bv s) \\approx \\sum_{i=1}^{r} \\alpha_{i,t} \\phi_i(\\bv s), \\quad t=0,\\dots,T.\n\\end{split}\n\\tag{3}\\]\nwhere we truncate the expansion at some \\(r\\in\\mathbb N\\). Notice that we can write this in vector/matrix form, where we consider the vector field \\(\\bv \\phi(\\cdot) = (\\phi_1(\\cdot),\\dots, \\phi_r(\\cdot))^\\intercal\\); considering times \\(t=1,2,\\dots, T\\), we set\n\\[\\begin{split}\n\\bv \\phi(\\bv s) &= (\\phi_1(\\bv s), \\phi_2(\\bv s), \\dots, \\phi_r(\\bv s))^{\\intercal},\\\\\n\\bv \\alpha_t &= (\\alpha_{1,t}, \\alpha_{2,t}, \\dots, \\alpha_{r, t})^{\\intercal}.\n\\end{split}\n\\tag{4}\\]\nNow, (Equation 3) gives us, for any \\(\\bv s\\in \\mathcal D\\),\n\\[\\begin{split}\nY(\\bv s; t) \\approx \\bv \\phi^{\\intercal}(\\bv s)  \\alpha(t).\\\\\n\\end{split}\n\\tag{5}\\]\nWe can effectively now work exclusively with \\(\\bv \\alpha_t = (\\alpha_{1,t},\\dots, \\alpha_{r,t})^\\intercal\\). To do so, we need to find the evolution equation of \\(\\bv \\alpha_t\\), as given below.\n\nTheorem 1 (Spectral form of the state evolution) Define the Gram matrix;\n\\[\\Psi \\coloneq \\int_{\\mathcal D_s} \\bv \\phi(\\bv s) \\bv \\phi(\\bv s)^\\intercal d\\bv s.\n\\tag{6}\\]\nThen, the basis coefficients evolve by the equation\n\\[\\bv \\alpha_{t+1} = M \\bv\\alpha_t + \\bv\\eta_t,\n\\tag{7}\\]\nwhere \\(M = \\Psi^{-1} \\int\\int \\bv\\phi(\\bv s) \\kappa(\\bv s, \\bv r)\\bv\\phi(\\bv r)^\\intercal d\\bv r d \\bv s\\) and \\(\\bv\\eta_t =\\Psi^{-1} \\int \\bv \\phi(\\bv s)\\omega_t(s)d\\bv s\\).\n\n\nProof. (Adapting from Dewar, Scerri, and Kadirkamanathan 2008), write out the process equation, (Equation 2), using the first equation of (Equation 5);\n\\[Y_{t+1}(\\bv s) = \\bv \\phi(\\bv s)^{\\intercal} \\alpha_{t+1} = \\int_{\\mathcal D_s} \\kappa(\\bv s, \\bv r) \\bv\\phi(\\bv r)^{\\intercal}\\bv \\alpha_t d\\bv r + \\omega_t(\\bv s),\n\\]\nWe then multiply both sides by \\(\\bv \\phi(s)\\) and integrate over \\(\\bv s\\)\n\\[\\begin{split}\n\\int_{\\mathcal D_s} \\bv\\phi(\\bv s)\\bv\\phi(\\bv s)^{\\intercal} d\\bv s \\bv\\alpha_{t+1} &= \\int\\bv\\phi(\\bv s)\\int \\kappa(\\bv s, \\bv r)\\bv\\phi(\\bv r)^\\intercal d\\bv r  d \\bv s\\ \\bv\\alpha_t + \\int \\bv \\phi(\\bv s)\\omega_t(s)d\\bv s\\\\\n\\Psi \\bv\\alpha_{t+1} &= \\int\\int \\bv\\phi(\\bv s)\\kappa(\\bv s, \\bv r) \\bv\\phi(\\bv r)^\\intercal d\\bv r d \\bv s\\ \\bv\\alpha_t + \\int \\bv \\phi(\\bv s)\\omega_t(s)d\\bv s.\n\\end{split}\n\\]\nSo, finally, pre-multipling by the inverse of the gram matrix, \\(\\Psi^{-1}\\) (Equation 6), we arrive at the result."
  },
  {
    "objectID": "site/mathematics.html#spectral-form-of-the-process-noise",
    "href": "site/mathematics.html#spectral-form-of-the-process-noise",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.2 Spectral form of the Process Noise",
    "text": "3.2 Spectral form of the Process Noise\nWe still have to set out what the process noise, \\(\\omega_t(\\bv s)\\), and it’s spectral counterpart, \\(\\bv \\eta_t\\), are. Dewar, Scerri, and Kadirkamanathan (2008) fix the variance of \\(\\omega_t(\\bv s)\\) to be uniform and uncorrelated across space and time, with \\(\\omega_t(\\bv s) \\sim \\mathcal N(0,\\sigma^2)\\) It is then easily shown that \\(\\bv\\eta_t\\) is also normal, with \\(\\bv\\eta_t \\sim \\mathcal N(0, \\sigma^2\\Psi^{-1})\\).\nHowever, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of \\(\\bv\\eta_t\\), and compute the variance of \\(\\omega_t(\\bv s)\\) if needed.\n\nLemma 1 Let \\(\\bv\\eta_t \\sim \\mathcal N(0, \\Sigma_\\eta)\\), and \\(\\cov[\\bv\\eta_t, \\bv \\eta_{t+\\tau}] =0\\), \\(\\forall \\tau&gt;0\\). Then \\(\\omega_t(\\bv s)\\) has covariance\n\\[\\cov [\\omega_t(\\bv s), \\omega_{t+\\tau}(\\bv r)] = \\begin{cases}\n\\bv\\phi(\\bv s)^\\intercal \\Sigma_\\eta \\bv\\phi(\\bv r) & \\text{if }\\tau=0\\\\\n0 & \\text{else}\\\\\n\\end{cases}\n\\]\n\n\nProof. Consider \\(\\Psi \\bv\\eta_t\\), and consider the case \\(\\tau=0\\). It is clearly normal, with zero expectation and variance (using Equation 6),\n\\[\\begin{split}\n\\var[\\Psi \\bv\\eta_t] &= \\Psi \\var[\\bv\\eta_t] \\Psi^\\intercal = \\Psi\\Sigma_\\eta\\Psi^\\intercal,\\\\\n&= \\int_{\\mathcal D_s} \\bv\\phi(\\bv s) \\bv\\phi(\\bv s)^\\intercal d\\bv s \\  \\Sigma_\\eta \\ \\int_{\\mathcal D_s} \\bv\\phi(\\bv r) \\bv\\phi(\\bv r)^\\intercal d\\bv r\\\\\n&=  \\int\\int_{\\mathcal D_s^2} \\bv\\phi(\\bv s) \\bv\\phi(\\bv s)^\\intercal \\  \\Sigma_\\eta \\  \\bv\\phi(\\bv r) \\bv\\phi(\\bv r)^\\intercal d\\bv r d\\bv s\\\\\n\\end{split}\n\\tag{8}\\]\nSince it has zero expectation, we also have\n\\[\\begin{split}\n\\var[\\Psi\\bv\\eta_t] &= \\mathbb E[(\\Psi\\bv\\eta_t) (\\Psi\\bv\\eta_t)^\\intercal] = \\mathbb E[\\Psi\\bv\\eta_t\\bv\\eta_t^\\intercal\\Psi^\\intercal]\\\\\n&= \\mathbb E \\left[ \\int_{\\mathcal D_s} \\bv\\phi(\\bv s)\\omega_t(\\bv s)d\\bv s \\int_{\\mathcal D_s} \\bv \\phi(\\bv r)^\\intercal \\omega_t(\\bv r) d\\bv r \\right]\\\\\n&= \\int\\int_{\\mathcal D_s^2} \\bv\\phi(\\bv s)\\  \\mathbb E[\\omega_t(\\bv s)\\omega_t(\\bv r)]\\  \\bv \\phi(\\bv r)^\\intercal d\\bv s d \\bv r.\n\\end{split}\n\\tag{9}\\]\nWe can see that, comparing (Equation 8) and (Equation 9), we have\n\\[\\cov [\\omega_t(\\bv s), \\omega_t(\\bv r)] = \\mathbb E[\\omega_t(\\bv s)\\omega_t(\\bv r)]= \\bv\\phi(\\bv s)^\\intercal \\Sigma_\\eta \\bv\\phi(\\bv r).\n\\]\nSince, once again, \\(\\mathbb E[\\bv\\omega_t(\\bv s)]=0\\).\nFor the \\(\\tau\\neq0\\) case, it is simple to show that the covariance is 0."
  },
  {
    "objectID": "site/mathematics.html#sec-kerneldecomp",
    "href": "site/mathematics.html#sec-kerneldecomp",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.3 Kernel Parameterisations",
    "text": "3.3 Kernel Parameterisations\nNext is the part of the system, which defines the dynamics; the kernel function, \\(\\kappa\\). There are a few ways to handle the kernel. One of the most obvious is to expand it out into a spectral decomposition as well;\n\\[\\kappa \\approx \\sum_i \\beta_i\\psi(\\bv s, \\bv r).\n\\]\nThis can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on \\(\\mathbb R^2\\times \\mathbb R^2\\); to get a wide enough space of possible functions, we would likely need many terms in the spectral expansion.\nA much simpler approach would be to simply parametrise the kernel function, to \\(\\kappa(\\bv s, \\bv r, \\bv \\theta_\\kappa)\\). We then establish a simple shape for the kernel (e.g. Gaussian) and rely on very few parameters (for example, scale, shape, offsets). The example kernel used in the jaxidem is a Gaussian-shape kernel;\n\\[\\kappa(\\bv s, \\bv r; \\bv m, a, b) = a \\exp \\left( -\\frac{1}{b} \\vert \\bv s- \\bv r +\\bv m\\vert^2 \\right).\n\\]\nOf course, this kernel lacks spatial dependence. We can add spatial variance back by adding dependence on \\(\\bv s\\) to the parameters, for example, varying the offset term as \\(\\bv m(\\bv s)\\). Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground (Cressie and Wikle 2015). The actual parameters of such a spatially-variant kernel are then the spectral coefficients for the expansion of any spatially variant parameters, as well as any constant parameters. This is precisely what is plotting in Figure 1, where the spectral coefficients are randomly sampled from a multivariate normal distribution;\n\\[\\begin{split}\n  \\bv m(\\bv s) = \\left(\\begin{matrix}\n    \\sum_{i=1}^{r_m} \\phi_{\\kappa,i}(\\bv s) m^{(x)}_i\\\\\n    \\sum_{i=1}^{r_m} \\phi_{\\kappa,i}(\\bv s) m^{(y)}_i\n  \\end{matrix}\\right),\n\\end{split}\n\\]\nwhere \\(m^{(x)}_i\\) and \\(m^{(y)}_i\\) are coefficients for the x and y coordinates respectively, and \\(\\phi_{\\kappa, i}(\\bv s)\\) are basis functions (e.g. bisquare 3) functions in Figure 1)."
  },
  {
    "objectID": "site/mathematics.html#idem-as-a-linear-dynamical-system",
    "href": "site/mathematics.html#idem-as-a-linear-dynamical-system",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.4 IDEM as a linear dynamical system",
    "text": "3.4 IDEM as a linear dynamical system\nTo summarise, we have taken a truncated spectral decomposition to write the Integro-difference equation model as a more traditional linear dynamical system form (Equation 7). All that is left is to include our observations in our system.\nLets assume that at each time \\(t\\) there are \\(n_t\\) observations at locations \\(\\bv s_{1,t},\\dots, \\bv s_{n_{t},t}\\). We write the vector of the process at these points as \\(\\bv Y(t) = (Y(s_{1,t};t), \\dots, Y(s_{n_{t},t};t))^\\intercal\\), and, in it’s expanded form \\(\\bv Y_t = \\Phi_t \\bv\\alpha_t\\), where \\(\\Phi \\in \\mathbb R^{r\\times n_{t}}\\) is\n\\[\\begin{split}\n\\{\\Phi_{t}\\}_{i, j} = \\phi_{i}(s_{j,t}).\n\\end{split}\n\\]\nFor the covariates, we write the matrix \\(X_t = (\\bv X(\\bv s_{1, t}), \\dots, \\bv X(\\bv s_{1=n_{t}, t})^\\intercal\\). We then have\n\\[\\begin{split}\n\\bv Z_t &= \\Phi \\bv \\alpha_t + X_{t} \\bv \\beta + \\bv \\epsilon_t, \\quad t = 1,\\dots, T,\\\\\n\\bv \\alpha_{t+1} &= M\\bv \\alpha_t + \\bv\\eta_t,\\quad t = 0,1,\\dots, T-1,\\\\\nM &= \\int_{\\mathcal D_s}\\bv\\phi(\\bv s) \\bv\\phi(\\bv s)^\\intercal d\\bv s \\int_{\\mathcal D_s^2}\\bv\\phi(\\bv s) \\kappa(\\bv s, \\bv r; \\bv\\theta_\\kappa)\\bv\\phi(\\bv r)^\\intercal d\\bv r d \\bv s,\n\\end{split}\n\\]\nWriting \\(\\tilde{\\bv{Z}}_t = \\bv Z_t - X_t \\bv \\beta\\),\n\\[\\begin{split}\n\\tilde{\\bv Z}_t &= \\Phi_{t} \\bv \\alpha_t + \\bv \\epsilon_t,\\quad &t = 1,2,\\dots, T,\\\\\n\\bv \\alpha_{t+1} &= M \\bv \\alpha_t + \\bv\\eta_t,\\quad &t = 0,1, \\dots, T.\\\\\n\\end{split}\n\\tag{10}\\]\nWe should also initialise \\(\\bv \\alpha_0 \\sim \\mathcal N^{r}(\\bv m_{0}, \\Sigma_{0})\\), and fix simple distributions to the noise terms,\n\\[\\begin{split}\n\\bv \\epsilon_{t} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N_{n_\\mathrm{obs}}(0,\\Sigma_\\epsilon),\\\\\n\\bv \\eta_{t} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N_{R}(0,\\Sigma_\\eta),\n\\end{split}\n\\]\nwhich are independent in time.\nAs in, for example, (Wikle and Cressie 1999), Equation 10 is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood. We can use these quantities in either an EM algorithm or a Bayesian approach, or directly maximise the marginal data likelihood\nWe now move on to an example simulation of this kind of model using its spectral decomposition and jaxidem."
  },
  {
    "objectID": "site/mathematics.html#example-simulation",
    "href": "site/mathematics.html#example-simulation",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.5 Example Simulation",
    "text": "3.5 Example Simulation\nWe can now use the above to simulate easily from such models; once we have chosen the appropriate decompositions, we simply compute \\(M\\) and propagate \\(\\bv \\alpha_t\\) as we would when simulating any other linear dynamic system. We then use the spectral coefficients to generate \\(Y_t(\\bv s)\\) and \\(Z_t(\\bv s)\\) in the obvious way.\njaxidem implements this in the function sim_idem, or through the more user-friendly method idem.IDEM.simulate. An object of the IDEM class contains all the necessary information about basis decompositions, and the simulate methods calls simIDEM without compromising its jit-ability (although just-in-time computation obviously isn’t as important for simulation, the jit-ed function could save compile time if someone want to simulate from many models).\nThe gen_example_idem method creates a simple IDEM object without many required parameters;\n\n\nCode\nkey = jax.random.PRNGKey(42)\nkeys = rand.split(key, 2)\n\nmodel = idem.gen_example_idem(keys[0], k_spat_inv=False)\n\nprocess_data, obs_data = model.simulate(keys[1], T=3, nobs=50)\n\n\nThe resulting objects are of class st_data, containing a couple of niceties for handling spatio-temporal data, while still storing all data as JAX arrays. For example, the show_plot, save_plot and save_gif methods provide easy plotting;\n\n\nCode\nprocess_data.save_plot('figure/process_data_example.png')\nobs_data.save_plot('figure/obs_data_example.png')\n\n\n\n\n\n\n\n\n\n\nProcess Simulation\n\n\n\n\n\n\n\n\n\nObservation Simulation\n\n\n\n\n\n\nFigure 2: Example simulations from an Integro-difference Equation Model. Kernel is generated with spatially varying flow terms, generated by bisquare basis functions with randomly generated coefficient. Note that some artefacts from the decomposition are visible, such as a faint chequerboard pattern in the process."
  },
  {
    "objectID": "site/mathematics.html#sec-kalmanfilter",
    "href": "site/mathematics.html#sec-kalmanfilter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.1 The Kalman Filter",
    "text": "4.1 The Kalman Filter\nFirstly, we should establish some notation. Write\n\\[\\begin{split}\nm_{i \\mid j} &= \\mathbb E[\\bv\\alpha_i \\mid \\{\\bv Z_t=\\bv z_t\\}_{t=0,\\dots,j}],\\\\\nP_{i \\mid j} &= \\var[\\bv\\alpha_i \\mid \\{\\bv Z_t=\\bv z_t\\}_{t=0,\\dots,j}],\\\\\nP_{i,j \\mid k} &= \\cov[\\bv\\alpha_i, \\bv\\alpha_k \\mid \\{\\bv Z_t=\\bv z_t\\}_{t=0,\\dots,k}].\n\\end{split}\n\\]\nFor the initial terms, we choose Bayesian-like prior moments \\(m_{0\\mid0}=m_0\\) and \\(P_{0\\mid0}=\\Sigma_0\\). For convenience and generality, we write \\(\\Sigma_\\eta\\) and \\(\\Sigma_\\epsilon\\) for the variance matrices of the process and observations. Note that, if the number of observations change at each time point (for example, due to missing data), then \\(\\Sigma_\\epsilon\\) should be time varying (even in its shape); we could either always keep it as uncorrelated so that \\(\\Sigma_\\epsilon = \\mathrm{diag} (\\sigma_\\epsilon^2)\\), or perhaps put some kind of distance-dependant covariance function to it.\nTo move the filter forward, that is, given \\(m_{t\\mid t}\\) and \\(P_{t\\mid t}\\), to get \\(m_{t+1\\mid t+1}\\) and \\(P_{t+1\\mid t+1}\\), we first predict\n\\[\\begin{split}\n\\bv m_{t+1\\mid t} &= M \\bv m_{t\\mid t},\\\\\nP_{t+1\\mid t} &= M P_{t\\mid t} M^\\intercal + \\Sigma_\\eta,\n\\end{split}\n\\tag{11}\\]\nthen we add our new information, update, with \\(z_{t}\\);\n\\[\\begin{split}\n\\bv m_{t+1\\mid t+1} &= \\bv m_{t+1\\mid t} + K_{t+1} \\bv e_{t+1}\\\\\nP_{t+1\\mid t+1} &= [I- K_{t+1}\\Phi_{t+1}]P_{t+1\\mid t}\n\\end{split}\n\\tag{12}\\]\nwhere \\(K_{t+1}\\) is the Kalman gain;\n\\[\\begin{split}\nK_{t+1} = P_{t+1\\mid t}\\Phi_{t+1}^\\intercal [\\Phi_{t+1} P_{t+1\\mid t} \\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}, \\quad t=0,\\dots,T-1,\n\\end{split}\n\\]\nand \\(\\bv e_{t+1}\\) are the prediction errors\n\\[\\begin{split}\n\\bv e_{t+1} = \\tilde{\\bv z}_{t+1}-\\Phi_{t+1} \\bv m_{t+1\\mid t}, \\quad t=1,\\dots,T.\n\\end{split}\n\\]\nStarting with \\(m_0\\) and \\(P_0\\), we can then iteratively move across the data to eventually compute \\(m_{T\\mid T}\\) and \\(P_{T\\mid T}\\).\nAssuming Gaussian all random variables here are Gaussian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of linear operators.\nWe can compute the marginal data likelihood alongside the Kalman filter using the prediction errors \\(\\bv e_t\\). These, under the assumptions we have made about \\(\\bv \\eta_t\\) and \\(\\bv\\epsilon_t\\) being normal, are also normal with zero mean and variance\n\\[\\begin{split}\n\\mathbb V\\mathrm{ar}[\\bv e_t]=\\Sigma_t= \\Phi_{t} P_{t\\mid t-1} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon.\n\\end{split}\n\\tag{13}\\]\nTherefore, the log-likelihood at each time is\n\\[\\begin{split}\n\\mathcal L(Z\\mid\\bv\\theta) = -\\frac12\\sum \\log\\det(\\Sigma_t(\\bv\\theta)) - \\frac12 \\sum\\bv e_t(\\bv\\theta)^\\intercal\\Sigma_{t}(\\bv\\theta)^{-1} \\bv e_t(\\bv\\theta) - \\frac{n_{t}}{2}\\log(2*\\pi).\n\\end{split}\n\\]\nSumming these across time, we get the log likelihood for all the data.\nA simplified example of the Kalman filter function, written to be JAX compatible, used in the package is this;\n\n\nCode\n@jax.jit\ndef kalman_filter(m_, P_0, M, PHI_obs, Sigma_eta, Sigma_eps, ztildes):\n    nbasis = m_0.shape[0]\n    nobs = ztildes.shape[0]\n\n    @jax.jit\n    def step(carry, z_t):\n        m_tt, P_tt, _, _, ll, _ = carry\n\n        # predict\n        m_pred = M @ m_tt\n        P_pred = M @ P_tt @ M.T + Sigma_eta\n\n        # Update\n        # Prediction Errors\n        eps_t = z_t - PHI_obs @ m_pred\n\n        Sigma_t = PHI_obs @ P_pred @ PHI_obs.T + Sigma_eps\n        # Kalman Gain\n        K_t = (jnp.linalg.solve(Sigma_t, PHI_obs)@ P_pred.T).T\n\n        m_up = m_pred + K_t @ eps_t\n        P_up = (jnp.eye(nbasis) - K_t @ PHI_obs) @ P_pred\n\n        # likelihood of epsilon, using cholesky decomposition\n        ll_new = ll - 0.5 * n * jnp.log(2*jnp.pi) - \\\n            0.5 * jnp.log(jnp.linalg.det(Sigma_t)) -\\\n            0.5 * e.T @ jnp.linalg.solve(Sigma_t, e)\n\n        return (m_up, P_up, m_pred, P_pred, ll_new, K_t), (m_up, P_up, m_pred, P_pred, ll_new, K_t,)\n\n    carry, seq = jl.scan(\n        step,\n        (m_0, P_0, m_0, P_0, 0, jnp.zeros((nbasis, nobs))),\n        ztildes.T,\n    )\n\n    return (carry[4], seq[0], seq[1], seq[2][1:], seq[3][1:], seq[5][1:])\n\n\nFor the documentation of the method provided by the package, see filter_smoother_functions.kalman_filter."
  },
  {
    "objectID": "site/mathematics.html#the-information-filter",
    "href": "site/mathematics.html#the-information-filter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.2 The Information Filter",
    "text": "4.2 The Information Filter\nIn some computational scenarios, it is beneficial to work with vectors of consistent dimension. In Python JAX, the efficient scan method works only with such arrays; JAX has no support for jagged arrays, and traditional for loops will likely lead to long compile times when jit-compiled. Although there are some tools in JAX to get around this problem (namely the jax.tree functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it’s core, a scan-type operation (scanning over the data), this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.\nBut it is possible to re-write the Kalman filter in a way which is compatible with this kind of data. The ‘information filter’ (sometimes called inverse Kalman filter or other names) involves transforming the data into its ‘information form’, which will always have consistent dimension, allowing us to avoid jagged scans.\nThe information filter is simply the Kalman filter re-written to use the Gaussian distribution’s canonical parameters 4, those being the information vector and the information matrix. If a Gaussian distribution has mean \\(\\bv\\mu\\) and variance matrix \\(\\Sigma\\), then the corresponding information vector and information matrix is \\(\\nu = \\Sigma^{-1}\\mu\\) and \\(Q = \\Sigma^{-1}\\), correspondingly.\n\nTheorem 2 The Kalman filter can be rewritten in information form as follows (for example, Khan 2005). Write\n\\[\\begin{split}\nQ_{i\\mid j} &= P_{i\\mid j}^{-1}\\\\\n\\bv\\nu_{i\\mid j} &= Q_{i\\mid j} \\bv m_{i\\mid j}\n\\end{split}\n\\]\nand transform the observations into their ‘information form’, for \\(t=1,\\dots, T\\)\n\\[\\begin{split}\nI_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1}\\Phi_{t},\\\\\ni_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1} \\bv z_{t}.\n\\end{split}\n\\tag{14}\\]\nThe prediction step now becomes\n\\[\\begin{split}\n\\bv\\nu_{t+1\\mid t} &= (I-J_t) M^{-1}\\bv\\nu_{t\\mid t}\\\\\nQ_{t+1\\mid t} &= (I-J_t) S_{t}\n\\end{split}\n\\tag{15}\\]\nwhere \\(S_t = M^{-\\intercal} Q_{t\\mid t} M^{-1}\\) and \\(J_t = S_t [S_{t}+\\Sigma_{\\eta}^{-1}]^{-1}\\).\nUpdating is now as simple as adding the information-form observations;\n\\[\\begin{split}\n  \\bv\\nu_{t+1\\mid t+1} &= \\bv\\nu_{t+1\\mid t} + i_{t+1}\\\\\n  Q_{t+1\\mid t+1} &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\tag{16}\\]\n\nProof in Appendix (Section 7.2.)\nWe can see that the information form of the observations (Equation 14) will always have the same dimension 5. For our purposes, this means that jax.lax.scan will work after we ‘informationify’ the data, which can be done using jax.tree.map. This is implemented in the functions information_filter and information_filter_indep (for uncorrelated errors).\nThere are other often cited advantages to filtering in this form. It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with \\([S_t + \\Sigma_\\eta]^{-1}\\) in the process dimension instead of \\([\\Phi_t P_{t+1\\mid t} \\Phi_t^\\intercal + \\Sigma_\\epsilon]^{-1}\\) in the observation dimension) (Assimakis, Adam, and Douladiris 2012).\nThe other often mentioned advantage is the ability to use a flat prior for \\(\\alpha_0\\); that is, we can set \\(Q_0\\) as the zero matrix, without worrying about an infinite variance matrix. While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see Section 7.3.\nAs with the Kalman filter, it is also possible to get the data likelihood in-line as well. Again, we would like to stick with things in the state dimension, so working directly with the prediction errors \\(\\bv e_t\\) should be avoided. Luckily, by multiplying the errors by \\(\\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\), we can define the ‘information errors’ \\(\\bv \\iota_t\\);\n\\[\\begin{split}\n  \\bv \\iota_t &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\bv e_t = \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\tilde{\\bv z}_t -\\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_t m_{t\\mid t-1}\\\\\n  &= i_t - I_tQ_{t\\mid t-1}^{-1}\\bv \\nu_{t\\mid t-1}.\n\\end{split}\n\\]\nThe variance of this quantity is also easy to find;\n\\[\\begin{split}\n  \\var[\\bv \\iota_t] &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\var[\\bv e_t]\\Sigma_\\epsilon^{-1}\\Phi_t\\\\\n  &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} [\\Phi_{t} P_{t\\mid t-1} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon] \\Sigma_\\epsilon^{-1}\\Phi_t\\\\\n  &= \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_{t} Q_{t\\mid t-1}^{-1} \\Phi_{t}^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_t \\Phi_t^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_t\\\\\n  &= I_t Q_{t\\mid t-1}^{-1} I_t^\\intercal + I_t =: \\Sigma_{\\iota, t}.\n\\end{split}\n\\]\nNoting that \\(\\bv \\iota\\) clearly still has mean zero, this allows us once again to compute the log likelihood, this time through \\(\\bv\\iota\\)\n\\[\\begin{split}\n\\mathcal L(z_t\\mid\\bv\\theta) = -\\frac12\\sum \\log\\det(\\Sigma_{\\iota, t}(\\bv\\theta)) - \\frac12 \\sum\\bv \\iota_t(\\bv\\theta)^\\intercal\\Sigma_{\\iota, t}(\\bv\\theta)^{-1} \\bv \\iota_t(\\bv\\theta) - \\frac{r}{2}\\log(2*\\pi).\n\\end{split}\n\\]"
  },
  {
    "objectID": "site/mathematics.html#the-square-root-filters",
    "href": "site/mathematics.html#the-square-root-filters",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.3 The Square-Root filters",
    "text": "4.3 The Square-Root filters\nIn certain high-dimensional cases, the Kalman filter (and, indeed, the information filter) can encounter numerical stability issues. For example, in the predict step of the standard Kalman filter, note the update step for the variance matrix\n\\[\\begin{split}\nP_{t+1\\mid t+1} &= [I- K_{t+1}\\Phi_{t+1}]P_{t+1\\mid t}.\n\\end{split}\n\\]\nSomewhat masked within this equation is two (often very small) variance matrices subtracted from eachother. While analytically, the result is still guaranteed to be positive (semi-)definite, when done in floating point arithmetic (especially in single-precision or lower), the result can often be numerically indefinite. When the variances are very low (as they often become in these Kalman filters), the eigenvalues come out very close to zero and can tick over to becoming negative erroneously. This can lead to definiteness issues with all the other variance matrices, most crucially \\(\\Sigma_t\\) Equation 13. When this happens, computation of the likelihood likely fails (certainly when such a computation involves a Cholesky decomposition). Even if such is rare to happen with 64-bit precision, modern GPU hardware tends to be much more efficient with Single (32-bit) precision, so it may still be desirable to increase stability if it permits using a lower precision. The Square-root filter and the SVD filter are such algorithms.\n\n4.3.1 The Square-root Kalman filter\nThe square-root Kalman filter has it’s origins soon after the standard Kalman filter gained popularity (Kaminski, Bryson, and Schmidt 1971). Of course, computational and memory constraints necessitated stable and memory-efficient approaches, while today the standard Kalman filter (and, more recently, it’s parallel counterpart, to be covered in section [TBD]) usually suffice.\nAs its name suggests, this variant involves carriyng through the square roots of variances 6 instead of the variances themselves. This leads to, at least in some sense, an increased precision, and we can always guarentee that, at least analytically, the square of these square roots (the variances) are positive (semi-)definite.\nWhile the square root filter has been known for a long time (even used during NASA’s Apollo program), more recently, (Tracy 2022) wrote it neatly in terms of the QR decomposition, and this is what we base the presentation on here.\nThe key observation used for this filter is that if we have the sum of two equations where a square root is known for both, it can be written\n\\[\\begin{split}\n  X + Y &= A^\\intercal A + B^\\intercal B\\\\\n  &= \\left[A^\\intercal\\ B^\\intercal\\right] \\left[\\begin{matrix}A\\\\B\\end{matrix}\\right]\n\\end{split}\n\\]\nTaking the QR decomposition of the vertical block yields QR, and since \\((QR)^\\intercal\\ (QR) = R^\\intercal Q^\\intercal Q R = R^\\intercal R\\), so \\(R\\) is a square root of \\(X+Y\\). This motivates the following ‘QR operator’\n\\[\\begin{split}\n\\mathrm qr_R(A, B),\n\\end{split}\n\\]\nas the matrix \\(R\\) in the QR decomposition of the block matrix\n\\[\\begin{split}\n\\left[\\begin{matrix}A\\\\B\\end{matrix}\\right].\n\\end{split}\n\\]\nBeginning with the Cholesky decomposition of the initial variances, \\(P_0 = U_0^\\intercal U_0\\), \\(\\Sigma_{\\eta} = U_{\\eta}^\\intercal U_{\\eta}\\) and \\(\\Sigma_\\epsilon = U_{\\epsilon}^\\intercal U_{\\epsilon}\\) the predict step for the variance becomes\n\\[\\begin{split}\nU_{t+1\\mid t} = \\sqrt{P_{t+1\\mid t}} = \\mathrm{qr}_R(U_{t\\mid t} M^\\intercal, U_{\\eta}),\n\\end{split}\n\\]\nwith the step for the means being the same as before (Equation 11). The prediction errors, prediction variance and Kalman gain are now\n\\[\\begin{split}\n  \\bv e_{t+1} &= \\tilde{\\bv z}_t - \\Phi_{t+1} \\bv m_{t+1\\mid t},\\\\\n  \\Sigma_{t+1} &= \\Phi_{t+1} P_{t+1\\mid t} \\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon,\\\\\n  \\sqrt{\\Sigma_{t+1}} &= U_{e, t+1} = \\mathrm{qr}_R(\\Phi_{t+1} U_{t+1\\mid t}, U_\\epsilon),\\\\\n  K_{t+1} &= P_{t+1\\mid t} \\Phi_{t+1}^\\intercal \\Sigma_{t+1}^{-1} = U_{t+1\\mid t}^\\intercal U_{t+1\\mid t} \\Phi_{t+1}^\\intercal (U_{e, t+1}^\\intercal U_{e, t+1})^{-1}\\\\\n  &= (U_{e, t+1}^{-1}U_{e, t+1}^{-\\intercal}\\Phi_{t+1}U_{t+1\\mid t}^\\intercal U_{t+1\\mid t})^\\intercal\n\\end{split}\n\\]\nwhere the last equation for the Kalman gain can easily be solved with a computationally efficient triangular solve.\nFinally, the update step for the mean is simply\n\\[\\begin{split}\n  \\bv m_{t+1\\mid t+1} = \\bv m_{t \\mid t+1} + K_{t+1} {\\bv e_{t+1}}.\n\\end{split}\n\\]\nHowever, for the update we use the so-called Joseph stabilised form (sometimes used in the derivation of the Kalman filter)\n\\[\\begin{split}\n  P_{t+1\\mid t+1} &= \\mathbb C\\mathrm{ov}[\\bv \\alpha_t - \\bv m_{t+1\\mid t+1}]\\\\\n             &= \\mathbb C\\mathrm{ov}[\\bv \\alpha_t - \\bv m_{t \\mid t+1} - K_{t+1} (\\tilde{\\bv z}_{t+1} - \\Phi_{t+1} \\bv m_{t+1\\mid t})]\\\\\n             &= \\mathbb C\\mathrm{ov}[\\bv \\alpha_t - \\bv m_{t \\mid t+1} - K_{t+1} (\\Phi_{t+1} \\bv m_{t+1} + \\bv \\epsilon_{t+1} - \\Phi_{t+1}\\bv m_{t+1\\mid t})]\\\\\n             &= \\mathbb C\\mathrm{ov}[(I - K_{t+1} \\Phi_{t+1})(\\bv \\alpha_t - \\bv m_{t+1 \\mid t}) - \\bv \\epsilon_{t+1}]\\\\\n             &= (I - K_{t+1} \\Phi_{t+1}) \\mathbb C\\mathrm{ov}[\\bv \\alpha_t - \\bv m_{t+1 \\mid t}](I - K_{t+1} \\Phi_{t+1})^\\intercal + \\mathbb C\\mathrm{ov}[\\bv \\epsilon_{t+1}]\\\\\n             &= (I - K_{t+1} \\Phi_{t+1}) P_{t+1\\mid t}(I - K_{t+1} \\Phi_{t+1})^\\intercal + \\Sigma_{\\epsilon}\n\\end{split}\n\\]\nwhich is often simplified further to Equation 12, but as discussed that involves negation of two square root matrices; this form is more complicated and involves more matrix computation, but guarentees that the result will be positive (semi-)definite. Furthermore, this is also in a form that allows us to easily find the square root with the QR trick;\n\\[\\begin{split}\nU_{t+1\\mid t+1} = \\sqrt{P_{t+1\\mid t+1}} = \\mathrm{qr}_R(U_{t+1\\mid t}(I - K_{t+1} \\Phi_{t+1})^{\\intercal}, U_\\epsilon).\n\\end{split}\n\\]\nOf course, from here, we can similarily easily compute the data likelihood using \\(U_{e,t+1}\\), using standard techniques; the multivariate normal likelihood is usually computed using the cholesky decomposition of the variance matrix anyway. The result is an algorithm which is of a higher order than the standard Kalman filter, but the stability is often worth the comprimise. Once jit-compiled, the function sqrt_filter_indep on a moderately sized IDEM (on a discrete GPU) on 64-bit precision 7 takes approximately 23.5ms, compared to kalman_filter_indep taking approximately 7.8ms, achieving similar log-likelihoods (whith some difference due to precision). However, running the code in 32-bit causes the Kalman filter likelihood computation to fail, the square-root filter succeeds at a time of 7.0ms.\n\n\n4.3.2 Square-root Information filter\nVery similarily, we can write the information filter using the square roots of the information matrices. We will label roots of ‘information-type’ matrices with \\(R\\), and ‘variance-type’ matrices (their inverse) with \\(U\\).\nWe now carry the data’s information matrix’s (Equation 14) square root as well, \\(R_t^{(I)} = \\sqrt(\\Phi_t^\\intercal\\Sigma_\\epsilon^{-1} \\Phi_t)\\), with the same observation vector.\nSo, once again, beginning with the lower-triangular cholesky decomposition \\(Q_0 = R^\\intercal R\\), and the upper-triangular \\(\\Sigma_{\\eta} = U_{\\eta}^\\intercal U_{\\eta}\\) and \\(\\Sigma_\\epsilon = U_{\\epsilon}^\\intercal U_{\\epsilon}\\).\nSo, to predict step for the information matrix (Equation 15) becomes\n\\[\\begin{split}\n  Q_{t+1\\mid t} &= (M Q_{t\\mid t}^{-1} M^\\intercal + \\Sigma_\\eta)^{-1}\\\\\n  &= (M R_{t\\mid t}^{-1}R_{t\\mid t}^{-\\intercal} M^\\intercal + U_\\eta^\\intercal U_\\eta)^{-1}\\\\\n  &= \\left[(M R_{t\\mid t}^{-1}, U_\\eta^\\intercal)\\left(\\begin{matrix}R_{t\\mid t}^{-\\intercal} M^\\intercal\\\\U_\\eta\\end{matrix}\\right)\\right]^{-1}\\\\\n  R_{t+1\\mid t}^{-1} &= \\mathrm{qr}_R(R_{t\\mid t}^{-\\intercal} M^\\intercal, U_\\eta)\n\\end{split}\n\\tag{17}\\]\nThis must now be explicitly inverted, which isn’t a big problem since it is upper-triangular.\nThe update on the information vector is now\n\\[\\begin{split}\n\\bv\\nu_{t+1\\mid t} &= Q_{t+1\\mid t} M Q_{t\\mid t}^{-1} \\bv \\nu_{t\\mid t}\\\\\n&= R_{t+1\\mid t}^\\intercal R_{t+1\\mid t} M R_{t\\mid t}^{-1}R_{t\\mid t}^{-\\intercal} \\bv \\nu_{t\\mid t},\n\\end{split}\n\\tag{18}\\]\nwhich can be done, as in the square-root Kalman filter’s Kalman gain computation, using forward-solves.\nNow the update step is\n\\[\\begin{split}\n  \\bv\\nu_{t+1\\mid t+1} &= \\bv\\nu_{t+1\\mid t} + i_{t+1}\\\\\n  Q_{t+1\\mid t+1} &= Q_{t+1\\mid t} + I_{t+1}\\\\\n  &= R_{t+1\\mid t}^\\intercal R_{t+1\\mid t} + R^{(I)\\intercal}_{t+1}R^{(I)}_{t+1}\\\\\n  R_{t+1\\mid t+1}  &= \\mathrm{qr}_R(R_{t+1\\mid t}, R^{(I)}_{t+1}).\n\\end{split}\n\\tag{19}\\]"
  },
  {
    "objectID": "site/mathematics.html#smoothing",
    "href": "site/mathematics.html#smoothing",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "4.4 Smoothing",
    "text": "4.4 Smoothing\nBeyond the filtering, another task is smoothing. That is, filters estimate \\(\\bv m_{T\\mid T}\\) and \\(P_{T\\mid T}\\), but there is use for estimating \\(\\bv m_{t\\mid T}\\) and \\(P_{t\\mid T}\\) for all \\(t=0,\\dots, T\\).\nWe simply work backwards from \\(\\bv m_{T\\mid T}\\) and \\(P_{T\\mid T}\\) values using what is known as the Rauch-Tung-Striebel (RTS) smoother;\n\\[\\begin{split}\n\\bv m_{t-1\\mid T} &= \\bv m_{t-1\\mid t-1} + J_{t-1}(\\bv m_{t\\mid T} - \\bv m_{t\\mid t-1}),\\\\\nP_{t-1\\mid T} &= P_{t-1\\mid t-1} + J_{t-1}(P_{t\\mid T} - P_{t\\mid t-1})J_{t-1}^\\intercal,\n\\end{split}\n\\tag{20}\\]\nwhere,\n\\[\\begin{split}\nJ_{t-1} = P_{t-1\\mid t-1}M^\\intercal[P_{t\\mid t-1}]^{-1}.\n\\end{split}\n\\]\nWe can clearly see, then, that it is crucial to keep the values in Equation 11.\nWe can then also compute the lag-one cross-covariance matrices \\(P_{t,t-1\\mid T}\\) using the Lag-One Covariance Smoother. This will b useful, for example, in the expectation-maximisation algorithm later. From\n\\[\\begin{split}\nP_{T,T-1\\mid T} = (I - K_T\\Phi_{T}) MP_{T-1\\mid T-1},\n\\end{split}\n\\]\nwe can compute the lag-one covariances\n\\[\\begin{split}\nP_{t, t-1\\mid T} = P_{t\\mid t}J_{t-1}^\\intercal + J_{t}[P_{t+1,t\\mid T} - MP_{t-1\\mid t-1}]J_{t-1}^\\intercal\n\\end{split}\n\\tag{21}\\]\nThese values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later."
  },
  {
    "objectID": "site/mathematics.html#woodburys-identity",
    "href": "site/mathematics.html#woodburys-identity",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "7.1 Woodbury’s identity",
    "text": "7.1 Woodbury’s identity\nThe following two sections will make heavy use of the Woodbury identity.\n\nLemma 2 (Woodbury’s Identity) We have, for conformable matrices \\(A, U, C, V\\),\n\\[\\begin{split}\n(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.\n\\end{split}\n\\tag{24}\\]\nAdditionally, we have the variant\n\\[\\begin{split}\n(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.\n\\end{split}\n\\tag{25}\\]\n\n\nProof. We only prove (Equation 25), since various proofs of (Equation 24) are well known (see, for example, the Wikipedia page).\nSimply multipliying (Equation 24) by \\(CU\\), (similar to Khan 2005, although there is an error in their proof)\n\\[\\begin{split}\n(A+UCV)^{-1}UC &= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\\\\n&= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\\\\n&= A^{-1}U(C^{-1}+VA^{-1}U)\n\\end{split}\n\\]\nas needed."
  },
  {
    "objectID": "site/mathematics.html#sec-app1",
    "href": "site/mathematics.html#sec-app1",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "7.2 Proof of Theorem 2",
    "text": "7.2 Proof of Theorem 2\n\nProof. Firstly, for the prediction step, using \\(S_t = M^{-\\intercal}Q_{t\\mid t}M^{-1}\\) and \\(J_t = S_t(\\Sigma_\\eta^{-1} + S_t)^{-1}\\) and the identities Equation 24 and Equation 25,\n\\[\\begin{split}\n  Q_{t+1\\mid t} &= P_{t+1\\mid t}^{-1} = (MQ_{t\\mid t}^{-1}M^\\intercal + \\Sigma_\\eta)^{-1}\\\\\n  &= S_t - J_t S_t = (I-J_t)S_t,\n\\end{split}\n\\]\nwhere we used \\(A=MQ_{t\\mid t}^{-1}M^\\intercal\\), \\(C=\\Sigma_\\eta\\) and \\(U=C=I\\) in Equation 24. Thurthermore,\n\\[\\begin{split}\n  \\bv \\nu_{t+1\\mid t} &= Q_{t+1\\mid t} \\bv m_{t+1\\mid t}\\\\\n  &= Q_{t+1\\mid t} M Q_{t\\mid t}^{-1} \\bv \\nu_{t\\mid t} = Q_{t+1\\mid t} (M Q_{t\\mid t}^{-1}) \\bv \\nu_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal}Q_{t\\mid t}M^{-1} (M Q_{t\\mid t}^{-1}) \\bv \\nu_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal} \\bv \\nu_{t\\mid t}.\n\\end{split}\n\\]\nFor the update step,\n\\[\\begin{split}\n  Q_{t+1\\mid t+1} &= P_{t+1\\mid t+1}^{-1}\\\\\n  &= (Q_{t+1}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}\\Sigma_\\epsilon\\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})^{-1}\\\\\n  &= ((Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1})^{-1})^{-1} = Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1}\\\\\n  &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\nThen, writing \\(\\bv m_{t+1\\mid t+1}\\) in terms of \\(Q_{t+1\\mid t}\\) and \\(\\bv \\nu_{t+1\\mid t}\\)\n\\[\\begin{split}\n  \\bv m_{t+1\\mid t+1} &= Q_{t+1\\mid t}^{-1} \\bv \\nu_{t+1\\mid t} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1} [\\tilde{\\bv z}_{t+1} - \\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\bv \\nu_{t+1\\mit t}]\\\\\n  &= (Q_{t+1\\mid t}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})\\bv \\nu_{t+1\\mid t} \\\\\n  &\\quad + Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\tilde{\\bv z}_{t+1}\\\\\n  &= [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\bv \\nu_{t+1\\mid t}\\\\\n  &\\quad + [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\Phi_{t+1}\\Sigma_\\epsilon^{-1}\\tilde{\\bv z}_{t+1},\n\\end{split}\n\\]\nand now noting that \\(\\bv\\nu_{t+1\\mid t+1} = (Q_{t+1\\mid t} + I_{t+1}) \\bv m_{t+1\\mid t+1}\\), we complete the proof."
  },
  {
    "objectID": "site/mathematics.html#sec-vagueprior",
    "href": "site/mathematics.html#sec-vagueprior",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "7.3 Truly Vague Prior with the Kalman Filter",
    "text": "7.3 Truly Vague Prior with the Kalman Filter\nIt has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior \\(Q_{0}=0\\). While this is true, it is actually possible to do this in the Kalman filter by ‘skipping’ the first step (contrary to some sources, such as the Wikipedia page as of January 2025).\n\nTheorem 3 In the Kalman Filter (Section 4.1), if we allow \\(P_{0}^{-1} = 0\\), effectively setting infinite variance, and assuming the propagator matrix \\(M\\) is invertible, we have\n\\[\\begin{split}\n  \\bv m_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1} \\Phi_1 \\Sigma_\\epsilon^{-1} \\tilde{\\bv z}_1,\\\\\n  P_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}.\n\\end{split}\n\\tag{26}\\]\nTherefore, starting with these values then continuing the filter as normal, we can perform the Kalman filter with ‘infinite’ prior variance.\n[NOTE: The requirement that M be invertible should be droppable, see the proof below]\n\n\nProof. Unsurprisingly, the proof is effectively equivalent to proving the information filter and setting \\(Q_0 = P_0^{-1}=0\\).\nFor the first predict step (Equation 11),\n\\[\\begin{split}\n  \\bv m_{1\\mid0} &= M \\bv m_0,\\\\\n  P_{1\\mid0} &= M P_0 M^\\intercal + \\Sigma_\\eta.\n\\end{split}\n\\]\nBy (Equation 24),\n\\[\\begin{split}\n  P_{1\\mid0}^{-1} &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (P_0^{-1} + M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} = 0.\n\\end{split}\n\\]\nSo, moving to the update step (Equation 12),\n\\[\\begin{split}\n  \\bv m_{1\\mid1} = M \\bv m_0 + P_{1\\mid0}\\Phi_1 [\\Phi_1 P_{1\\mid0} \\Phi_1^\\intercal + \\Sigma_\\epsilon]^{-1}(\\tilde{\\bv{z}}_1 - \\Phi M \\bv m_0).\\\\\n\\end{split}\n\\]\nApplying (Equation 25) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1, V=\\Phi_1^\\intercal, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  \\bv m_{1\\mid1} &= M \\bv m_0 + (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}(\\tilde{\\bv{z}}_1 - \\Phi_1 M\\bv m_0)\\\\\n  &= M \\bv m_0 + (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\bv{z}}_1 - (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1M\\bv m_0\\\\\n  &= (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\bv{z}}_1.\n\\end{split}\n\\]\nFor the variance, we apply the (Equation 24) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1^\\intercal, V=\\Phi_1, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  P_{1\\mid1} &= (I - P_{1\\mid0}\\Phi_1^\\intercal[\\Sigma_\\epsilon + \\Phi_1^\\intercal P_{1\\mid0}\\Phi_1]^{-1}\\Phi_1)P_{1\\mid0}\\\\\n  &= (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\\\\n  &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1},\n\\end{split}\n\\]\nas needed. \n\nIt is worth noting that (Equation 26) seems to make a lot of sense; namely, we expect the estimate for \\(\\bv m_0\\) to look like a correlated least squares-type estimator like this."
  },
  {
    "objectID": "site/mathematics.html#footnotes",
    "href": "site/mathematics.html#footnotes",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHistorically, this has been abbreviated as IDE. However, with that abbreviation almost universally meaning ‘Integrated Development Environment’, here, we choose to include the ‘M’ in the abbreviation.↩︎\nat least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see Liu, Yeo, and Lu (2022)↩︎\nThe bisquare functions, here, \\(\\phi_i(\\bv s) = [1-\\frac{\\Vert \\bv s - \\bv c_i \\Vert}{w_i}]^2 \\cdot I(\\Vert \\bv s - \\bv c_i \\Vert &lt; w_i)\\) for \\(i\\) ‘centroids’ or ‘knots’, \\(\\bv c_i\\in \\mathcal D\\), each with ‘radius’ \\(w_i\\)↩︎\nthat is, the parameters of the Gaussian distribution in it’s exponential family form↩︎\nthat being the process dimension, previously labelled \\(r\\), the number of basis functions used in the expansion of the process↩︎\nA matrix \\(A\\) is said to be a ‘square root’ of a positive-definate matrix \\(X\\) if \\(A^\\intercal A = X\\). Note that these square roots are not unique, but can be ‘rotated’ by an arbitrary unitary matrix. The ‘canonical’ square root is the Cholesky factor, the unique upper (or occasionally lower) triangular square root. This can be found for arbitraty square roots by taking the QR decomposition (or RQ decomposition), which effectively computes the upper-triangular square root, \\(R\\), and the unitary transformation \\(Q^\\intercal\\) necessary to get there.↩︎\nwhich must be explicitely enabled in JAX↩︎"
  },
  {
    "objectID": "site/Sydney_hamilton_traces.html",
    "href": "site/Sydney_hamilton_traces.html",
    "title": "Sydney Radar Data",
    "section": "",
<<<<<<< HEAD
    "text": "import jax\n\nimport os\nos.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n\nimport jax.numpy as jnp\nimport jax.random as rand\nimport pandas as pd\n\nimport jaxidem.idem as idem\nimport jaxidem.utils as utils\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchainA.csv', delimiter=',')\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.16603821516036987\nPosterior Mean of (transformed) parameters: \n[ 1.8546455   3.3390546  -2.5480504   1.4188743  -5.231614   -1.7454457\n  0.48737225]\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchainB.csv', delimiter=',')\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.1642402559518814\nPosterior Mean of (transformed) parameters: \n[ 1.8953321   3.333831   -2.4407494   1.2960057  -5.1963077  -1.733458\n  0.42287624]"
=======
    "text": "import jax\n\nimport os\nos.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n\nimport jax.numpy as jnp\nimport jax.random as rand\nimport pandas as pd\n\nimport jaxidem.idem as idem\nimport jaxidem.utils as utils\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchainA.csv', delimiter=',')\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.16603821516036987\nPosterior Mean of (transformed) parameters: \n[ 1.8546456   3.3390546  -2.5480506   1.4188741  -5.231614   -1.7454457\n  0.48737225]\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchainB.csv', delimiter=',')\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.1642402559518814\nPosterior Mean of (transformed) parameters: \n[ 1.8953321   3.3338315  -2.4407494   1.2960057  -5.1963077  -1.7334583\n  0.42287624]"
>>>>>>> 9deb318 (Testing and re-writing simulation and st_data)
  },
  {
    "objectID": "site/Sydney_Radar.html#maximum-likelihood-estimation",
    "href": "site/Sydney_Radar.html#maximum-likelihood-estimation",
    "title": "Sydney Radar Data",
    "section": "2.1 Maximum Likelihood Estimation",
    "text": "2.1 Maximum Likelihood Estimation\nOnce we have this marginal likelihood, there are a few ways to progress. A good start is with a maximum likelihood method. Obviously, we can no just take this lgo marginal function and maximise it in any way we see fit, but jaxidem.Model has a built-in method for this, Model.fit_mle. Given data, this will use a method from ‘optax’ to create a new output model with the fitted parameters.\n\nimport optax\n\n\nfit_model_mle, mle_params = model.fit_mle(radar_data,\n                                          optimizer = optax.adam(1e-2),\n                                          max_its = 100,\n                                          method = 'sqinf')\n\nThe resulting parameters are then\n\nidem.print_params(mle_params)\n\nParameters:\n  sigma2_eps: 5.723726272583008\n  sigma2_eta: 28.266475677490234\n  Kernel Parameters:\n    Scale: [0.08538345247507095]\n    Shape: [3.7510576248168945]\n    Offset X: [-5.437947750091553]\n    Offset Y: [-1.7626336812973022]\n  beta: [0.42389795184135437]\n\n\nOf course, we can use any other method to maximmise this. We can update the model with new parameters using the method Model.update, and utils.flatten_and_unflatten (see documentation) allows working with flat arrays instead of PyTrees if needed."
  },
  {
    "objectID": "site/filtering_and_smoothing.html#pseudo-information-filter",
    "href": "site/filtering_and_smoothing.html#pseudo-information-filter",
    "title": "Filtering in JAX-IDEM",
    "section": "2.1 Pseudo-information filter?",
<<<<<<< HEAD
    "text": "2.1 Pseudo-information filter?\nThis is a test.\n\n\nCode\ni = PHI.T @ zs.T / sigma2_eps\nI = PHI.T @ PHI / sigma2_eps\n\nfilt_results_alt = filt.kalman_filter(m_0,\n                                  P_0,\n                                  M,\n                                  I,\n                                  sigma2_eta,\n                                  I,\n                                  i,\n                                  sigma2_eps_dim = 2,\n                                  sigma2_eta_dim = sigma2_eta_dim,\n                                  forecast=0,\n                                  likelihood='full'\n    )\nms_df_alt = pd.DataFrame(list(filt_results_alt['ms']), columns = [\"x\", \"y\"])\n\ncombined_df = pd.concat([alphas_df.assign(line='True Process'), ms_df_alt.assign(line='Filtered Process Means')])\n\n# Creating the line plot with custom colors\nfig_alt = px.line(combined_df, x='x', y='y', color='line',\n               color_discrete_sequence=['blue', 'red'], height=200)  # Specify colors here\n\nfig_alt.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n# Show the plot\nfig_alt.show()\nprint(filt_results_alt[\"ll\"])\n\n\n\n\n                            \n                                            \n\n\nFigure 4\n\n\n\n\n-386.7478\n\n\nLet’s check if they really are the same;\n\n\nCode\nprint(jnp.allclose(filt_results['ms'], filt_results_alt['ms'], atol=1e-04))\nprint(jnp.allclose(filt_results['Ps'], filt_results_alt['Ps'], atol=1e-04))\nprint(filt_results['ms'][0:5])\nprint(filt_results_alt['ms'][0:5])\n\n\nTrue\nTrue\n[[ 0.6675685   1.1575371 ]\n [ 0.22375351  1.3272061 ]\n [-0.21793792  1.3617673 ]\n [-0.49496293  1.2157638 ]\n [-0.81052357  1.0334464 ]]\n[[ 0.6675626   1.1575242 ]\n [ 0.22379427  1.3271072 ]\n [-0.2179012   1.3617306 ]\n [-0.49493164  1.2157242 ]\n [-0.81049937  1.0334232 ]]"
=======
    "text": "2.1 Pseudo-information filter?\nThis is a test.\n\n\n\n\nCode\ni = PHI.T @ zs / sigma2_eps\nI = PHI.T @ PHI / sigma2_eps\n\nfilt_results_alt = filt.kalman_filter(m_0,\n                                  P_0,\n                                  M,\n                                  I,\n                                  sigma2_eta,\n                                  I,\n                                  i,\n                                  sigma2_eps_dim = 2,\n                                  sigma2_eta_dim = sigma2_eta_dim,\n                                  forecast=0,\n                                  likelihood='full'\n    )\nms_df_alt = pd.DataFrame(list(filt_results_alt['ms']), columns = [\"x\", \"y\"])\n\ncombined_df = pd.concat([alphas_df.assign(line='True Process'), ms_df_alt.assign(line='Filtered Process Means')])\n\n# Creating the line plot with custom colors\nfig_alt = px.line(combined_df, x='x', y='y', color='line',\n               color_discrete_sequence=['blue', 'red'], height=200)  # Specify colors here\n\nfig_alt.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n# Show the plot\nfig_alt.show()\nprint(filt_results_alt[\"ll\"])\n\n\n\nFigure 4\n\n\n\nLet’s check if they really are the same;\n\n\nCode\n#print(jnp.allclose(filt_results['ms'], filt_results_alt['ms'], atol=1e-04))\n#print(jnp.allclose(filt_results['Ps'], filt_results_alt['Ps'], atol=1e-04))\nprint(filt_results['ms'][0:5])\n#print(filt_results_alt['ms'][0:5])\n\n\n[[ 0.6675799   1.157503  ]\n [ 0.22372052  1.3271735 ]\n [-0.21794711  1.3617302 ]\n [-0.4949575   1.2157357 ]\n [-0.8105154   1.0334284 ]]"
>>>>>>> 9deb318 (Testing and re-writing simulation and st_data)
  },
  {
    "objectID": "site/Sydney_hamilton_traces.html#new-stuff",
    "href": "site/Sydney_hamilton_traces.html#new-stuff",
    "title": "Sydney Radar Data",
    "section": "1 New Stuff",
<<<<<<< HEAD
    "text": "1 New Stuff\n\n1.1 AM chain\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/AMchainA_prior.csv', delimiter=',')\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.09133665263652802\nPosterior Mean of (transformed) parameters: \n[ 1.8659704   3.3365324  -2.447654    1.3034371  -5.207955   -1.7281468\n  0.38862774]"
=======
    "text": "1 New Stuff\n\n1.1 AM chain\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/AMchainA_prior.csv', delimiter=',')\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.09133665263652802\nPosterior Mean of (transformed) parameters: \n[ 1.8659703   3.3365324  -2.447654    1.3034372  -5.207955   -1.7281467\n  0.38862774]"
>>>>>>> 9deb318 (Testing and re-writing simulation and st_data)
  },
  {
    "objectID": "site/Sydney_hamilton_traces.html#hmc-chain-prior-with-variance-from-am",
    "href": "site/Sydney_hamilton_traces.html#hmc-chain-prior-with-variance-from-am",
    "title": "Sydney Radar Data",
    "section": "2 HMC chain prior (with variance from AM)",
<<<<<<< HEAD
    "text": "2 HMC chain prior (with variance from AM)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchain_prior1.csv', delimiter=',')\n\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.15118016302585602\nPosterior Mean of (transformed) parameters: \n[ 1.8730884   3.329427   -2.4443157   1.297696   -5.2637725  -1.752132\n  0.37615058]"
=======
    "text": "2 HMC chain prior (with variance from AM)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchain_prior1.csv', delimiter=',')\n\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.15118016302585602\nPosterior Mean of (transformed) parameters: \n[ 1.8730884   3.329427   -2.4443157   1.297696   -5.2637725  -1.7521318\n  0.37615058]"
  },
  {
    "objectID": "site/Sydney_hamilton_traces.html#hamilton-64-bit-comparison",
    "href": "site/Sydney_hamilton_traces.html#hamilton-64-bit-comparison",
    "title": "Sydney Radar Data",
    "section": "3 Hamilton 64-bit comparison",
    "text": "3 Hamilton 64-bit comparison\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchain_prior2_64.csv', delimiter=',')\n\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots (HMC, 64-bit)', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.9999799728393555\nPosterior Mean of (transformed) parameters: \n[ 1.8559067  3.3358524 -2.4354758  1.2856554 -5.236265  -1.7246909\n  0.7030874]\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the CSV file using NumPy\ncsv_data = np.loadtxt('data/Chains/HMCchain_prior3.csv', delimiter=',')\n\n# Convert the NumPy array to a JAX array\nrmh_sample = jnp.array(csv_data)[:,1:]\n\nprint(f\"Acceptance Ratio: {jnp.mean(csv_data[:,0])}\")\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):, :]), axis=0)\nprint(f\"Posterior Mean of (transformed) parameters: \\n{post_mean}\")\n\n\nsamples = rmh_sample\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots (HMC, 32-bit)', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n\nAcceptance Ratio: 0.20917999744415283\nPosterior Mean of (transformed) parameters: \n[ 1.890468   3.345159  -2.4531734  1.3154677 -5.209804  -1.7464292\n  0.4981435]"
>>>>>>> 9deb318 (Testing and re-writing simulation and st_data)
  }
]