[
  {
    "objectID": "quarto/filtering_and_smoothing.html",
    "href": "quarto/filtering_and_smoothing.html",
    "title": "Filtering in JAX-IDEM",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "quarto/filtering_and_smoothing.html#the-simple-model",
    "href": "quarto/filtering_and_smoothing.html#the-simple-model",
    "title": "Filtering in JAX-IDEM",
    "section": "1.1 The simple model",
    "text": "1.1 The simple model\nConsider the simple system, for \\(t=1,\\dots,T\\)\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{\\alpha}}_{t+1} &= M\\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\eta}}_t,\\\\\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\alpha_0 = (1,1)^\\intercal\\) and\n\\[\\begin{split}\nM = \\left[\\begin{matrix}\n    \\cos(0.3) & -\\sin(0.3)\\\\\n    \\sin(0.3) & \\sin(0.3)\n\\end{matrix}\\right].\n\\end{split}\n\\tag{2}\\]\nThe error terms are mutually independant and have variances \\(\\sigma^{2}_\\epsilon=0.02\\) and \\(\\sigma^{2}_{\\eta}=0.03\\) and \\(\\boldsymbol{\\mathbf{z}}_t\\) are transformed linear ‘observations’ of \\(\\boldsymbol{\\mathbf{\\alpha}}\\)\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{z}}_t &= \\Phi \\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\epsilon}}_t,\\\\\n\\Phi &= \\left[\\begin{matrix}\n1   & 0  \\\\\n0.6 & 0.4\\\\\n0.4 & 0.6\n\\end{matrix}\\right]\n\\end{split}.\n\\tag{3}\\]\nThe process, \\(\\alpha\\), simply spins in a circle with some noise. Lets simulate from this system;\n\n\nCode\nimport sys\nimport os\nsys.path.append(os.path.abspath('../src/jax_idem'))\n\nimport jax.random as rand\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nimport filter_smoother_functions as fsf\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objs as go\nimport pandas as pd\n\n\n\n\nCode\nkey = jax.random.PRNGKey(1)\n\n\nalpha_0 = jnp.ones(2)  # 2D, easily plottable\nM = jnp.array([[jnp.cos(0.3), -jnp.sin(0.3)],\n              [jnp.sin(0.3), jnp.cos(0.3)]])  # spinny\n\nalphas = [alpha_0]\nzs = []\n\nT = 50\nkeys = rand.split(key, T*2)\n\nsigma2_eta = 0.03\nSigma_eta = sigma2_eta*jnp.eye(2)\nchol_s_eta = jax.scipy.linalg.cholesky(Sigma_eta, lower=True)\nsigma2_eps = 0.02\nSigma_eps = sigma2_eps*jnp.eye(3)\nchol_s_eps = jax.scipy.linalg.cholesky(Sigma_eps, lower=True)\nPHI = jnp.array([[1, 0], [0.6, 0.4], [0.4, 0.6]])\n\nfor i in range(T):\n    alphas.append(M@alphas[i] + chol_s_eta @\n                  rand.normal(keys[2*i], shape=(2,)))\n    zs.append(PHI @ alphas[i+1] + chol_s_eps @\n              rand.normal(keys[2*i+1], shape=(3,)))\n\nalphas_df = pd.DataFrame(alphas, columns = [\"x\", \"y\"])\nzs_df = pd.DataFrame(zs, columns = [\"x\", \"y\", \"z\"])\n\n\nalphas = jnp.array(alphas)\nzs = jnp.array(zs)\n\n    \nfig1 = px.line(alphas_df, x='x', y='y', height=200)\nfig1.update_layout(xaxis=dict(scaleanchor=\"y\", scaleratio=1, title='X Axis'), yaxis=dict(scaleanchor=\"x\", scaleratio=1, title='Y Axis'))\n\n\n\n\n                                                \n\n\nFigure 1\n\n\n\n\n\n\nCode\nfig2 = go.Figure(data=[go.Scatter3d(\n    x=zs_df['x'],\n    y=zs_df['y'],\n    z=zs_df['z'],\n    mode='markers',\n    marker=dict(\n        symbol='cross',  # Change marker to cross\n        size=5           # Adjust marker size\n    )\n)])\n\nfig2.update_layout(height=200)\n    \nfig2.show()\n\n\n\n\n                                                \n\n\nFigure 2\n\n\n\n\nWe can see how the process is an odd random spiral, and the observations are skewed noisy observations of this in 3D space\nWith filtering, we aim to recover the process {fig-truth} from the observations {fig-obs}. We do this with two ‘forms’ of the filter, which should be equivalent."
  },
  {
    "objectID": "quarto/index.html",
    "href": "quarto/index.html",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "",
    "text": "Code\nimport sys\nimport os\nsys.path.append(os.path.abspath('../src/jax_idem'))\n\nimport jax\nimport importlib\nimport utilities\nimport IDEM\n\nimportlib.reload(utilities)\nimportlib.reload(IDEM)\n\nfrom utilities import *\nfrom IDEM import *\nimport warnings\n\n\nseed = 2\nkey = jax.random.PRNGKey(seed)\nkeys = rand.split(key, 2)\n\nmodel = gen_example_idem(keys[0], k_spat_inv=False, ngrid=jnp.array([50, 50]))\n\n# Simulation\nT = 100\nprocess_data, obs_data = model.simulate(key, T=T, nobs=100)\n\n# plot the objects\ngif_st_grid(process_data, \"process.gif\")\ngif_st_pts(obs_data, \"obs.gif\")\nplot_kernel(model.kernel, \"kernel.png\")\n \n\nfrom PIL import Image, ImageSequence\n\ngif1 = Image.open('process.gif')\ngif2 = Image.open('tardis.gif')\nframes = []\nnum_frames_gif1 = len(list(ImageSequence.Iterator(gif1)))\nnum_frames_gif2 = len(list(ImageSequence.Iterator(gif2)))\nmax_frames = max(num_frames_gif1, num_frames_gif2)\n\nfor i in range(max_frames):\n    frame1 = ImageSequence.Iterator(gif1)[i % num_frames_gif1].convert(\"RGBA\")\n    frame2 = ImageSequence.Iterator(gif2)[i % num_frames_gif2].convert(\"RGBA\")\n    combined = Image.alpha_composite(frame1, frame2)\n    frames.append(combined)\n\n\nframes[0].save('process.gif', save_all=True, append_images=frames[1:], duration=gif1.info['duration'], loop=0)"
  },
  {
    "objectID": "quarto/index.html#the-mathematics",
    "href": "quarto/index.html#the-mathematics",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "1 The Mathematics",
    "text": "1 The Mathematics\nFor a rundown of the mathematics underpinning this model and implementation, see here."
  },
  {
    "objectID": "quarto/index.html#documentation",
    "href": "quarto/index.html#documentation",
    "title": "Integro-Difference Equation Models in JAX",
    "section": "2 Documentation",
    "text": "2 Documentation\nDocumentation for the package is available here"
  },
  {
    "objectID": "quarto/mathematics.html",
    "href": "quarto/mathematics.html",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "",
    "text": "Index"
  },
  {
    "objectID": "quarto/mathematics.html#process-noise",
    "href": "quarto/mathematics.html#process-noise",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.1 Process Noise",
    "text": "2.1 Process Noise\nWe still have to set out what the process noise, \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\), and it’s spectral couterpart, \\(\\boldsymbol{\\mathbf{\\eta}}_t\\), are. Dewar (Dewar, Scerri, and Kadirkamanathan 2008) fixes the variance of \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\) to be uniform and uncorrelated across space and time, with \\(\\omega_t(\\boldsymbol{\\mathbf{s}}) \\sim \\mathcal N(0,\\sigma^2)\\) It is then easily shown that \\(\\boldsymbol{\\mathbf{\\eta}}_t\\) is also normal, with \\(\\boldsymbol{\\mathbf{\\eta}}_t \\sim \\mathcal N(0, \\sigma^2\\Psi^{-1})\\).\nHowever, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of \\(\\boldsymbol{\\mathbf{\\eta}}_t\\), and compute the variance of \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\) if needed. This is exactly what the IDE package (Zammit-Mangion 2022) in R does, and, correspondingly, what this JAX project does.\n\nLemma 1 Let \\(\\boldsymbol{\\mathbf{\\eta}}_t \\sim \\mathcal N(0, \\Sigma_\\eta)\\), and \\(\\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\boldsymbol{\\mathbf{\\eta}}_t, \\boldsymbol{\\mathbf{\\eta}}_{t+\\tau}] =0\\), \\(\\forall \\tau&gt;0\\). Then \\(\\omega_t(\\boldsymbol{\\mathbf{s}})\\) has covariance\n\\[\\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\omega_t(\\boldsymbol{\\mathbf{s}}), \\omega_{t+\\tau}(\\boldsymbol{\\mathbf{r}})] = \\begin{cases}\n\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal \\Sigma_\\eta \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}) & \\text{if }\\tau=0\\\\\n0 & \\text{else}\\\\\n\\end{cases}\n\\]\n\n\nProof. Consider \\(\\Psi \\boldsymbol{\\mathbf{\\eta}}_t\\). It is clearly normal, with expectation zero and variance (using (Equation 5)),\n\\[\\begin{split}\n\\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\Psi \\boldsymbol{\\mathbf{\\eta}}_t] &= \\Psi \\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\boldsymbol{\\mathbf{\\omega}}_t] \\Psi^\\intercal = \\Psi\\Sigma_\\eta\\Psi^\\intercal,\\\\\n&= \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal d\\boldsymbol{\\mathbf{s}} \\  \\Sigma_\\eta \\ \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{r}}\\\\\n&=  \\int\\int_{\\mathcal D_s^2} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal \\  \\Sigma_\\eta \\  \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{r}} d\\boldsymbol{\\mathbf{s}}\\\\\n\\end{split}\n\\tag{6}\\]\nSince it has zero expectation, we also have\n\\[\\begin{split}\n\\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\Psi\\boldsymbol{\\mathbf{\\eta}}_t] &= \\mathbb E[(\\Psi\\boldsymbol{\\mathbf{\\eta}}_t) (\\Psi\\boldsymbol{\\mathbf{\\eta}}_t)^\\intercal] = \\mathbb E[\\Psi\\boldsymbol{\\mathbf{\\eta}}_t\\boldsymbol{\\mathbf{\\eta}}_t^\\intercal\\Psi^\\intercal]\\\\\n&= \\mathbb E \\left[ \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})\\omega_t(\\boldsymbol{\\mathbf{s}})d\\boldsymbol{\\mathbf{s}} \\int_{\\mathcal D_s} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal \\omega_t(\\boldsymbol{\\mathbf{r}}) d\\boldsymbol{\\mathbf{r}} \\right]\\\\\n&= \\int\\int_{\\mathcal D_s^2} \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})\\  \\mathbb E[\\omega_t(\\boldsymbol{\\mathbf{s}})\\omega_t(\\boldsymbol{\\mathbf{r}})]\\  \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{s}} d \\boldsymbol{\\mathbf{r}}.\n\\end{split}\n\\tag{7}\\]\nWe can see that, comparing (Equation 6) and (Equation 7), we have\n\\[\\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\omega_t(\\boldsymbol{\\mathbf{s}}), \\omega_t(\\boldsymbol{\\mathbf{r}})] = \\mathbb E[\\omega_t(\\boldsymbol{\\mathbf{s}})\\omega_t(\\boldsymbol{\\mathbf{r}})]= \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal \\Sigma_\\eta \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}}).\n\\]"
  },
  {
    "objectID": "quarto/mathematics.html#kernel-decomposition",
    "href": "quarto/mathematics.html#kernel-decomposition",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.2 Kernel Decomposition",
    "text": "2.2 Kernel Decomposition\nNext is the key part od the system, which defines the dynamics; the kernelf function, \\(\\kappa\\). There are a few ways to handle the kernel. One of the most obvious is to expand it out into a spectral decomposition as well;\n\\[\\kappa \\approx \\sum_i \\beta_i\\psi(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}).\n\\]\nThis can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on \\(\\mathbb R^2\\times \\mathbb R^2\\); to get a wide enough space of possible functions, we would likely need many terms of the basis expansion.\nA much simpler approach would be to simply parameterise the kernel function, to \\(\\kappa(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}, \\boldsymbol{\\mathbf{\\theta}}_\\kappa)\\). We then establish a simple shape for the kernel (e.g. Gaussian) and rely on very few parameters (for example, scale, shape, offsets). The example kernel used in the program is aGaussian kernel;\n\\[\\kappa(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}; \\boldsymbol{\\mathbf{m}}, a, b) = a \\exp \\left( -\\frac{1}{b} \\Vert \\boldsymbol{\\mathbf{s}}- \\boldsymbol{\\mathbf{r}} +\\boldsymbol{\\mathbf{m}}\\Vert^2 \\right)\n\\]\nOf course, this kernel lacks spatial dependance. We can add spatial variance back in in a nice way by adding dependance on \\(\\boldsymbol{\\mathbf{s}}\\) to the parameters, for example, variyng the offset term as \\(\\boldsymbol{\\mathbf{m}}(\\boldsymbol{\\mathbf{s}})\\). Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground (Cressie and Wikle 2015). The actual parameters of such a spatially-variant kernel are then the basis coefficients for the expansion of any spatially variant parameters, as well as any constant parameters."
  },
  {
    "objectID": "quarto/mathematics.html#idem-as-a-linear-dynamical-system",
    "href": "quarto/mathematics.html#idem-as-a-linear-dynamical-system",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "2.3 IDEM as a linear dynamical system",
    "text": "2.3 IDEM as a linear dynamical system\nTo recap, we have discretized space in such a way that the Integro-difference model is of a more traditional linear dynamical system form. All that is left is to include our observations in our system.\nLets assume that at each time \\(t\\) there are \\(n_t\\) observations at locations \\(\\boldsymbol{\\mathbf{s}}_{1,t},\\dots, \\boldsymbol{\\mathbf{s}}_{n_{t},t}\\). We write the vector of the process at these points as \\(\\boldsymbol{\\mathbf{Y}}(t) = (Y(s_{1,t};t), \\dots, Y(s_{n_{t},t};t))^\\intercal\\), and, in it’s expanded form \\(\\boldsymbol{\\mathbf{Y}}_t = \\Phi_t \\boldsymbol{\\mathbf{\\alpha}}_t\\), where \\(\\Phi \\in \\mathbb R^{r\\times n_{t}}\\) is\n\\[\\begin{split}\n\\{\\Phi_{t}\\}_{i, j} = \\phi_{i}(s_{j,t}).\n\\end{split}\n\\]\nFor the covariates, we write the matrix \\(X_t = (\\boldsymbol{\\mathbf{X}}(\\boldsymbol{\\mathbf{s}}_{1, t}), \\dots, \\boldsymbol{\\mathbf{X}}(\\boldsymbol{\\mathbf{s}}_{1=n_{t}, t})^\\intercal\\). We then have\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{Z}}_t &= \\Phi \\alpha_t + X_{t} \\boldsymbol{\\mathbf{\\beta }}+ \\boldsymbol{\\mathbf{\\epsilon}}_t, \\quad t=0,1,\\dots, T,\\\\\n\\boldsymbol{\\mathbf{\\alpha}}_{t+1} &= M\\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\eta}}_t,\\quad t = 1,2,\\dots, T,\\\\\nM &= \\int_{\\mathcal D_s}\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}})^\\intercal d\\boldsymbol{\\mathbf{s}} \\int_{\\mathcal D_s^2}\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{s}}) \\kappa(\\boldsymbol{\\mathbf{s}}, \\boldsymbol{\\mathbf{r}}; \\boldsymbol{\\mathbf{\\theta}}_\\kappa)\\boldsymbol{\\mathbf{\\phi}}(\\boldsymbol{\\mathbf{r}})^\\intercal d\\boldsymbol{\\mathbf{r}} d \\boldsymbol{\\mathbf{s}},\n\\end{split}\n\\]\nWriting \\(\\tilde{\\boldsymbol{\\mathbf{Z}}}_t = \\boldsymbol{\\mathbf{Z}}_t - X_t \\boldsymbol{\\mathbf{\\beta}}\\),\n\\[\\begin{split}\n\\tilde{\\boldsymbol{\\mathbf{Z}}}_t &= \\Phi_{t} \\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\epsilon}}_t,\\quad &t = 1,2,\\dots, T,\\\\\n\\boldsymbol{\\mathbf{\\alpha}}_{t+1} &= M \\boldsymbol{\\mathbf{\\alpha}}_t + \\boldsymbol{\\mathbf{\\eta}}_t,\\quad &t = 0,1, \\dots, T.\\\\\n\\end{split}\n\\tag{8}\\]\nWe should also initialise \\(\\boldsymbol{\\mathbf{\\alpha}}_0 \\sim \\mathcal N^{r}(\\boldsymbol{\\mathbf{m}}_{0}, \\Sigma_{0})\\), and fix simple distrubtions to the noise terms,\n\\[\\begin{split}\n\\epsilon_{t,i} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\sigma^2_\\epsilon),\\\\\n\\eta_{t,i} \\overset{\\mathrm{iid}}{\\sim} \\mathcal N(0,\\sigma^2_\\eta),\n\\end{split}\n\\]\nwhich are (also) independant in time.\nAs in, for example, (Wikle and Cressie 1999), this is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood. We can use these quantities in either an EM algorithm or a Bayesian approach. Firstly, we cover the EM algorithm applied to this system.\nAt most, the parameters to be estimated are\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{\\theta }}= \\left(\\boldsymbol{\\mathbf{\\theta}}_\\kappa^\\intercal, \\boldsymbol{\\mathbf{\\beta}}^\\intercal, \\boldsymbol{\\mathbf{m}}_0^\\intercal, \\sigma^{2}_{\\epsilon}, \\sigma^{2}_{\\eta}, \\mathrm{vec}[\\Sigma_0]\\right),\n\\end{split}\n\\]\nwhere the \\(\\mathrm{vec}[\\cdot]\\) operator gives the elements of the matrix in a column vector. Of course, in practice, some of these may be estimated outside of the algorithm, fixed, given a much simpler form (e.g. \\(\\Sigma_\\eta = \\sigma_\\eta^2 I_d\\)), etc.\nThere are two approaches we can make from here; directly maximising the marginal data likelihood using only the Kalman filter, or maximising the full likelihood with the EM algorithm.\nNow (Equation 8) is of the very familar linear dynamical system (LDS) type. This is a well-understood problem, and optimal state estimation can be done using the kalman filter and (RTS) smoother."
  },
  {
    "objectID": "quarto/mathematics.html#sec-kalmanfilter",
    "href": "quarto/mathematics.html#sec-kalmanfilter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.1 The Kalman Filter",
    "text": "3.1 The Kalman Filter\nFirstly, we should establish some notation. Write\n\\[\\begin{split}\nm_{r \\mid s} &= \\mathbb E[\\boldsymbol{\\mathbf{\\alpha}}_r \\mid \\{Z_t\\}_{t=0,\\dots,s}]\\\\\nP_{r \\mid s} &= \\mathop{\\mathrm{\\mathbb{V}\\mathrm{ar}}}[\\boldsymbol{\\mathbf{\\alpha}}_r \\mid \\{Z_t\\}_{t=0,\\dots,s}]\\\\\nP_{r,q \\mid s} &= \\mathop{\\mathrm{\\mathbb{C}\\mathrm{ov}}}[\\boldsymbol{\\mathbf{\\alpha}}_r, \\boldsymbol{\\mathbf{\\alpha}}_q \\mid \\{Z_t\\}_{t=0,\\dots,s}].\n\\end{split}\n\\]\nFor the initial terms, \\(m_{0\\mid0}=m_0\\) and \\(P_{0\\mid0}=\\Sigma_0\\). For convenience and generality, we write \\(\\Sigma_\\eta\\) and \\(\\Sigma_\\epsilon\\) for the variance matrices of the process and observations. Note that, if the number of observations change at each time point (for example, due to missing data), then \\(\\Sigma_\\epsilon\\) should be time variyng; we could either always keep it as uncorrelated so that \\(\\Sigma_\\epsilon = \\mathrm{diag} (\\sigma_\\epsilon^2)\\), or perhaps put some kind of distance-dependant covariance function to it.\nTo move the filter forward, that is, given \\(m_{r\\mid s}\\) and \\(P_{r\\mid s}\\), to get \\(m_{t+1\\mid t+1}\\) and \\(P_{t+1\\mid t+1}\\), we first predict\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{m}}_{t+1\\mid t} &= M \\boldsymbol{\\mathbf{m}}_{t\\mid t}\\\\\nP_{t+1\\mid t} &= M P_{t\\mid t} M^\\intercal + \\Sigma_\\eta,\n\\end{split}\n\\tag{9}\\]\nthen we add our new information, \\(z_{t}\\), adjusted for the Kalman gain;\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1} &= \\boldsymbol{\\mathbf{m}}_{t+1\\mid t} + K_{t+1} \\boldsymbol{\\mathbf{e}}_{t+1}\\\\\nP_{t+1\\mid t+1} &= [I- K_{t+1}\\Phi_{t+1}]P_{t+1\\mid t}\n\\end{split}\n\\tag{10}\\]\nwhere \\(K_{t+1}\\) is the Kalman gain;\n\\[\\begin{split}\nK_{t+1} = P_{t+1\\mid t}\\Phi_{t+1}^\\intercal [\\Phi_{t+1} P_{t+1\\mid t} \\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}, \\quad t=0,\\dots,T-1\n\\end{split}\n\\]\nand \\(\\boldsymbol{\\mathbf{e}}_{t+1}\\) are the prediction errors\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{e}}_{t+1} = \\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1}-\\Phi_{t+1} \\boldsymbol{\\mathbf{m}}_{t+1\\mid t}, \\quad t=1,\\dots,T\n\\end{split}\n\\]\nStarting with \\(m_{0\\mid0} = m_0\\) and \\(P_{0\\mid0} =\\Sigma_0\\), we can then iteratively move across the data to eventually compute \\(m_{T\\mid T}\\) and \\(P_{T\\mid T}\\).\nAssuming Gaussian all random variables here are Guassian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of linear operators.\nWe can compute the marginal data likelihood alongside the kalman fiter using the prediciton errors \\(\\boldsymbol{\\mathbf{e}}_t\\). These, under the assumptions we have made about \\(\\eta\\) and \\(\\epsilon\\) being normal, are also normal with zero mean and variance\n\\[\\begin{split}\n\\mathbb V\\mathrm{ar}[\\boldsymbol{\\mathbf{e}}_t]=\\Sigma_t= \\Phi_{t} P_{t+1\\mid t} \\Phi_{t}^\\intercal + \\Sigma_\\epsilon.\n\\end{split}\n\\]\nTherefore, the log-likelihood at each time is\n\\[\\begin{split}\n\\mathcal L(Z\\mid\\boldsymbol{\\mathbf{\\theta}}) = -\\frac12\\sum \\log\\det(\\Sigma_t(\\boldsymbol{\\mathbf{\\theta}})) - \\frac12 \\sum\\boldsymbol{\\mathbf{e}}_t(\\boldsymbol{\\mathbf{\\theta}})^\\intercal\\Sigma_{t}(\\boldsymbol{\\mathbf{\\theta}})^{-1} \\boldsymbol{\\mathbf{e}}_t(\\boldsymbol{\\mathbf{\\theta}}) - \\frac{n_{t}}{2}\\log(2*\\pi).\n\\end{split}\n\\]\nSumming these across time, we get the log likelihood for all the data.\nA simplified example of the kalman filter function, written to be jax compatible, used in the package is this;\n\n\nCode\n# TODO: Replace this with a simpler 'naive' implementation\n@jax.jit\ndef kalman_filter(\n    m_0: ArrayLike,\n    P_0: ArrayLike,\n    M: ArrayLike,\n    PHI_obs: ArrayLike,\n    Sigma_eta: ArrayLike,\n    Sigma_eps: ArrayLike,\n    ztildes: ArrayLike,  # data matrix, with time across columns\n) -&gt; tuple:\n    nbasis = m_0.shape[0]\n    nobs = ztildes.shape[0]\n\n    @jax.jit\n    def step(carry, z_t):\n        m_tt, P_tt, _, _, ll, _ = carry\n\n        # predict\n        m_pred = M @ m_tt\n        P_pred = M @ P_tt @ M.T + Sigma_eta\n\n        # Update\n\n        # Prediction Errors\n        eps_t = z_t - PHI_obs @ m_pred\n\n        Sigma_t = PHI_obs @ P_pred @ PHI_obs.T + Sigma_eps\n\n        # Kalman Gain\n        K_t = (\n            jnp.linalg.solve(Sigma_t, PHI_obs)\n            @ P_pred.T\n        ).T\n\n        m_up = m_pred + K_t @ eps_t\n\n        P_up = (jnp.eye(nbasis) - K_t @ PHI_obs) @ P_pred\n\n        # likelihood of epsilon, using cholesky decomposition\n        chol_Sigma_t = jnp.linalg.cholesky(Sigma_t)\n        z = jax.scipy.linalg.solve_triangular(chol_Sigma_t, eps_t)\n        ll_new = ll - jnp.sum(jnp.log(jnp.diag(chol_Sigma_t))\n                              ) - 0.5 * jnp.dot(z, z)\n\n        return (m_up, P_up, m_pred, P_pred, ll_new, K_t), (\n            m_up,\n            P_up,\n            m_pred,\n            P_pred,\n            ll_new,\n            K_t,\n        )\n\n    carry, seq = jl.scan(\n        step,\n        (m_0, P_0, m_0, P_0, 0, jnp.zeros((nbasis, nobs))),\n        ztildes.T,\n    )\n\n    return (carry[4], seq[0], seq[1], seq[2][1:], seq[3][1:], seq[5][1:])\n\n\nFor the documentation of the method proveded by the package, see [WORK OUT HOW TO LINK DO PAGES]"
  },
  {
    "objectID": "quarto/mathematics.html#the-information-filter",
    "href": "quarto/mathematics.html#the-information-filter",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.2 The Information Filter",
    "text": "3.2 The Information Filter\nIn some computational scenarios, it is beneficial to work with vectors of consistent dimension. In python jax, the efficient scan and map operations work only with such operations; JAX has no support for jagged arrays, and traditional for loops with have long compile times when jit-compiled. Although there are some tools in JAX to get around this problem (namely the jax.tree functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it’s core, a scan-type operation, this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.\nBut it is possible to re-write the kalman filter in a way which is compatible with this kind of data. the sometimes called ‘information filter’ involves transforming the data into a kind of ‘information form’, which will always have consistent dimension.\nThe information filter is simply the kalman filter re-written to use the Gaussian distribution’s canonical parameters, those being the information vector and the information matrix. If a Gaussian distribution has mean \\(\\boldsymbol{\\mathbf{\\mu}}\\) and variance matrix \\(\\Sigma\\), then the corresponding information vector and information matrix is \\(\\nu = \\Sigma^{-1}\\mu\\) and \\(Q = \\Sigma^{-1}\\), correspondingly.\n\nTheorem 2 (The Information Filter) The Kalman filter can be rewritten in information form as follows (for example, Khan 2005). Write\n\\[\\begin{split}\nQ_{i\\mid j} &= P_{i\\mid j}\\\\\n\\boldsymbol{\\mathbf{\\nu}}_{i\\mid j} &= Q_{i\\mid j} \\boldsymbol{\\mathbf{m}}_{i\\mid j}\n\\end{split}\n\\]\nand transform the observations into their ‘information form’, for \\(t=1,\\dots, T\\)\n\\[\\begin{split}\nI_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1}\\Phi_{t},\\\\\ni_{t} = \\Phi_{t}^{\\intercal} \\Sigma_{\\epsilon}^{-1} \\boldsymbol{\\mathbf{z}}_{t}.\n\\end{split}\n\\tag{11}\\]\nThe prediction step now becomes\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} &= (I-J_t) M^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}\\\\\nQ_{t+1\\mid t} &= (I-J_t) S_{t}\n\\end{split}\n\\]\nwhere \\(S_t = M^{-\\intercal} Q_{t\\mid t} M^{-1}\\) and \\(J_t = S_t [S_{t}+\\Sigma_{\\eta}^{-1}]^{-1}\\).\nUpdating is now as simple as adding the information-form observations;\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t+1} &= \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} + i_{t+1}\\\\\n  Q_{t+1\\mid t+1} &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\n\nProof in Appendix (Section 6.2.)\nWe can see that the information form of the observations (Equation 11) will always have the same dimension (that being the process dimension, previously labelled \\(r\\), the number of basis functions used in the expansion). For our purposes, this means that jax.lax.scan will work after we ‘informationify’ the data, which can be done using jax.tree.map. This is implemented in the functions information_filter and information_filter_indep (for uncorrelated errors).\nThere are other often cited advantages to filtering in this form. It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with \\([S_t + \\Sigma_\\eta]^{-1}\\) in the process dimesnion instead of \\([\\Phi_t P_{t+1\\mid t} \\Phi_t^\\intercal + \\Sigma_\\epsilon]^{-1}\\) in the observation dimension) (Assimakis, Adam, and Douladiris 2012).\nThe other often mentioned advantage is the ability to use a truly vague prior for \\(\\alpha_0\\); that is, we can set \\(Q_0\\) as the zero matrix, without worriying about an infinite variance matrix. While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see (Section 6.3)."
  },
  {
    "objectID": "quarto/mathematics.html#kalman-smoothers",
    "href": "quarto/mathematics.html#kalman-smoothers",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "3.3 Kalman Smoothers",
    "text": "3.3 Kalman Smoothers\nBeyond the Kalman filters, we can also do Kalman smoothers. That is, filters estimate \\(\\boldsymbol{\\mathbf{m}}_{T\\mid T}\\) and \\(P_{T\\mid T}\\), but there is use for estimating \\(\\boldsymbol{\\mathbf{m}}_t\\mid T\\) and \\(P_{t\\mid T}\\) for all \\(t=0,\\dots, T\\).\nWe can then work backwards from these values using what is known as the Rauch-Tung-Striebel (RTS) smoother;\n\\[\\begin{split}\n\\boldsymbol{\\mathbf{m}}_{t-1\\mid T} &= \\boldsymbol{\\mathbf{m}}_{t-1\\mid t-1} + J_{t-1}(\\boldsymbol{\\mathbf{m}}_{t\\mid T} - \\boldsymbol{\\mathbf{m}}_{t\\mid t-1}),\\\\\nP_{t-1\\mid T} &= P_{t-1\\mid t-1} + J_{t-1}(P_{t\\mid T} - P_{t\\mid t-1})J_{t-1}^\\intercal,\n\\end{split}\n\\tag{12}\\]\nwhere,\n\\[\\begin{split}\nJ_{t-1} = P_{t-1\\mid t-1}M^\\intercal[P_{t\\mid t-1}]^{-1}.\n\\end{split}\n\\]\nWe can clearly see, then, that it is crucial to keep the values in Equation 9.\nWe can then also compute the lag-one cross-covariance matrices \\(P_{t,t-1\\mid T}\\) using the Lag-One Covariance Smoother (is this what they call the RTS smoother?) From\n\\[\\begin{split}\nP_{T,T-1\\mid T} = (I - K_T\\Phi_{T}) MP_{T-1\\mid T-1},\n\\end{split}\n\\]\nwe can compute the lag-one covariances\n\\[\\begin{split}\nP_{t, t-1\\mid T} = P_{t\\mid t}J_{t-1}^\\intercal + J_{t}[P_{t+1,t\\mid T} - MP_{t-1\\mid t-1}]J_{t-1}^\\intercal\n\\end{split}\n\\tag{13}\\]\nThese values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later."
  },
  {
    "objectID": "quarto/mathematics.html#sec-app1",
    "href": "quarto/mathematics.html#sec-app1",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "6.2 Proof of Theorem 2",
    "text": "6.2 Proof of Theorem 2\n\nProof. Firstly, for the prediction step, using \\(S_t = M^{-\\intercal}Q_{t\\mid t}M^{-1}\\) and \\(J_t = S_t(\\Sigma_\\eta^{-1} + S_t)^{-1}\\) and the identities Equation 16 and Equation 17,\n\\[\\begin{split}\n  Q_{t+1\\mid t} &= P_{t+1\\mid t}^{-1} = (MQ_{t\\mid t}^{-1}M^\\intercal + \\Sigma_\\eta)^{-1}\\\\\n  &= S_t - J_t S_t = (I-J_t)S_t,\n\\end{split}\n\\]\nwhere we used \\(A=MQ_{t\\mid t}^{-1}M^\\intercal\\), \\(C=\\Sigma_\\eta\\) and \\(U=C=I\\) in Equation 16. Thurthermore,\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} &= Q_{t+1\\mid t} \\boldsymbol{\\mathbf{m}}_{t+1\\mid t}\\\\\n  &= Q_{t+1\\mid t} M Q_{t\\mid t}^{-1} \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t} = Q_{t+1\\mid t} (M Q_{t\\mid t}^{-1}) \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal}Q_{t\\mid t}M^{-1} (M Q_{t\\mid t}^{-1}) \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}\\\\\n  &= (I-J_t)M^{-\\intercal} \\boldsymbol{\\mathbf{\\nu}}_{t\\mid t}.\n\\end{split}\n\\]\nFor the update step,\n\\[\\begin{split}\n  Q_{t+1\\mid t+1} &= P_{t+1\\mid t+1}^{-1}\\\\\n  &= (Q_{t+1}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}\\Sigma_\\epsilon\\Phi_{t+1}^\\intercal + \\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})^{-1}\\\\\n  &= ((Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1})^{-1})^{-1} = Q_{t+1\\mid t} + \\Phi_{t+1}^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_{t+1}\\\\\n  &= Q_{t+1\\mid t} + I_{t+1}.\n\\end{split}\n\\]\nThen, writing \\(\\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1}\\) in terms of \\(Q_{t+1\\mid t}\\) and \\(\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t}\\)\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1} &= Q_{t+1\\mid t}^{-1} \\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1} [\\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1} - \\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mit t}]\\\\\n  &= (Q_{t+1\\mid t}^{-1} - Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\Phi_{t+1}Q_{t+1\\mid t}^{-1})\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t} \\\\\n  &\\quad + Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal[\\Phi_{t+1}Q_{t+1\\mid t}^{-1}\\Phi_{t+1}^\\intercal +\\Sigma_\\epsilon]^{-1}\\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1}\\\\\n  &= [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t}\\\\\n  &\\quad + [Q_{t+1\\mid t} + I_{t+1}]^{-1}\\Phi_{t+1}\\Sigma_\\epsilon^{-1}\\tilde{\\boldsymbol{\\mathbf{z}}}_{t+1},\n\\end{split}\n\\]\nand now noting that \\(\\boldsymbol{\\mathbf{\\nu}}_{t+1\\mid t+1} = (Q_{t+1\\mid t} + I_{t+1}) \\boldsymbol{\\mathbf{m}}_{t+1\\mid t+1}\\), we complete the proof."
  },
  {
    "objectID": "quarto/mathematics.html#sec-vagueprior",
    "href": "quarto/mathematics.html#sec-vagueprior",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "6.3 Truly Vague Prior with the Kalman Filter",
    "text": "6.3 Truly Vague Prior with the Kalman Filter\nIt has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior \\(Q_{0}=0\\). While this is true, it is actually possible to do this in the Kalman filter by ‘skipping’ the first step (contrary to some sources, such as the wikipedia page as of January 2025).\n\nTheorem 3 In the Kalman Filter (Section 3.1), if we allow \\(P_{0}^{-1} = 0\\), effectively setting infinite variance, and assuming the propegator matrix \\(M\\) is invertible, we have\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1} \\Phi_1 \\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_1,\\\\\n  P_{1\\mid1} &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}.\n\\end{split}\n\\tag{18}\\]\nTherefore, starting with these values then continuing the filter as normal, we can perform the kalman filter with ‘infinite’ prior variance.\n[NOTE: The requirement that M be invertible should be droppable, see the proof below]\n\n\nProof. Unsuprisingly, the proof is effectively equivalent to proving the information filter and setting \\(Q_0 = P_0^{-1}=0\\).\nFor the first predict step (Equation 9),\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid0} &= M \\boldsymbol{\\mathbf{m}}_0,\\\\\n  P_{1\\mid0} &= M P_0 M^\\intercal + \\Sigma_\\eta.\n\\end{split}\n\\]\nBy (Equation 16),\n\\[\\begin{split}\n  P_{1\\mid0}^{-1} &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (P_0^{-1} + M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} M (M^\\intercal \\Sigma_\\eta^{-1} M)^{-1}M^\\intercal\\Sigma_\\eta^{-1}\\\\\n  &= \\Sigma_\\eta^{-1} - \\Sigma_\\eta^{-1} = 0.\n\\end{split}\n\\]\nSo, moving to the update step (Equation 10),\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid1} = M \\boldsymbol{\\mathbf{m}}_0 + P_{1\\mid0}\\Phi_1 [\\Phi_1 P_{1\\mid0} \\Phi_1^\\intercal + \\Sigma_\\epsilon]^{-1}(\\tilde{\\boldsymbol{\\mathbf{z}}}_1 - \\Phi M \\boldsymbol{\\mathbf{m}}_0).\\\\\n\\end{split}\n\\]\nApplying (Equation 17) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1, V=\\Phi_1^\\intercal, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  \\boldsymbol{\\mathbf{m}}_{1\\mid1} &= M \\boldsymbol{\\mathbf{m}}_0 + (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\Phi_1)^{-1}\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}(\\tilde{\\boldsymbol{\\mathbf{z}}}_1 - \\Phi_1 M\\boldsymbol{\\mathbf{m}}_0)\\\\\n  &= M \\boldsymbol{\\mathbf{m}}_0 + (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_1 - (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1M\\boldsymbol{\\mathbf{m}}_0\\\\\n  &= (\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\Phi_1^\\intercal\\Sigma_\\epsilon^{-1} \\tilde{\\boldsymbol{\\mathbf{z}}}_1.\n\\end{split}\n\\]\nFor the variance, we apply the (Equation 16) with \\(A = P_{1\\mid0}^{-1}, U=\\Phi_1^\\intercal, V=\\Phi_1, C=\\Sigma_\\epsilon^{-1}\\),\n\\[\\begin{split}\n  P_{1\\mid1} &= (I - P_{1\\mid0}\\Phi_1^\\intercal[\\Sigma_\\epsilon + \\Phi_1^\\intercal P_{1\\mid0}\\Phi_1]^{-1}\\Phi_1)P_{1\\mid0}\\\\\n  &= (P_{1\\mid0}^{-1} + \\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1}\\\\\n  &= (\\Phi_1^\\intercal \\Sigma_\\epsilon^{-1}\\Phi_1)^{-1},\n\\end{split}\n\\]\nas needed.\n\nIt is worth noting that (Equation 18) seems to make a lot of sense; namely, we expect the estimate for \\(\\boldsymbol{\\mathbf{m}}_0\\) to look like a correlated least squares-type estimator like this."
  },
  {
    "objectID": "quarto/mathematics.html#woodburys-identity",
    "href": "quarto/mathematics.html#woodburys-identity",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "6.1 Woodbury’s identity",
    "text": "6.1 Woodbury’s identity\n\nThe following two sections will make heavy use of the Woodbury identity.\n\nLemma 2 (Woodbury’s Identity) We have, for conformable matrices \\(A, U, C, V\\),\n\\[\\begin{split}\n(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.\n\\end{split}\n\\tag{16}\\]\nAdditionally, we have the variant\n\\[\\begin{split}\n(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.\n\\end{split}\n\\tag{17}\\]\n\n\nProof. We only prove (Equation 17), since various proofs of (Equation 16) are well known (see, for example, the wikipedia page).\nSimply multipliying (Equation 16) by \\(CU\\), (similar to Khan 2005, although there is an error in their proof)\n\\[\\begin{split}\n(A+UCV)^{-1}UC &= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\\\\n&= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\\\\n&= A^{-1}U(C^{-1}+VA^{-1}U)\n\\end{split}\n\\]\nas needed."
  },
  {
    "objectID": "quarto/mathematics.html#footnotes",
    "href": "quarto/mathematics.html#footnotes",
    "title": "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nat least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see Liu, Yeo, and Lu (2022)↩︎"
  }
]