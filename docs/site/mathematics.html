<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Evan Tate Paterson Hughes">

<title>Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations – Integro-Difference Equation Models in Python-JAX</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-b7e3851fd6037cd78c5cadc88b3bcc06.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="text/javascript">
MathJax = {
  tex: {
    macros: {
      var: "{\\mathbb{V}\\mathrm{ar}}",
      cov: "{\\mathbb{C}\\mathrm{ov}}",
      eqc: ["{\\stackrel{c}{=}}", 0],
      bv: ["{\\boldsymbol{\\mathbf{#1}}}", 1]
    }
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Integro-Difference Equation Models in Python-JAX</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../site/mathematics.html" aria-current="page"> 
<span class="menu-text">The Maths</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../reference/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/tatephughes/JAX-IDEM">
            Source
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/tatephughes/JAX-IDEM/issues">
            Issues
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#integro-difference-based-dynamics" id="toc-integro-difference-based-dynamics" class="nav-link" data-scroll-target="#integro-difference-based-dynamics"><span class="header-section-number">2</span> Integro-difference Based Dynamics</a></li>
  <li><a href="#spectral-representations" id="toc-spectral-representations" class="nav-link" data-scroll-target="#spectral-representations"><span class="header-section-number">3</span> Spectral Representations</a>
  <ul class="collapse">
  <li><a href="#process-decomposition" id="toc-process-decomposition" class="nav-link" data-scroll-target="#process-decomposition"><span class="header-section-number">3.1</span> Process decomposition</a></li>
  <li><a href="#spectral-form-of-the-process-noise" id="toc-spectral-form-of-the-process-noise" class="nav-link" data-scroll-target="#spectral-form-of-the-process-noise"><span class="header-section-number">3.2</span> Spectral form of the Process Noise</a></li>
  <li><a href="#sec-kerneldecomp" id="toc-sec-kerneldecomp" class="nav-link" data-scroll-target="#sec-kerneldecomp"><span class="header-section-number">3.3</span> Kernel Parameterisations</a></li>
  <li><a href="#idem-as-a-linear-dynamical-system" id="toc-idem-as-a-linear-dynamical-system" class="nav-link" data-scroll-target="#idem-as-a-linear-dynamical-system"><span class="header-section-number">3.4</span> IDEM as a linear dynamical system</a></li>
  <li><a href="#example-simulation" id="toc-example-simulation" class="nav-link" data-scroll-target="#example-simulation"><span class="header-section-number">3.5</span> Example Simulation</a></li>
  </ul></li>
  <li><a href="#the-kalman-filter-and-its-many-flavours" id="toc-the-kalman-filter-and-its-many-flavours" class="nav-link" data-scroll-target="#the-kalman-filter-and-its-many-flavours"><span class="header-section-number">4</span> The Kalman filter, and its many flavours</a>
  <ul class="collapse">
  <li><a href="#sec-kalmanfilter" id="toc-sec-kalmanfilter" class="nav-link" data-scroll-target="#sec-kalmanfilter"><span class="header-section-number">4.1</span> The Kalman Filter</a></li>
  <li><a href="#the-information-filter" id="toc-the-information-filter" class="nav-link" data-scroll-target="#the-information-filter"><span class="header-section-number">4.2</span> The Information Filter</a></li>
  <li><a href="#the-square-root-filters" id="toc-the-square-root-filters" class="nav-link" data-scroll-target="#the-square-root-filters"><span class="header-section-number">4.3</span> The Square-Root filters</a>
  <ul class="collapse">
  <li><a href="#the-square-root-kalman-filter" id="toc-the-square-root-kalman-filter" class="nav-link" data-scroll-target="#the-square-root-kalman-filter"><span class="header-section-number">4.3.1</span> The Square-root Kalman filter</a></li>
  <li><a href="#square-root-information-filter" id="toc-square-root-information-filter" class="nav-link" data-scroll-target="#square-root-information-filter"><span class="header-section-number">4.3.2</span> Square-root Information filter</a></li>
  </ul></li>
  <li><a href="#smoothing" id="toc-smoothing" class="nav-link" data-scroll-target="#smoothing"><span class="header-section-number">4.4</span> Smoothing</a></li>
  </ul></li>
  <li><a href="#em-algorithm-needs-a-lot-of-work-probably-ignore-for-now" id="toc-em-algorithm-needs-a-lot-of-work-probably-ignore-for-now" class="nav-link" data-scroll-target="#em-algorithm-needs-a-lot-of-work-probably-ignore-for-now"><span class="header-section-number">5</span> EM Algorithm (NEEDS A LOT OF WORK, PROBABLY IGNORE FOR NOW)</a></li>
  <li><a href="#algorithm-for-maximum-complete-data-likelihood-estimation" id="toc-algorithm-for-maximum-complete-data-likelihood-estimation" class="nav-link" data-scroll-target="#algorithm-for-maximum-complete-data-likelihood-estimation"><span class="header-section-number">6</span> Algorithm for Maximum Complete-data Likelihood estimation</a></li>
  
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="mathematics.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Evan Tate Paterson Hughes </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p><a href="../index.html">Index</a></p>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The Integro-Difference equation model (here abbreviated as IDEM <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) is dynamics-based spatio-temporal aiming to model diffusion and convection by making the value of a process a weighted average of it’s previous time, plus noise.</p>
<p><span style="color: red;">[NOTE: I intend to create a more thorough background for the introduction here.]</span></p>
</section>
<section id="integro-difference-based-dynamics" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Integro-difference Based Dynamics</h1>
<p>As common and widespread as the problem is, spatio-temporal modelling still presents a great deal of difficulty. Inherently, Spatio-Temporal datasets are almost always high-dimensional, and repeated observations are usually not possible.</p>
<p>Traditionally, the problem has been tackled by the moments (usually the means and covariances) of the process in order to make inference (<span class="citation" data-cites="wikle2019spatio">Wikle, Zammit-Mangion, and Cressie (<a href="#ref-wikle2019spatio" role="doc-biblioref">2019</a>)</span>, for example, call this ‘descriptive’ modelling). While this method can be sufficient for many problems, there are many cases where we are underutilizing some knowledge of the underlying dynamic systems involved. For instance, in temperature models, we know that temperature has movement (convection) and spread (diffusion), and that the state at any given time will depend on its state at previous times <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. We call models which make use of this ‘dynamic’ models.</p>
<p>A general way of writing such hierarchical dynamical models might be</p>
<p><span class="math display">\[\begin{split}
Y_{t+1}(\cdot) &amp;= \mathcal M_t(Y_0(\cdot), \dots, Y_t(\cdot)) + \omega_t(\cdot), \quad t=0, \dots, T-1,\\
Z_t(\cdot) &amp;= \mathcal O_t(Y_t(\cdot)) + x(\cdot)^{\intercal}\bv \beta + \epsilon_t(\cdot), \quad t=1,\dots,T.
\end{split}
\]</span></p>
<p>This describes the scalar random fields <span class="math inline">\(Z_t(\cdot), Y_t(\cdot)\in \mathbb R\)</span> over the space <span class="math inline">\(\mathcal D\subset \mathcal R^d\)</span>, which are the observed data and unobserved dynamic process, respectively. <span class="math inline">\(\mathcal M_t\)</span> here is a non-random ‘propagation operator’, defining how the process evolves with respect to it’s previous state(s), and <span class="math inline">\(\mathcal O_t\)</span> is a non-random ‘observation operator’, defining how observations of a given process state are taken. Both these fields have random (usually time-independent) additive random fields, <span class="math inline">\(\omega_t(\cdot), \epsilon_t(\cdot)\)</span>, and we also include non-random measured linear covariate terms <span class="math inline">\(x(\cdot)^{\intercal}\bv \beta\)</span>.</p>
<p>If we discretize the space into <span class="math inline">\(n\)</span> $spatial locations <span class="math inline">\(\{\bv s_i\}_{i=1,\dots, n}\)</span>, assume the operator are linear, assert a Markov condition, and assume the errors are all normal, we get a simple linear dynamic system;</p>
<p><span id="eq-ldstm"><span class="math display">\[\begin{split}
\bv Y_{t+1} &amp;= M_t\bv Y_t + \bv \omega_t, \quad t=0, \dots, T-1,\\
\tilde{\bv Z}_t &amp;= O_t\bv Y_t + \bv \epsilon_t, \quad t=1,\dots,T,
\end{split}
\tag{1}\]</span></span></p>
<p>where we have written <span class="math inline">\(\bv Y_t = (Y_t(\bv s_1),\dots, Y_t(\bv s_n))\)</span>, and similar for <span class="math inline">\(\bv Z_t, \bv \epsilon_t\)</span> and <span class="math inline">\(\bv\omega_t\)</span>, and we have written <span class="math inline">\(\tilde{\bv Z}_t = \bv Z_t + X^{\intercal}\bv \beta\)</span>. This is a well-known type of system, the process <span class="math inline">\(Y\)</span> can easily be estimated either directly of with a Kalman filter/smoother and variants, which will be discussed later.</p>
<p>However, this model is restrictive and high-dimensional; <span class="math inline">\(M_t\)</span>, the primary quantities which needs estimation, is of dimension <span class="math inline">\(n\times n\)</span>, of which there are <span class="math inline">\(T\)</span> matrices to be estimated. Even if we allow the propagation matrix to be invariant in time, we can still only make predictions at the stations <span class="math inline">\(\{\bv s_i\}\)</span>.</p>
<p>This motivates a different approach; in particular, one which allows us to estimate the random field at arbitrary points <span class="math inline">\(Y_t(\bv s)\)</span> using some spectral decomposition, which would alleviate these problems.</p>
<p>The Integro-difference equation model attempts to generalise <a href="#eq-ldstm" class="quarto-xref">Equation&nbsp;1</a> into the continuous space by replacing the discrete linear <span class="math inline">\(M_t\)</span> by a continuous integral equivalent;</p>
<p><span id="eq-IDEM"><span class="math display">\[\begin{split}
  Y_{t+1}(\bv s) &amp;= \int_{\mathcal D_s} \kappa_t(\bv s,\bv r) Y_t(\bv r) d\bv r + \omega_t(\bv s), \quad t=0, \dots, T-1, \\
  Z_t(\bv s) &amp;= Y_t(\bv s) + X(\bv s)^{\intercal}\bv \beta + \epsilon_t(\bv s), \quad t=1,\dots,T.
\end{split}
\tag{2}\]</span></span></p>
<p>Where <span class="math inline">\(\omega_t(\bv s)\)</span> is a small scale gaussian variation with no temporal dynamics <span class="citation" data-cites="cressie2015statistics">(<a href="#ref-cressie2015statistics" role="doc-biblioref">Cressie and Wikle 2015</a> call this a ‘spatially descriptive’ component)</span>, <span class="math inline">\(\bv X(\bv s)\)</span> are spatially varying covariates (for example, in a large-scale climate scenario, this might be latitude, concentration of some chemical/element like nitrogen) <span class="math inline">\(\kappa(\bv s, \bv r)\)</span> is the driving ‘kernel’ function, and <span class="math inline">\(\epsilon_t\)</span> is a Gaussian white noise ‘measurement error’ term.</p>
<p>Our operator is now <span class="math inline">\(\mathcal M(Y_t(\bv s)) = \int_{\mathcal D_s} \kappa_t(\bv s,\bv r) Y_t(\bv r) d\bv r\)</span>, which can model diffusion and convection by choosing the shape of <span class="math inline">\(\kappa\)</span> (which, from now on, we will assume to be temporally invariant). This kernel defines how each point in space is affected by every other point in space at the previous time. For example, if we choose a Gaussian-like shape,</p>
<p><span class="math display">\[\begin{split}
  \kappa(\bv s, \bv r; \bv m, a, b) = a \exp \left( -\frac{1}{b} \vert \bv s- \bv r +\bv m(\bv s)\vert^2 \right),
\end{split}
\]</span></p>
<p>then the ‘flow’ would be in the direction of <span class="math inline">\(-\bv m(\bv s)\)</span>, and the diffusion would be controlled by <span class="math inline">\(b\)</span> and <span class="math inline">\(a\)</span>. This creates a ‘spatially variant kernel’, where the direction of flow varies across the space, as in <a href="#fig-examplekernelvar" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-examplekernelvar" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-examplekernelvar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="figure/kernel_example_direction_var.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Invariant Kernel Direction"><img src="figure/kernel_example_direction_var.png" class="img-fluid figure-img"></a></p>
<figcaption>Invariant Kernel Direction</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="figure/kernel_example_strength_var.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Invariant Kernel Strength"><img src="figure/kernel_example_strength_var.png" class="img-fluid figure-img"></a></p>
<figcaption>Invariant Kernel Strength</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-examplekernelvar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A spatially variant kernel across the region <span class="math inline">\([0,1]\times[0,1]\)</span>. The kernel direction is shown on the left, and on the right is the amount that each point affects the point <span class="math inline">\((0.5,0.5)\)</span>, marked with a red cross. ‘Flow’ is allowed to vary by a function <span class="math inline">\(\bv m(\bv s)\)</span> which is chosen randomly using a basis expansion (see <a href="#sec-kerneldecomp" class="quarto-xref">Section&nbsp;3.3</a>). The other two parameters are set at <span class="math inline">\(a=150,b=0.2\)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="spectral-representations" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Spectral Representations</h1>
<p>The key to being able to computationally work with IDEMs, as perhaps originally made by <span class="citation" data-cites="wikle1999dimension">Wikle and Cressie (<a href="#ref-wikle1999dimension" role="doc-biblioref">1999</a>)</span>, is to work with the spectral decomposition of the process, in order to coerce the model hierarchy into a more familiar linear dynamical system form, like <a href="#eq-ldstm" class="quarto-xref">Equation&nbsp;1</a>.</p>
<p>This kind of dimension-reduction allows us to parametrise spatial fields with as few or as many parameters as we want.</p>
<section id="process-decomposition" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="process-decomposition"><span class="header-section-number">3.1</span> Process decomposition</h2>
<p>Choose a complete class of spatial spectral basis functions, <span class="math inline">\(\{\phi_i(\cdot): \mathcal D\to \mathbb R\}_{i=1,\dots}\)</span>, and decompose the process spatial field at each time;</p>
<p><span id="eq-processdecomp"><span class="math display">\[\begin{split}
Y_t(\bv s) \approx \sum_{i=1}^{r} \alpha_{i,t} \phi_i(\bv s), \quad t=0,\dots,T.
\end{split}
\tag{3}\]</span></span></p>
<p>where we truncate the expansion at some <span class="math inline">\(r\in\mathbb N\)</span>. Notice that we can write this in vector/matrix form, where we consider the vector field <span class="math inline">\(\bv \phi(\cdot) = (\phi_1(\cdot),\dots, \phi_r(\cdot))^\intercal\)</span>; considering times <span class="math inline">\(t=1,2,\dots, T\)</span>, we set</p>
<p><span id="eq-vecmats"><span class="math display">\[\begin{split}
\bv \phi(\bv s) &amp;= (\phi_1(\bv s), \phi_2(\bv s), \dots, \phi_r(\bv s))^{\intercal},\\
\bv \alpha_t &amp;= (\alpha_{1,t}, \alpha_{2,t}, \dots, \alpha_{r, t})^{\intercal}.
\end{split}
\tag{4}\]</span></span></p>
<p>Now, (<a href="#eq-processdecomp" class="quarto-xref">Equation&nbsp;3</a>) gives us, for any <span class="math inline">\(\bv s\in \mathcal D\)</span>,</p>
<p><span id="eq-pbvec"><span class="math display">\[\begin{split}
Y(\bv s; t) \approx \bv \phi^{\intercal}(\bv s)  \alpha(t).\\
\end{split}
\tag{5}\]</span></span></p>
<p>We can effectively now work exclusively with <span class="math inline">\(\bv \alpha_t = (\alpha_{1,t},\dots, \alpha_{r,t})^\intercal\)</span>. To do so, we need to find the evolution equation of <span class="math inline">\(\bv \alpha_t\)</span>, as given below.</p>
<div id="thm-state_form" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Spectral form of the state evolution)</strong></span> Define the <em>Gram matrix</em>;</p>
<p><span id="eq-gram"><span class="math display">\[\Psi \coloneq \int_{\mathcal D_s} \bv \phi(\bv s) \bv \phi(\bv s)^\intercal d\bv s.
\tag{6}\]</span></span></p>
<p>Then, the basis coefficients evolve by the equation</p>
<p><span id="eq-stateev"><span class="math display">\[\bv \alpha_{t+1} = M \bv\alpha_t + \bv\eta_t,
\tag{7}\]</span></span></p>
<p>where <span class="math inline">\(M = \Psi^{-1} \int\int \bv\phi(\bv s) \kappa(\bv s, \bv r)\bv\phi(\bv r)^\intercal d\bv r d \bv s\)</span> and <span class="math inline">\(\bv\eta_t =\Psi^{-1} \int \bv \phi(\bv s)\omega_t(s)d\bv s\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="citation" data-cites="dewar2008data">(Adapting from <a href="#ref-dewar2008data" role="doc-biblioref">Dewar, Scerri, and Kadirkamanathan 2008</a>)</span>, write out the process equation, (<a href="#eq-IDEM" class="quarto-xref">Equation&nbsp;2</a>), using the first equation of (<a href="#eq-pbvec" class="quarto-xref">Equation&nbsp;5</a>);</p>
<p><span class="math display">\[Y_{t+1}(\bv s) = \bv \phi(\bv s)^{\intercal} \alpha_{t+1} = \int_{\mathcal D_s} \kappa(\bv s, \bv r) \bv\phi(\bv r)^{\intercal}\bv \alpha_t d\bv r + \omega_t(\bv s),
\]</span></p>
<p>We then multiply both sides by <span class="math inline">\(\bv \phi(s)\)</span> and integrate over <span class="math inline">\(\bv s\)</span></p>
<p><span class="math display">\[\begin{split}
\int_{\mathcal D_s} \bv\phi(\bv s)\bv\phi(\bv s)^{\intercal} d\bv s \bv\alpha_{t+1} &amp;= \int\bv\phi(\bv s)\int \kappa(\bv s, \bv r)\bv\phi(\bv r)^\intercal d\bv r  d \bv s\ \bv\alpha_t + \int \bv \phi(\bv s)\omega_t(s)d\bv s\\
\Psi \bv\alpha_{t+1} &amp;= \int\int \bv\phi(\bv s)\kappa(\bv s, \bv r) \bv\phi(\bv r)^\intercal d\bv r d \bv s\ \bv\alpha_t + \int \bv \phi(\bv s)\omega_t(s)d\bv s.
\end{split}
\]</span></p>
<p>So, finally, pre-multipling by the inverse of the gram matrix, <span class="math inline">\(\Psi^{-1}\)</span> (<a href="#eq-gram" class="quarto-xref">Equation&nbsp;6</a>), we arrive at the result. </p>
</div>
</section>
<section id="spectral-form-of-the-process-noise" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="spectral-form-of-the-process-noise"><span class="header-section-number">3.2</span> Spectral form of the Process Noise</h2>
<p>We still have to set out what the process noise, <span class="math inline">\(\omega_t(\bv s)\)</span>, and it’s spectral counterpart, <span class="math inline">\(\bv \eta_t\)</span>, are. <span class="citation" data-cites="dewar2008data">Dewar, Scerri, and Kadirkamanathan (<a href="#ref-dewar2008data" role="doc-biblioref">2008</a>)</span> fix the variance of <span class="math inline">\(\omega_t(\bv s)\)</span> to be uniform and uncorrelated across space and time, with <span class="math inline">\(\omega_t(\bv s) \sim \mathcal N(0,\sigma^2)\)</span> It is then easily shown that <span class="math inline">\(\bv\eta_t\)</span> is also normal, with <span class="math inline">\(\bv\eta_t \sim \mathcal N(0, \sigma^2\Psi^{-1})\)</span>.</p>
<p>However, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of <span class="math inline">\(\bv\eta_t\)</span>, and compute the variance of <span class="math inline">\(\omega_t(\bv s)\)</span> if needed.</p>
<div id="lem-omegadist" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> Let <span class="math inline">\(\bv\eta_t \sim \mathcal N(0, \Sigma_\eta)\)</span>, and <span class="math inline">\(\cov[\bv\eta_t, \bv \eta_{t+\tau}] =0\)</span>, <span class="math inline">\(\forall \tau&gt;0\)</span>. Then <span class="math inline">\(\omega_t(\bv s)\)</span> has covariance</p>
<p><span class="math display">\[\cov [\omega_t(\bv s), \omega_{t+\tau}(\bv r)] = \begin{cases}
\bv\phi(\bv s)^\intercal \Sigma_\eta \bv\phi(\bv r) &amp; \text{if }\tau=0\\
0 &amp; \text{else}\\
\end{cases}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Consider <span class="math inline">\(\Psi \bv\eta_t\)</span>, and consider the case <span class="math inline">\(\tau=0\)</span>. It is clearly normal, with zero expectation and variance (using <a href="#eq-gram" class="quarto-xref">Equation&nbsp;6</a>),</p>
<p><span id="eq-var1"><span class="math display">\[\begin{split}
\var[\Psi \bv\eta_t] &amp;= \Psi \var[\bv\eta_t] \Psi^\intercal = \Psi\Sigma_\eta\Psi^\intercal,\\
&amp;= \int_{\mathcal D_s} \bv\phi(\bv s) \bv\phi(\bv s)^\intercal d\bv s \  \Sigma_\eta \ \int_{\mathcal D_s} \bv\phi(\bv r) \bv\phi(\bv r)^\intercal d\bv r\\
&amp;=  \int\int_{\mathcal D_s^2} \bv\phi(\bv s) \bv\phi(\bv s)^\intercal \  \Sigma_\eta \  \bv\phi(\bv r) \bv\phi(\bv r)^\intercal d\bv r d\bv s\\
\end{split}
\tag{8}\]</span></span></p>
<p>Since it has zero expectation, we also have</p>
<p><span id="eq-var2"><span class="math display">\[\begin{split}
\var[\Psi\bv\eta_t] &amp;= \mathbb E[(\Psi\bv\eta_t) (\Psi\bv\eta_t)^\intercal] = \mathbb E[\Psi\bv\eta_t\bv\eta_t^\intercal\Psi^\intercal]\\
&amp;= \mathbb E \left[ \int_{\mathcal D_s} \bv\phi(\bv s)\omega_t(\bv s)d\bv s \int_{\mathcal D_s} \bv \phi(\bv r)^\intercal \omega_t(\bv r) d\bv r \right]\\
&amp;= \int\int_{\mathcal D_s^2} \bv\phi(\bv s)\  \mathbb E[\omega_t(\bv s)\omega_t(\bv r)]\  \bv \phi(\bv r)^\intercal d\bv s d \bv r.
\end{split}
\tag{9}\]</span></span></p>
<p>We can see that, comparing (<a href="#eq-var1" class="quarto-xref">Equation&nbsp;8</a>) and (<a href="#eq-var2" class="quarto-xref">Equation&nbsp;9</a>), we have</p>
<p><span class="math display">\[\cov [\omega_t(\bv s), \omega_t(\bv r)] = \mathbb E[\omega_t(\bv s)\omega_t(\bv r)]= \bv\phi(\bv s)^\intercal \Sigma_\eta \bv\phi(\bv r).
\]</span></p>
<p>Since, once again, <span class="math inline">\(\mathbb E[\bv\omega_t(\bv s)]=0\)</span>.</p>
<p>For the <span class="math inline">\(\tau\neq0\)</span> case, it is simple to show that the covariance is 0.</p>
<p></p>
</div>
</section>
<section id="sec-kerneldecomp" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-kerneldecomp"><span class="header-section-number">3.3</span> Kernel Parameterisations</h2>
<p>Next is the part of the system, which defines the dynamics; the kernel function, <span class="math inline">\(\kappa\)</span>. There are a few ways to handle the kernel. One of the most obvious is to expand it out into a spectral decomposition as well;</p>
<p><span class="math display">\[\kappa \approx \sum_i \beta_i\psi(\bv s, \bv r).
\]</span></p>
<p>This can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on <span class="math inline">\(\mathbb R^2\times \mathbb R^2\)</span>; to get a wide enough space of possible functions, we would likely need many terms in the spectral expansion.</p>
<p>A much simpler approach would be to simply parametrise the kernel function, to <span class="math inline">\(\kappa(\bv s, \bv r, \bv \theta_\kappa)\)</span>. We then establish a simple shape for the kernel (e.g.&nbsp;Gaussian) and rely on very few parameters (for example, scale, shape, offsets). The example kernel used in the <code>jaxidem</code> is a Gaussian-shape kernel;</p>
<p><span class="math display">\[\kappa(\bv s, \bv r; \bv m, a, b) = a \exp \left( -\frac{1}{b} \vert \bv s- \bv r +\bv m\vert^2 \right).
\]</span></p>
<p>Of course, this kernel lacks spatial dependence. We can add spatial variance back by adding dependence on <span class="math inline">\(\bv s\)</span> to the parameters, for example, varying the offset term as <span class="math inline">\(\bv m(\bv s)\)</span>. Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground <span class="citation" data-cites="cressie2015statistics">(<a href="#ref-cressie2015statistics" role="doc-biblioref">Cressie and Wikle 2015</a>)</span>. The actual parameters of such a spatially-variant kernel are then the spectral coefficients for the expansion of any spatially variant parameters, as well as any constant parameters. This is precisely what is plotting in <a href="#fig-examplekernelvar" class="quarto-xref">Figure&nbsp;1</a>, where the spectral coefficients are randomly sampled from a multivariate normal distribution;</p>
<p><span class="math display">\[\begin{split}
  \bv m(\bv s) = \left(\begin{matrix}
    \sum_{i=1}^{r_m} \phi_{\kappa,i}(\bv s) m^{(x)}_i\\
    \sum_{i=1}^{r_m} \phi_{\kappa,i}(\bv s) m^{(y)}_i
  \end{matrix}\right),
\end{split}
\]</span></p>
<p>where <span class="math inline">\(m^{(x)}_i\)</span> and <span class="math inline">\(m^{(y)}_i\)</span> are coefficients for the x and y coordinates respectively, and <span class="math inline">\(\phi_{\kappa, i}(\bv s)\)</span> are basis functions (e.g.&nbsp;bisquare <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>) functions in <a href="#fig-examplekernelvar" class="quarto-xref">Figure&nbsp;1</a>).</p>
</section>
<section id="idem-as-a-linear-dynamical-system" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="idem-as-a-linear-dynamical-system"><span class="header-section-number">3.4</span> IDEM as a linear dynamical system</h2>
<p>To summarise, we have taken a truncated spectral decomposition to write the Integro-difference equation model as a more traditional linear dynamical system form (<a href="#eq-stateev" class="quarto-xref">Equation&nbsp;7</a>). All that is left is to include our observations in our system.</p>
<p>Lets assume that at each time <span class="math inline">\(t\)</span> there are <span class="math inline">\(n_t\)</span> observations at locations <span class="math inline">\(\bv s_{1,t},\dots, \bv s_{n_{t},t}\)</span>. We write the vector of the process at these points as <span class="math inline">\(\bv Y(t) = (Y(s_{1,t};t), \dots, Y(s_{n_{t},t};t))^\intercal\)</span>, and, in it’s expanded form <span class="math inline">\(\bv Y_t = \Phi_t \bv\alpha_t\)</span>, where <span class="math inline">\(\Phi \in \mathbb R^{r\times n_{t}}\)</span> is</p>
<p><span class="math display">\[\begin{split}
\{\Phi_{t}\}_{i, j} = \phi_{i}(s_{j,t}).
\end{split}
\]</span></p>
<p>For the covariates, we write the matrix <span class="math inline">\(X_t = (\bv X(\bv s_{1, t}), \dots, \bv X(\bv s_{1=n_{t}, t})^\intercal\)</span>. We then have</p>
<p><span class="math display">\[\begin{split}
\bv Z_t &amp;= \Phi \bv \alpha_t + X_{t} \bv \beta + \bv \epsilon_t, \quad t = 1,\dots, T,\\
\bv \alpha_{t+1} &amp;= M\bv \alpha_t + \bv\eta_t,\quad t = 0,2,\dots, T-1,\\
M &amp;= \int_{\mathcal D_s}\bv\phi(\bv s) \bv\phi(\bv s)^\intercal d\bv s \int_{\mathcal D_s^2}\bv\phi(\bv s) \kappa(\bv s, \bv r; \bv\theta_\kappa)\bv\phi(\bv r)^\intercal d\bv r d \bv s,
\end{split}
\]</span></p>
<p>Writing <span class="math inline">\(\tilde{\bv{Z}}_t = \bv Z_t - X_t \bv \beta\)</span>,</p>
<p><span id="eq-stateidem"><span class="math display">\[\begin{split}
\tilde{\bv Z}_t &amp;= \Phi_{t} \bv \alpha_t + \bv \epsilon_t,\quad &amp;t = 1,2,\dots, T,\\
\bv \alpha_{t+1} &amp;= M \bv \alpha_t + \bv\eta_t,\quad &amp;t = 0,1, \dots, T.\\
\end{split}
\tag{10}\]</span></span></p>
<p>We should also initialise <span class="math inline">\(\bv \alpha_0 \sim \mathcal N^{r}(\bv m_{0}, \Sigma_{0})\)</span>, and fix simple distributions to the noise terms,</p>
<p><span class="math display">\[\begin{split}
\bv \epsilon_{t} \overset{\mathrm{iid}}{\sim} \mathcal N_{n_\mathrm{obs}}(0,\Sigma_\epsilon),\\
\bv \eta_{t} \overset{\mathrm{iid}}{\sim} \mathcal N_{R}(0,\Sigma_\eta),
\end{split}
\]</span></p>
<p>which are independent in time.</p>
<p>As in, for example, <span class="citation" data-cites="wikle1999dimension">(<a href="#ref-wikle1999dimension" role="doc-biblioref">Wikle and Cressie 1999</a>)</span>, <a href="#eq-stateidem" class="quarto-xref">Equation&nbsp;10</a> is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood. We can use these quantities in either an EM algorithm or a Bayesian approach, or directly maximise the marginal data likelihood</p>
<p>We now move on to an example simulation of this kind of model using its spectral decomposition and <code>jaxidem</code>.</p>
</section>
<section id="example-simulation" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="example-simulation"><span class="header-section-number">3.5</span> Example Simulation</h2>
<p>We can now use the above to simulate easily from such models; once we have chosen the appropriate decompositions, we simply compute <span class="math inline">\(M\)</span> and propagate <span class="math inline">\(\bv \alpha_t\)</span> as we would when simulating any other linear dynamic system. We then use the spectral coefficients to generate <span class="math inline">\(Y_t(\bv s)\)</span> and <span class="math inline">\(Z_t(\bv s)\)</span> in the obvious way.</p>
<p><code>jaxidem</code> implements this in the function <code>sim_idem</code>, or through the more user-friendly method <code>idem.IDEM.simulate</code>. An object of the <code>IDEM</code> class contains all the necessary information about basis decompositions, and the simulate methods calls <code>simIDEM</code> without compromising its jit-ability (although just-in-time computation obviously isn’t as important for simulation, the jit-ed function could save compile time if someone want to simulate from many models).</p>
<p>The <code>gen_example_idem</code> method creates a simple IDEM object without many required parameters;</p>
<div id="30e17044" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> rand.split(key, <span class="dv">2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> idem.gen_example_idem(keys[<span class="dv">0</span>], k_spat_inv<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>process_data, obs_data <span class="op">=</span> model.simulate(keys[<span class="dv">1</span>], T<span class="op">=</span><span class="dv">3</span>, nobs<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The resulting objects are of class <code>st_data</code>, containing a couple of niceties for handling spatio-temporal data, while still storing all data as JAX arrays. For example, the <code>show_plot</code>, <code>save_plot</code> and <code>save_gif</code> methods provide easy plotting;</p>
<div id="7be59598" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>process_data.save_plot(<span class="st">'figure/process_data_example.png'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>obs_data.save_plot(<span class="st">'figure/obs_data_example.png'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="mathematics_files/figure-html/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="mathematics_files/figure-html/cell-5-output-1.png" width="568" height="134" class="figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="mathematics_files/figure-html/cell-5-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="mathematics_files/figure-html/cell-5-output-2.png" width="568" height="134" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="fig-examplesim" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-examplesim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="figure/process_data_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Process Simulation"><img src="figure/process_data_example.png" class="img-fluid figure-img"></a></p>
<figcaption>Process Simulation</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="figure/obs_data_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Observation Simulation"><img src="figure/obs_data_example.png" class="img-fluid figure-img"></a></p>
<figcaption>Observation Simulation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-examplesim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Example simulations from an Integro-difference Equation Model. Kernel is generated with spatially varying flow terms, generated by bisquare basis functions with randomly generated coefficient. Note that some artefacts from the decomposition are visible, such as a faint chequerboard pattern in the process.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="the-kalman-filter-and-its-many-flavours" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Kalman filter, and its many flavours</h1>
<p>The Kalman filter gives us linear estimates for the distribution of <span class="math inline">\(\bv\alpha_r\mid \{\bv Z_t=\bv z_t\}_{t=0,...,r}\)</span> in any dynamical system like <a href="#eq-ldstm" class="quarto-xref">Equation&nbsp;1</a>. Now that we have written the IDEM in this form, this filter can now help compute estimates for the moments of the state <span class="math inline">\(\bv \alpha_t\)</span>. The Kalman filter also computes the marginal data likelihood, <span class="math inline">\(\pi(\{\bv z_t\}_{t=1,\dots, T}\mid \bv\theta)\)</span>, where <span class="math inline">\(\bv\theta\)</span> are the model parameters. This allows us to perform maximum-likelihood estimation (as well as any other likelihood-based method of optimization). We will not prove the Kalman filter here, <span class="citation" data-cites="shumway2000time">(for that, see, for example, <a href="#ref-shumway2000time" role="doc-biblioref">Shumway, Stoffer, and Stoffer 2000</a>)</span>.</p>
<p>Since it’s initial formulation in the 50s by a variety of authors (Kálmán included) there have been many variations of the Kalman filter proposed, even as recently as this decade with the temporally paralellised Kalman filter, more technically a variant of the information form of the Kalman filter, by <span class="citation" data-cites="sarkka2020temporal">Särkkä and Garcı́a-Fernández (<a href="#ref-sarkka2020temporal" role="doc-biblioref">2020</a>)</span>.</p>
<section id="sec-kalmanfilter" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-kalmanfilter"><span class="header-section-number">4.1</span> The Kalman Filter</h2>
<p>Firstly, we should establish some notation. Write</p>
<p><span class="math display">\[\begin{split}
m_{i \mid j} &amp;= \mathbb E[\bv\alpha_i \mid \{\bv Z_t=\bv z_t\}_{t=0,\dots,j}],\\
P_{i \mid j} &amp;= \var[\bv\alpha_i \mid \{\bv Z_t=\bv z_t\}_{t=0,\dots,j}],\\
P_{i,j \mid k} &amp;= \cov[\bv\alpha_i, \bv\alpha_k \mid \{\bv Z_t=\bv z_t\}_{t=0,\dots,k}].
\end{split}
\]</span></p>
<p>For the initial terms, we choose Bayesian-like prior moments <span class="math inline">\(m_{0\mid0}=m_0\)</span> and <span class="math inline">\(P_{0\mid0}=\Sigma_0\)</span>. For convenience and generality, we write <span class="math inline">\(\Sigma_\eta\)</span> and <span class="math inline">\(\Sigma_\epsilon\)</span> for the variance matrices of the process and observations. Note that, if the number of observations change at each time point (for example, due to missing data), then <span class="math inline">\(\Sigma_\epsilon\)</span> should be time varying (even in its shape); we could either always keep it as uncorrelated so that <span class="math inline">\(\Sigma_\epsilon = \mathrm{diag} (\sigma_\epsilon^2)\)</span>, or perhaps put some kind of distance-dependant covariance function to it.</p>
<p>To move the filter forward, that is, given <span class="math inline">\(m_{t\mid t}\)</span> and <span class="math inline">\(P_{t\mid t}\)</span>, to get <span class="math inline">\(m_{t+1\mid t+1}\)</span> and <span class="math inline">\(P_{t+1\mid t+1}\)</span>, we first <em>predict</em></p>
<p><span id="eq-kalman-predict"><span class="math display">\[\begin{split}
\bv m_{t+1\mid t} &amp;= M \bv m_{t\mid t},\\
P_{t+1\mid t} &amp;= M P_{t\mid t} M^\intercal + \Sigma_\eta,
\end{split}
\tag{11}\]</span></span></p>
<p>then we add our new information, <em>update</em>, with <span class="math inline">\(z_{t}\)</span>;</p>
<p><span id="eq-kalman-update"><span class="math display">\[\begin{split}
\bv m_{t+1\mid t+1} &amp;= \bv m_{t+1\mid t} + K_{t+1} \bv e_{t+1}\\
P_{t+1\mid t+1} &amp;= [I- K_{t+1}\Phi_{t+1}]P_{t+1\mid t}
\end{split}
\tag{12}\]</span></span></p>
<p>where <span class="math inline">\(K_{t+1}\)</span> is the <em>Kalman gain</em>;</p>
<p><span class="math display">\[\begin{split}
K_{t+1} = P_{t+1\mid t}\Phi_{t+1}^\intercal [\Phi_{t+1} P_{t+1\mid t} \Phi_{t+1}^\intercal + \Sigma_\epsilon]^{-1}, \quad t=0,\dots,T-1,
\end{split}
\]</span></p>
<p>and <span class="math inline">\(\bv e_{t+1}\)</span> are the <em>prediction errors</em></p>
<p><span class="math display">\[\begin{split}
\bv e_{t+1} = \tilde{\bv z}_{t+1}-\Phi_{t+1} \bv m_{t+1\mid t}, \quad t=1,\dots,T.
\end{split}
\]</span></p>
<p>Starting with <span class="math inline">\(m_0\)</span> and <span class="math inline">\(P_0\)</span>, we can then iteratively move across the data to eventually compute <span class="math inline">\(m_{T\mid T}\)</span> and <span class="math inline">\(P_{T\mid T}\)</span>.</p>
<p>Assuming Gaussian all random variables here are Gaussian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of <em>linear</em> operators.</p>
<p>We can compute the marginal data likelihood alongside the Kalman filter using the prediction errors <span class="math inline">\(\bv e_t\)</span>. These, under the assumptions we have made about <span class="math inline">\(\bv \eta_t\)</span> and <span class="math inline">\(\bv\epsilon_t\)</span> being normal, are also normal with zero mean and variance</p>
<p><span id="eq-predvar"><span class="math display">\[\begin{split}
\mathbb V\mathrm{ar}[\bv e_t]=\Sigma_t= \Phi_{t} P_{t\mid t-1} \Phi_{t}^\intercal + \Sigma_\epsilon.
\end{split}
\tag{13}\]</span></span></p>
<p>Therefore, the log-likelihood at each time is</p>
<p><span class="math display">\[\begin{split}
\mathcal L(Z\mid\bv\theta) = -\frac12\sum \log\det(\Sigma_t(\bv\theta)) - \frac12 \sum\bv e_t(\bv\theta)^\intercal\Sigma_{t}(\bv\theta)^{-1} \bv e_t(\bv\theta) - \frac{n_{t}}{2}\log(2*\pi).
\end{split}
\]</span></p>
<p>Summing these across time, we get the log likelihood for all the data.</p>
<p>A simplified example of the Kalman filter function, written to be JAX compatible, used in the package is this;</p>
<div id="a022ef2f" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kalman_filter(m_, P_0, M, PHI_obs, Sigma_eta, Sigma_eps, ztildes):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    nbasis <span class="op">=</span> m_0.shape[<span class="dv">0</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    nobs <span class="op">=</span> ztildes.shape[<span class="dv">0</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(carry, z_t):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        m_tt, P_tt, _, _, ll, _ <span class="op">=</span> carry</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predict</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        m_pred <span class="op">=</span> M <span class="op">@</span> m_tt</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        P_pred <span class="op">=</span> M <span class="op">@</span> P_tt <span class="op">@</span> M.T <span class="op">+</span> Sigma_eta</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prediction Errors</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        eps_t <span class="op">=</span> z_t <span class="op">-</span> PHI_obs <span class="op">@</span> m_pred</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        Sigma_t <span class="op">=</span> PHI_obs <span class="op">@</span> P_pred <span class="op">@</span> PHI_obs.T <span class="op">+</span> Sigma_eps</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Kalman Gain</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        K_t <span class="op">=</span> (jnp.linalg.solve(Sigma_t, PHI_obs)<span class="op">@</span> P_pred.T).T</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        m_up <span class="op">=</span> m_pred <span class="op">+</span> K_t <span class="op">@</span> eps_t</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        P_up <span class="op">=</span> (jnp.eye(nbasis) <span class="op">-</span> K_t <span class="op">@</span> PHI_obs) <span class="op">@</span> P_pred</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># likelihood of epsilon, using cholesky decomposition</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        ll_new <span class="op">=</span> ll <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> n <span class="op">*</span> jnp.log(<span class="dv">2</span><span class="op">*</span>jnp.pi) <span class="op">-</span> <span class="op">\</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.5</span> <span class="op">*</span> jnp.log(jnp.linalg.det(Sigma_t)) <span class="op">-\</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.5</span> <span class="op">*</span> e.T <span class="op">@</span> jnp.linalg.solve(Sigma_t, e)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (m_up, P_up, m_pred, P_pred, ll_new, K_t), (m_up, P_up, m_pred, P_pred, ll_new, K_t,)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    carry, seq <span class="op">=</span> jl.scan(</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        step,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        (m_0, P_0, m_0, P_0, <span class="dv">0</span>, jnp.zeros((nbasis, nobs))),</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        ztildes.T,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (carry[<span class="dv">4</span>], seq[<span class="dv">0</span>], seq[<span class="dv">1</span>], seq[<span class="dv">2</span>][<span class="dv">1</span>:], seq[<span class="dv">3</span>][<span class="dv">1</span>:], seq[<span class="dv">5</span>][<span class="dv">1</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For the documentation of the method provided by the package, see <a href="../reference/kalman_filter.html"><code>filter_smoother_functions.kalman_filter</code></a>.</p>
</section>
<section id="the-information-filter" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="the-information-filter"><span class="header-section-number">4.2</span> The Information Filter</h2>
<p>In some computational scenarios, it is beneficial to work with vectors of consistent dimension. In Python JAX, the efficient <code>scan</code> method works only with such arrays; JAX has no support for jagged arrays, and traditional for loops will likely lead to long compile times when jit-compiled. Although there are some tools in JAX to get around this problem (namely the <code>jax.tree</code> functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it’s core, a scan-type operation (scanning over the data), this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.</p>
<p>But it is possible to re-write the Kalman filter in a way which is compatible with this kind of data. The ‘information filter’ (sometimes called inverse Kalman filter or other names) involves transforming the data into its ‘information form’, which will always have consistent dimension, allowing us to avoid jagged scans.</p>
<p>The information filter is simply the Kalman filter re-written to use the Gaussian distribution’s canonical parameters <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, those being the information vector and the information matrix. If a Gaussian distribution has mean <span class="math inline">\(\bv\mu\)</span> and variance matrix <span class="math inline">\(\Sigma\)</span>, then the corresponding <em>information vector</em> and <em>information matrix</em> is <span class="math inline">\(\nu = \Sigma^{-1}\mu\)</span> and <span class="math inline">\(Q = \Sigma^{-1}\)</span>, correspondingly.</p>
<div id="thm-information_filter" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> The Kalman filter can be rewritten in information form as follows <span class="citation" data-cites="khan2005matrix">(for example, <a href="#ref-khan2005matrix" role="doc-biblioref">Khan 2005</a>)</span>. Write</p>
<p><span class="math display">\[\begin{split}
Q_{i\mid j} &amp;= P_{i\mid j}^{-1}\\
\bv\nu_{i\mid j} &amp;= Q_{i\mid j} \bv m_{i\mid j}
\end{split}
\]</span></p>
<p>and transform the observations into their ‘information form’, for <span class="math inline">\(t=1,\dots, T\)</span></p>
<p><span id="eq-obsinfo"><span class="math display">\[\begin{split}
I_{t} = \Phi_{t}^{\intercal} \Sigma_{\epsilon}^{-1}\Phi_{t},\\
i_{t} = \Phi_{t}^{\intercal} \Sigma_{\epsilon}^{-1} \bv z_{t}.
\end{split}
\tag{14}\]</span></span></p>
<p>The prediction step now becomes</p>
<p><span id="eq-infpred"><span class="math display">\[\begin{split}
\bv\nu_{t+1\mid t} &amp;= (I-J_t) M^{-1}\bv\nu_{t\mid t}\\
Q_{t+1\mid t} &amp;= (I-J_t) S_{t}
\end{split}
\tag{15}\]</span></span></p>
<p>where <span class="math inline">\(S_t = M^{-\intercal} Q_{t\mid t} M^{-1}\)</span> and <span class="math inline">\(J_t = S_t [S_{t}+\Sigma_{\eta}^{-1}]^{-1}\)</span>.</p>
<p>Updating is now as simple as adding the information-form observations;</p>
<p><span id="eq-infupd"><span class="math display">\[\begin{split}
  \bv\nu_{t+1\mid t+1} &amp;= \bv\nu_{t+1\mid t} + i_{t+1}\\
  Q_{t+1\mid t+1} &amp;= Q_{t+1\mid t} + I_{t+1}.
\end{split}
\tag{16}\]</span></span></p>
</div>
<p>Proof in Appendix (<a href="#sec-app1" class="quarto-xref">Section&nbsp;7.2</a>.)</p>
<p>We can see that the information form of the observations (<a href="#eq-obsinfo" class="quarto-xref">Equation&nbsp;14</a>) will always have the same dimension <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. For our purposes, this means that <code>jax.lax.scan</code> will work after we ‘informationify’ the data, which can be done using <code>jax.tree.map</code>. This is implemented in the functions <code>information_filter</code> and <code>information_filter_indep</code> (for uncorrelated errors).</p>
<p>There are other often cited advantages to filtering in this form. It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with <span class="math inline">\([S_t + \Sigma_\eta]^{-1}\)</span> in the process dimension instead of <span class="math inline">\([\Phi_t P_{t+1\mid t} \Phi_t^\intercal + \Sigma_\epsilon]^{-1}\)</span> in the observation dimension) <span class="citation" data-cites="assimakis2012information">(<a href="#ref-assimakis2012information" role="doc-biblioref">Assimakis, Adam, and Douladiris 2012</a>)</span>.</p>
<p>The other often mentioned advantage is the ability to use a flat prior for <span class="math inline">\(\alpha_0\)</span>; that is, we can set <span class="math inline">\(Q_0\)</span> as the zero matrix, without worrying about an infinite variance matrix. While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see <a href="#sec-vagueprior" class="quarto-xref">Section&nbsp;7.3</a>.</p>
<p>As with the Kalman filter, it is also possible to get the data likelihood in-line as well. Again, we would like to stick with things in the state dimension, so working directly with the prediction errors <span class="math inline">\(\bv e_t\)</span> should be avoided. Luckily, by multiplying the errors by <span class="math inline">\(\Phi_t^\intercal \Sigma_\epsilon^{-1}\)</span>, we can define the ‘information errors’ <span class="math inline">\(\bv \iota_t\)</span>;</p>
<p><span class="math display">\[\begin{split}
  \bv \iota_t &amp;= \Phi_t^\intercal \Sigma_\epsilon^{-1} \bv e_t = \Phi_t^\intercal \Sigma_\epsilon^{-1} \tilde{\bv z}_t -\Phi_t^\intercal \Sigma_\epsilon^{-1}\Phi_t m_{t\mid t-1}\\
  &amp;= i_t - I_tQ_{t\mid t-1}^{-1}\bv \nu_{t\mid t-1}.
\end{split}
\]</span></p>
<p>The variance of this quantity is also easy to find;</p>
<p><span class="math display">\[\begin{split}
  \var[\bv \iota_t] &amp;= \Phi_t^\intercal \Sigma_\epsilon^{-1}\var[\bv e_t]\Sigma_\epsilon^{-1}\Phi_t\\
  &amp;= \Phi_t^\intercal \Sigma_\epsilon^{-1} [\Phi_{t} P_{t\mid t-1} \Phi_{t}^\intercal + \Sigma_\epsilon] \Sigma_\epsilon^{-1}\Phi_t\\
  &amp;= \Phi_t^\intercal \Sigma_\epsilon^{-1}\Phi_{t} Q_{t\mid t-1}^{-1} \Phi_{t}^\intercal \Sigma_\epsilon^{-1}\Phi_t \Phi_t^\intercal \Sigma_\epsilon^{-1} \Phi_t\\
  &amp;= I_t Q_{t\mid t-1}^{-1} I_t^\intercal + I_t =: \Sigma_{\iota, t}.
\end{split}
\]</span></p>
<p>Noting that <span class="math inline">\(\bv \iota\)</span> clearly still has mean zero, this allows us once again to compute the log likelihood, this time through <span class="math inline">\(\bv\iota\)</span></p>
<p><span class="math display">\[\begin{split}
\mathcal L(z_t\mid\bv\theta) = -\frac12\sum \log\det(\Sigma_{\iota, t}(\bv\theta)) - \frac12 \sum\bv \iota_t(\bv\theta)^\intercal\Sigma_{\iota, t}(\bv\theta)^{-1} \bv \iota_t(\bv\theta) - \frac{r}{2}\log(2*\pi).
\end{split}
\]</span></p>
</section>
<section id="the-square-root-filters" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="the-square-root-filters"><span class="header-section-number">4.3</span> The Square-Root filters</h2>
<p>In certain high-dimensional cases, the Kalman filter (and, indeed, the information filter) can encounter numerical stability issues. For example, in the predict step of the standard Kalman filter, note the update step for the variance matrix</p>
<p><span class="math display">\[\begin{split}
P_{t+1\mid t+1} &amp;= [I- K_{t+1}\Phi_{t+1}]P_{t+1\mid t}.
\end{split}
\]</span></p>
<p>Somewhat masked within this equation is two (often very small) variance matrices subtracted from eachother. While analytically, the result is still guarenteed to be positive (semi-)definite, when done in floating point arithmatic (especially in single-precision or lower), the result can often be numerically indefinate. When the variances are very low (as they often become in these Kalman filters), the eigenvalues come out very close to zero and can tick over to becoming negative erroneously. This can lead to definiteness issues with all the other variance matrices, most crucially <span class="math inline">\(\Sigma_t\)</span> <a href="#eq-predvar" class="quarto-xref">Equation&nbsp;13</a>. When this happens, computation of the likelihood likely fails (certainly when such a computation involvoes a Cholesky decomposition). Even if such is rare to happen with 64-bit precision, modern GPU hardware tends to be much more efficient with Single (32-bit) precision, so it may still be desirable to increase stability if it permits using a lower precision. The Square-root filter and the SVD filter are such algorithms.</p>
<section id="the-square-root-kalman-filter" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="the-square-root-kalman-filter"><span class="header-section-number">4.3.1</span> The Square-root Kalman filter</h3>
<p>The square-root Kalman filter has it’s origins soon after the standard Kalman filter gained popularity <span class="citation" data-cites="kaminski1971discrete">(<a href="#ref-kaminski1971discrete" role="doc-biblioref">Kaminski, Bryson, and Schmidt 1971</a>)</span>. Of course, computational and memory constraints necessitated stable and memory-efficient approaches, while today the standard Kalman filter (and, more recently, it’s parallel counterpart, to be covered in section [TBD]) usually suffice.</p>
<p>As its name suggests, this variant involves carriyng through the square roots of variances <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> instead of the variances themselves. This leads to, at least in some sense, an increased precision, and we can always guarentee that, at least analytically, the square of these square roots (the variances) are positive (semi-)definite.</p>
<p>While the square root filter has been known for a long time (even used during NASA’s Apollo program), more recently, <span class="citation" data-cites="tracy2022square">(<a href="#ref-tracy2022square" role="doc-biblioref">Tracy 2022</a>)</span> wrote it neatly in terms of the QR decomposition, and this is what we base the presentation on here.</p>
<p>The key observation used for this filter is that if we have the sum of two equations where a square root is known for both, it can be written</p>
<p><span class="math display">\[\begin{split}
  X + Y &amp;= A^\intercal A + B^\intercal B\\
  &amp;= \left[A^\intercal\ B^\intercal\right] \left[\begin{matrix}A\\B\end{matrix}\right]
\end{split}
\]</span></p>
<p>Taking the QR decomposition of the vertical block yields QR, and since <span class="math inline">\((QR)^\intercal\ (QR) = R^\intercal Q^\intercal Q R = R^\intercal R\)</span>, so <span class="math inline">\(R\)</span> is a square root of <span class="math inline">\(X+Y\)</span>. This motivates the following ‘QR operator’</p>
<p><span class="math display">\[\begin{split}
\mathrm qr_R(A, B),
\end{split}
\]</span></p>
<p>as the matrix <span class="math inline">\(R\)</span> in the QR decomposition of the block matrix</p>
<p><span class="math display">\[\begin{split}
\left[\begin{matrix}A\\B\end{matrix}\right].
\end{split}
\]</span></p>
<p>Beginning with the Cholesky decomposition of the initial variances, <span class="math inline">\(P_0 = U_0^\intercal U_0\)</span>, <span class="math inline">\(\Sigma_{\eta} = U_{\eta}^\intercal U_{\eta}\)</span> and <span class="math inline">\(\Sigma_\epsilon = U_{\epsilon}^\intercal U_{\epsilon}\)</span> the predict step for the variance becomes</p>
<p><span class="math display">\[\begin{split}
U_{t+1\mid t} = \sqrt{P_{t+1\mid t}} = \mathrm{qr}_R(U_{t\mid t} M^\intercal, U_{\eta}),
\end{split}
\]</span></p>
<p>with the step for the means being the same as before (<a href="#eq-kalman-predict" class="quarto-xref">Equation&nbsp;11</a>). The prediction errors, prediction variance and Kalman gain are now</p>
<p><span class="math display">\[\begin{split}
  \bv e_{t+1} &amp;= \tilde{\bv z}_t - \Phi_{t+1} \bv m_{t+1\mid t},\\
  \Sigma_{t+1} &amp;= \Phi_{t+1} P_{t+1\mid t} \Phi_{t+1}^\intercal + \Sigma_\epsilon,\\
  \sqrt{\Sigma_{t+1}} &amp;= U_{e, t+1} = \mathrm{qr}_R(\Phi_{t+1} U_{t+1\mid t}, U_\epsilon),\\
  K_{t+1} &amp;= P_{t+1\mid t} \Phi_{t+1}^\intercal \Sigma_{t+1}^{-1} = U_{t+1\mid t}^\intercal U_{t+1\mid t} \Phi_{t+1}^\intercal (U_{e, t+1}^\intercal U_{e, t+1})^{-1}\\
  &amp;= (U_{e, t+1}^{-1}U_{e, t+1}^{-\intercal}\Phi_{t+1}U_{t+1\mid t}^\intercal U_{t+1\mid t})^\intercal
\end{split}
\]</span></p>
<p>where the last equation for the Kalman gain can easily be solved with a computationally efficient triangular solve.</p>
<p>Finally, the update step for the mean is simply</p>
<p><span class="math display">\[\begin{split}
  \bv m_{t+1\mid t+1} = \bv m_{t \mid t+1} + K_{t+1} {\bv e_{t+1}}.
\end{split}
\]</span></p>
<p>However, for the update we use the so-called Joseph stabilised form (sometimes used in the derivation of the Kalman filter)</p>
<p><span class="math display">\[\begin{split}
  P_{t+1\mid t+1} &amp;= \mathbb C\mathrm{ov}[\bv \alpha_t - \bv m_{t+1\mid t+1}]\\
             &amp;= \mathbb C\mathrm{ov}[\bv \alpha_t - \bv m_{t \mid t+1} - K_{t+1} (\tilde{\bv z}_{t+1} - \Phi_{t+1} \bv m_{t+1\mid t})]\\
             &amp;= \mathbb C\mathrm{ov}[\bv \alpha_t - \bv m_{t \mid t+1} - K_{t+1} (\Phi_{t+1} \bv m_{t+1} + \bv \epsilon_{t+1} - \Phi_{t+1}\bv m_{t+1\mid t})]\\
             &amp;= \mathbb C\mathrm{ov}[(I - K_{t+1} \Phi_{t+1})(\bv \alpha_t - \bv m_{t+1 \mid t}) - \bv \epsilon_{t+1}]\\
             &amp;= (I - K_{t+1} \Phi_{t+1}) \mathbb C\mathrm{ov}[\bv \alpha_t - \bv m_{t+1 \mid t}](I - K_{t+1} \Phi_{t+1})^\intercal + \mathbb C\mathrm{ov}[\bv \epsilon_{t+1}]\\
             &amp;= (I - K_{t+1} \Phi_{t+1}) P_{t+1\mid t}(I - K_{t+1} \Phi_{t+1})^\intercal + \Sigma_{\epsilon}
\end{split}
\]</span></p>
<p>which is often simplified further to <a href="#eq-kalman-update" class="quarto-xref">Equation&nbsp;12</a>, but as discussed that involves negation of two square root matrices; this form is more complicated and involves more matrix computation, but guarentees that the result will be positive (semi-)definite. Furthermore, this is also in a form that allows us to easily find the square root with the QR trick;</p>
<p><span class="math display">\[\begin{split}
U_{t+1\mid t+1} = \sqrt{P_{t+1\mid t+1}} = \mathrm{qr}_R(U_{t+1\mid t}(I - K_{t+1} \Phi_{t+1})^{\intercal}, U_\epsilon).
\end{split}
\]</span></p>
<p>Of course, from here, we can similarily easily compute the data likelihood using <span class="math inline">\(U_{e,t+1}\)</span>, using standard techniques; the multivariate normal likelihood is usually computed using the cholesky decomposition of the variance matrix anyway. The result is an algorithm which is of a higher order than the standard Kalman filter, but the stability is often worth the comprimise. Once jit-compiled, the function <code>sqrt_filter_indep</code> on a moderately sized IDEM (on a discrete GPU) on 64-bit precision <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> takes approximately 23.5ms, compared to <code>kalman_filter_indep</code> taking approximately 7.8ms, achieving similar log-likelihoods (whith some difference due to precision). However, running the code in 32-bit causes the Kalman filter likelihood computation to fail, the square-root filter succeeds at a time of 7.0ms.</p>
</section>
<section id="square-root-information-filter" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="square-root-information-filter"><span class="header-section-number">4.3.2</span> Square-root Information filter</h3>
<p>Very similarily, we can write the information filter using the square roots of the information matrices. We will label roots of ‘information-type’ matrices with <span class="math inline">\(R\)</span>, and ‘variance-type’ matrices (their inverse) with <span class="math inline">\(U\)</span>.</p>
<p>We now carry the data’s information matrix’s (<a href="#eq-obsinfo" class="quarto-xref">Equation&nbsp;14</a>) square root as well, <span class="math inline">\(R_t^{(I)} = \sqrt(\Phi_t^\intercal\Sigma_\epsilon^{-1} \Phi_t)\)</span>, with the same observation vector.</p>
<p>So, once again, beginning with the lower-triangular cholesky decomposition <span class="math inline">\(Q_0 = R^\intercal R\)</span>, and the upper-triangular <span class="math inline">\(\Sigma_{\eta} = U_{\eta}^\intercal U_{\eta}\)</span> and <span class="math inline">\(\Sigma_\epsilon = U_{\epsilon}^\intercal U_{\epsilon}\)</span>.</p>
<p>So, to predict step for the information matrix (<a href="#eq-infpred" class="quarto-xref">Equation&nbsp;15</a>) becomes</p>
<p><span id="eq-sqrtinfpred"><span class="math display">\[\begin{split}
  Q_{t+1\mid t} &amp;= (M Q_{t\mid t}^{-1} M^\intercal + \Sigma_\eta)^{-1}\\
  &amp;= (M R_{t\mid t}^{-1}R_{t\mid t}^{-\intercal} M^\intercal + U_\eta^\intercal U_\eta)^{-1}\\
  &amp;= \left[(M R_{t\mid t}^{-1}, U_\eta^\intercal)\left(\begin{matrix}R_{t\mid t}^{-\intercal} M^\intercal\\U_\eta\end{matrix}\right)\right]^{-1}\\
  R_{t+1\mid t}^{-1} &amp;= \mathrm{qr}_R(R_{t\mid t}^{-\intercal} M^\intercal, U_\eta)
\end{split}
\tag{17}\]</span></span></p>
<p>This must now be explicitly inverted, which isn’t a big problem since it is upper-triangular.</p>
<p>The update on the information vector is now</p>
<p><span id="eq-sqrtinfopred2"><span class="math display">\[\begin{split}
\bv\nu_{t+1\mid t} &amp;= Q_{t+1\mid t} M Q_{t\mid t}^{-1} \bv \nu_{t\mid t}\\
&amp;= R_{t+1\mid t}^\intercal R_{t+1\mid t} M R_{t\mid t}^{-1}R_{t\mid t}^{-\intercal} \bv \nu_{t\mid t},
\end{split}
\tag{18}\]</span></span></p>
<p>which can be done, as in the square-root Kalman filter’s Kalman gain computation, using forward-solves.</p>
<p>Now the update step is</p>
<p><span id="eq-sqrtinfup"><span class="math display">\[\begin{split}
  \bv\nu_{t+1\mid t+1} &amp;= \bv\nu_{t+1\mid t} + i_{t+1}\\
  Q_{t+1\mid t+1} &amp;= Q_{t+1\mid t} + I_{t+1}\\
  &amp;= R_{t+1\mid t}^\intercal R_{t+1\mid t} + R^{(I)\intercal}_{t+1}R^{(I)}_{t+1}\\
  R_{t+1\mid t+1}  &amp;= \mathrm{qr}_R(R_{t+1\mid t}, R^{(I)}_{t+1}).
\end{split}
\tag{19}\]</span></span></p>
</section>
</section>
<section id="smoothing" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="smoothing"><span class="header-section-number">4.4</span> Smoothing</h2>
<p>Beyond the filtering, another task is <em>smoothing</em>. That is, filters estimate <span class="math inline">\(\bv m_{T\mid T}\)</span> and <span class="math inline">\(P_{T\mid T}\)</span>, but there is use for estimating <span class="math inline">\(\bv m_{t\mid T}\)</span> and <span class="math inline">\(P_{t\mid T}\)</span> for all <span class="math inline">\(t=0,\dots, T\)</span>.</p>
<p>We simply work backwards from <span class="math inline">\(\bv m_{T\mid T}\)</span> and <span class="math inline">\(P_{T\mid T}\)</span> values using what is known as the <em>Rauch-Tung-Striebel (RTS) smoother</em>;</p>
<p><span id="eq-kalmansmooth"><span class="math display">\[\begin{split}
\bv m_{t-1\mid T} &amp;= \bv m_{t-1\mid t-1} + J_{t-1}(\bv m_{t\mid T} - \bv m_{t\mid t-1}),\\
P_{t-1\mid T} &amp;= P_{t-1\mid t-1} + J_{t-1}(P_{t\mid T} - P_{t\mid t-1})J_{t-1}^\intercal,
\end{split}
\tag{20}\]</span></span></p>
<p>where,</p>
<p><span class="math display">\[\begin{split}
J_{t-1} = P_{t-1\mid t-1}M^\intercal[P_{t\mid t-1}]^{-1}.
\end{split}
\]</span></p>
<p>We can clearly see, then, that it is crucial to keep the values in <a href="#eq-kalman-predict" class="quarto-xref">Equation&nbsp;11</a>.</p>
<p>We can then also compute the lag-one cross-covariance matrices <span class="math inline">\(P_{t,t-1\mid T}\)</span> using the <em>Lag-One Covariance Smoother</em>. This will b useful, for example, in the expectation-maximisation algorithm later. From</p>
<p><span class="math display">\[\begin{split}
P_{T,T-1\mid T} = (I - K_T\Phi_{T}) MP_{T-1\mid T-1},
\end{split}
\]</span></p>
<p>we can compute the lag-one covariances</p>
<p><span id="eq-lag1smooth"><span class="math display">\[\begin{split}
P_{t, t-1\mid T} = P_{t\mid t}J_{t-1}^\intercal + J_{t}[P_{t+1,t\mid T} - MP_{t-1\mid t-1}]J_{t-1}^\intercal
\end{split}
\tag{21}\]</span></span></p>
<p>These values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later.</p>
</section>
</section>
<section id="em-algorithm-needs-a-lot-of-work-probably-ignore-for-now" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> EM Algorithm (NEEDS A LOT OF WORK, PROBABLY IGNORE FOR NOW)</h1>
<p>Instead of the marginal data likelihood, we may instead want to work with the ‘full’ likelihood, including the unobserved process, <span class="math inline">\(l(\bv z(1),\dots, \bv z(T), \bv Y(1), \dots, \bv Y(T)\mid \bv\theta)\)</span>, or, equivalently, <span class="math inline">\(l(\bv z(1),\dots, \bv z(t), \bv \alpha(1), \dots, \bv\alpha(T)\mid \bv\theta)\)</span>. This is difficult to maximise directly, but can be done with the EM algorithm, consisting of two steps, which can be shown to always increase the full likelihood.</p>
<p>Firstly, the E step is to find the function</p>
<p><span id="eq-Qdef"><span class="math display">\[\begin{split}
\mathcal Q(\bv \theta; \bv \theta') = \mathbb E_{\bv Z(t)\sim p(Z \mid \bv\alpha(t),\bv\theta)}[\log p_{\bv\theta}(Z^{(T)}, A^{(T)})\mid Z^{(T)}],
\end{split}
\tag{22}\]</span></span></p>
<p>where <span class="math inline">\(Z^{(T)} = \{\bv z_t\}_{t=0,\dots,T}\)</span>, <span class="math inline">\(A^{(T)} = \{\bv \alpha_t\}_{t=0,\dots,T}\)</span> and <span class="math inline">\(A^{(T-1)} = \{\bv \alpha_t\}_{t=0,\dots,T-1}\)</span>. This approximates <span class="math inline">\(\log p_\theta(Z^{(T)}, A^{(T)})\)</span>.</p>
<div id="prp-EMQ" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1</strong></span> We have <span style="color: red;">[NOTE: This may well be wrong in places…]</span></p>
<p><span id="eq-Q"><span class="math display">\[\begin{split}
-2\mathcal Q(\bv\theta;\bv\theta') &amp;= \mathbb E_{Z^{(T)}\sim p(Z \mid A^{(T)},\bv\theta')}[\log p_{\bv\theta}(Z^{(T)}, A^{(T)}\mid Z^{(T)} = z^{(T)})]\\
&amp;\eqc \sigma_\epsilon^2 [\sum_{t=0}^{T}\bv z_t^{\intercal}z_t - 2\Phi_t(\sum_{t=1}^{T} \bv z_t^\intercal \bv m_{t\mid T}) - 2(\sum_{t=0}^{T} \bv z_t^T)X_t\bv\beta\\
&amp;\quad\quad\quad +\Phi_t^\intercal(\sum_{t=0}^{T}\mathrm{tr}\{P_{t\mid T} - \bv m_{t\mid T}\bv m_{t\mid T}^{\intercal}\})\Phi_t + 2X_t\bv\beta\Phi_t(\sum_{t=0}^{T}\bv m_{t\mid T}) + (\sum_{t=1}^{T}X_t^\intercal \bv\beta^{\intercal}\bv\beta X_t)]\\
&amp;\quad + \mathrm{tr}\{\Sigma_\eta^{-1}[(\sum_{t=1}^{T}P_{t\mid T} - m_{t\mid T}) - 2M(\sum_{t=1}^{T}P_{t,t-1\mid T} - \bv m_{t-1,T}\bv m_{t\mid T}^{\intercal})\\
&amp;\quad\quad\quad\quad\quad + M(\sum_{t=1}^{T}P_{t-1\mid T} - \bv m_{t-1\mid T}\bv m_{t-1\mid T}^{\intercal})M^\intercal]\}\\
&amp;\quad + \mathrm{tr}\{\Sigma_0^{-1}[P_{0\mid T} - m_{0\mid T}m_{0\mid T}^{\intercal} - 2\bv m_{0\mid T}\bv m_0 + \bv m_0\bv m_0^\intercal]\}\\
&amp;\quad + \log(\det(\sigma_\epsilon^{2T}\Sigma_\eta^{T+1}\Sigma_0))
\end{split}
\tag{23}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>See appendix.</p>
</div>
<p>In the EM algorithm, we maximise the full likelihood by changing <span class="math inline">\(\bv \theta\)</span> in order to increase (<a href="#eq-Q" class="quarto-xref">Equation&nbsp;23</a>), which can be shown to guarantee that the Likelihood <span class="math inline">\(L(\bv \theta)\)</span> also increases. The idea is then that repeatedly alternating between adjusting <span class="math inline">\(\bv \theta\)</span> to increase <a href="#eq-Q" class="quarto-xref">Equation&nbsp;23</a>, and then doing the filters and smoothers to obtain new values for <span class="math inline">\(\bv m_{t\mid T}\)</span>, <span class="math inline">\(P_{t\mid T}\)</span>, and <span class="math inline">\(P_{t,t-1\mid T}\)</span>.</p>
</section>
<section id="algorithm-for-maximum-complete-data-likelihood-estimation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Algorithm for Maximum Complete-data Likelihood estimation</h1>
<p>Overall, our algorithm for Maximum Likelihood estimation is:</p>
<ol type="1">
<li>Set <span class="math inline">\(i=0\)</span> and take an initial guess for the parameters we are considering, <span class="math inline">\(\bv\theta_0=\bv\theta_i\)</span></li>
<li>Starting from <span class="math inline">\(\bv m_{0\mid 0}=\bv m_0, P_{0\mid0}=\Sigma_0\)</span>, run the <strong>Kalman Filter</strong> to get <span class="math inline">\(\bv m_{t\mid t}\)</span>, <span class="math inline">\(P_{t\mid t}\)</span>, and <span class="math inline">\(K_t\)</span> for all <span class="math inline">\(t\)</span> <a href="#eq-kalman-update" class="quarto-xref">Equation&nbsp;12</a>,</li>
<li>Starting from <span class="math inline">\(\bv m_{T\mid T}, P_{T\mid T}\)</span>, run the <strong>Kalman Smoother</strong> to get <span class="math inline">\(\bv m_{t\mid T}\)</span>, <span class="math inline">\(P_{t\mid T}\)</span>, and <span class="math inline">\(J_t\)</span> for all <span class="math inline">\(t\)</span> (<a href="#eq-kalmansmooth" class="quarto-xref">Equation&nbsp;20</a>),</li>
<li>Starting from <span class="math inline">\(P_{T,T-1\mid T} = (I - K_nA_n) MP_{T-1\mid T-1}\)</span>, run the <strong>Lag-One Smoother</strong> to get <span class="math inline">\(\bv m_{t,t-1\mid T}\)</span> and <span class="math inline">\(P_{t,t-1\mid T}\)</span> for all <span class="math inline">\(t\)</span> <a href="#eq-lag1smooth" class="quarto-xref">Equation&nbsp;21</a>,</li>
<li>Use the above values to construct <span class="math inline">\(\mathcal Q(\bv\theta;\bv \theta')\)</span> in <a href="#eq-Q" class="quarto-xref">Equation&nbsp;23</a>,</li>
<li>Maximise the function <span class="math inline">\(\mathcal Q(\bv\theta;\bv \theta')\)</span> to get a new guess <span class="math inline">\(\bv \theta_{i+1}\)</span>, then return to step 2,</li>
<li>Stop once a certain criteria is met.</li>
</ol>
</section>



<div id="quarto-appendix" class="default"><section id="appendix" class="level1 appendix" data-number="7"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">7</span> Appendix</h2><div class="quarto-appendix-contents">

<section id="woodburys-identity" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="woodburys-identity"><span class="header-section-number">7.1</span> Woodbury’s identity</h2>
<p>The following two sections will make heavy use of the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury identity</a>.</p>
<div id="lem-woodbury" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2 (Woodbury’s Identity)</strong></span> We have, for conformable matrices <span class="math inline">\(A, U, C, V\)</span>,</p>
<p><span id="eq-woodbury"><span class="math display">\[\begin{split}
(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
\end{split}
\tag{24}\]</span></span></p>
<p>Additionally, we have the variant</p>
<p><span id="eq-woodbury2"><span class="math display">\[\begin{split}
(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.
\end{split}
\tag{25}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We only prove (<a href="#eq-woodbury2" class="quarto-xref">Equation&nbsp;25</a>), since various proofs of (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;24</a>) are well known (see, for example, the Wikipedia page).</p>
<p>Simply multipliying (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;24</a>) by <span class="math inline">\(CU\)</span>, <span class="citation" data-cites="khan2005matrix">(similar to <a href="#ref-khan2005matrix" role="doc-biblioref">Khan 2005</a>, although there is an error in their proof)</span></p>
<p><span class="math display">\[\begin{split}
(A+UCV)^{-1}UC &amp;= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\
&amp;= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\
&amp;= A^{-1}U(C^{-1}+VA^{-1}U)
\end{split}
\]</span></p>
<p>as needed. </p>
</div>
</section>
<section id="sec-app1" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="sec-app1"><span class="header-section-number">7.2</span> Proof of <a href="#thm-information_filter" class="quarto-xref">Theorem&nbsp;2</a></h2>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Firstly, for the prediction step, using <span class="math inline">\(S_t = M^{-\intercal}Q_{t\mid t}M^{-1}\)</span> and <span class="math inline">\(J_t = S_t(\Sigma_\eta^{-1} + S_t)^{-1}\)</span> and the identities <a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;24</a> and <a href="#eq-woodbury2" class="quarto-xref">Equation&nbsp;25</a>,</p>
<p><span class="math display">\[\begin{split}
  Q_{t+1\mid t} &amp;= P_{t+1\mid t}^{-1} = (MQ_{t\mid t}^{-1}M^\intercal + \Sigma_\eta)^{-1}\\
  &amp;= S_t - J_t S_t = (I-J_t)S_t,
\end{split}
\]</span></p>
<p>where we used <span class="math inline">\(A=MQ_{t\mid t}^{-1}M^\intercal\)</span>, <span class="math inline">\(C=\Sigma_\eta\)</span> and <span class="math inline">\(U=C=I\)</span> in <a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;24</a>. Thurthermore,</p>
<p><span class="math display">\[\begin{split}
  \bv \nu_{t+1\mid t} &amp;= Q_{t+1\mid t} \bv m_{t+1\mid t}\\
  &amp;= Q_{t+1\mid t} M Q_{t\mid t}^{-1} \bv \nu_{t\mid t} = Q_{t+1\mid t} (M Q_{t\mid t}^{-1}) \bv \nu_{t\mid t}\\
  &amp;= (I-J_t)M^{-\intercal}Q_{t\mid t}M^{-1} (M Q_{t\mid t}^{-1}) \bv \nu_{t\mid t}\\
  &amp;= (I-J_t)M^{-\intercal} \bv \nu_{t\mid t}.
\end{split}
\]</span></p>
<p>For the update step,</p>
<p><span class="math display">\[\begin{split}
  Q_{t+1\mid t+1} &amp;= P_{t+1\mid t+1}^{-1}\\
  &amp;= (Q_{t+1}^{-1} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}\Sigma_\epsilon\Phi_{t+1}^\intercal + \Sigma_\epsilon]^{-1}\Phi_{t+1}Q_{t+1\mid t}^{-1})^{-1}\\
  &amp;= ((Q_{t+1\mid t} + \Phi_{t+1}^\intercal\Sigma_\epsilon^{-1}\Phi_{t+1})^{-1})^{-1} = Q_{t+1\mid t} + \Phi_{t+1}^\intercal\Sigma_\epsilon^{-1}\Phi_{t+1}\\
  &amp;= Q_{t+1\mid t} + I_{t+1}.
\end{split}
\]</span></p>
<p>Then, writing <span class="math inline">\(\bv m_{t+1\mid t+1}\)</span> in terms of <span class="math inline">\(Q_{t+1\mid t}\)</span> and <span class="math inline">\(\bv \nu_{t+1\mid t}\)</span></p>
<p><span class="math display">\[\begin{split}
  \bv m_{t+1\mid t+1} &amp;= Q_{t+1\mid t}^{-1} \bv \nu_{t+1\mid t} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1} [\tilde{\bv z}_{t+1} - \Phi_{t+1}Q_{t+1\mid t}^{-1}\bv \nu_{t+1\mit t}]\\
  &amp;= (Q_{t+1\mid t}^{-1} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1}\Phi_{t+1}Q_{t+1\mid t}^{-1})\bv \nu_{t+1\mid t} \\
  &amp;\quad + Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1}\tilde{\bv z}_{t+1}\\
  &amp;= [Q_{t+1\mid t} + I_{t+1}]^{-1}\bv \nu_{t+1\mid t}\\
  &amp;\quad + [Q_{t+1\mid t} + I_{t+1}]^{-1}\Phi_{t+1}\Sigma_\epsilon^{-1}\tilde{\bv z}_{t+1},
\end{split}
\]</span></p>
<p>and now noting that <span class="math inline">\(\bv\nu_{t+1\mid t+1} = (Q_{t+1\mid t} + I_{t+1}) \bv m_{t+1\mid t+1}\)</span>, we complete the proof. </p>
</div>
</section>
<section id="sec-vagueprior" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="sec-vagueprior"><span class="header-section-number">7.3</span> Truly Vague Prior with the Kalman Filter</h2>
<p>It has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior <span class="math inline">\(Q_{0}=0\)</span>. While this is true, it is actually possible to do this in the Kalman filter by ‘skipping’ the first step (contrary to some sources, such as the Wikipedia page as of January 2025).</p>
<div id="thm-vagueprior" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3</strong></span> In the Kalman Filter (<a href="#sec-kalmanfilter" class="quarto-xref">Section&nbsp;4.1</a>), if we allow <span class="math inline">\(P_{0}^{-1} = 0\)</span>, effectively setting infinite variance, and assuming the propagator matrix <span class="math inline">\(M\)</span> is invertible, we have</p>
<p><span id="eq-kalmanvague"><span class="math display">\[\begin{split}
  \bv m_{1\mid1} &amp;= (\Phi_1^\intercal \Sigma_\epsilon^{-1} \Phi_1)^{-1} \Phi_1 \Sigma_\epsilon^{-1} \tilde{\bv z}_1,\\
  P_{1\mid1} &amp;= (\Phi_1^\intercal \Sigma_\epsilon^{-1} \Phi_1)^{-1}.
\end{split}
\tag{26}\]</span></span></p>
<p>Therefore, starting with these values then continuing the filter as normal, we can perform the Kalman filter with ‘infinite’ prior variance.</p>
<p><span style="color: red;">[NOTE: The requirement that M be invertible should be droppable, see the proof below]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Unsurprisingly, the proof is effectively equivalent to proving the information filter and setting <span class="math inline">\(Q_0 = P_0^{-1}=0\)</span>.</p>
<p>For the first predict step (<a href="#eq-kalman-predict" class="quarto-xref">Equation&nbsp;11</a>),</p>
<p><span class="math display">\[\begin{split}
  \bv m_{1\mid0} &amp;= M \bv m_0,\\
  P_{1\mid0} &amp;= M P_0 M^\intercal + \Sigma_\eta.
\end{split}
\]</span></p>
<p>By (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;24</a>),</p>
<p><span class="math display">\[\begin{split}
  P_{1\mid0}^{-1} &amp;= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} M (P_0^{-1} + M^\intercal \Sigma_\eta^{-1} M)^{-1}M^\intercal\Sigma_\eta^{-1}\\
  &amp;= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} M (M^\intercal \Sigma_\eta^{-1} M)^{-1}M^\intercal\Sigma_\eta^{-1}\\
  &amp;= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} = 0.
\end{split}
\]</span></p>
<p>So, moving to the update step (<a href="#eq-kalman-update" class="quarto-xref">Equation&nbsp;12</a>),</p>
<p><span class="math display">\[\begin{split}
  \bv m_{1\mid1} = M \bv m_0 + P_{1\mid0}\Phi_1 [\Phi_1 P_{1\mid0} \Phi_1^\intercal + \Sigma_\epsilon]^{-1}(\tilde{\bv{z}}_1 - \Phi M \bv m_0).\\
\end{split}
\]</span></p>
<p>Applying (<a href="#eq-woodbury2" class="quarto-xref">Equation&nbsp;25</a>) with <span class="math inline">\(A = P_{1\mid0}^{-1}, U=\Phi_1, V=\Phi_1^\intercal, C=\Sigma_\epsilon^{-1}\)</span>,</p>
<p><span class="math display">\[\begin{split}
  \bv m_{1\mid1} &amp;= M \bv m_0 + (P_{1\mid0}^{-1} + \Phi_1^\intercal\Sigma_\epsilon^{-1} \Phi_1)^{-1}\Phi_1^\intercal \Sigma_\epsilon^{-1}(\tilde{\bv{z}}_1 - \Phi_1 M\bv m_0)\\
  &amp;= M \bv m_0 + (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1} \tilde{\bv{z}}_1 - (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1M\bv m_0\\
  &amp;= (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1} \tilde{\bv{z}}_1.
\end{split}
\]</span></p>
<p>For the variance, we apply the (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;24</a>) with <span class="math inline">\(A = P_{1\mid0}^{-1}, U=\Phi_1^\intercal, V=\Phi_1, C=\Sigma_\epsilon^{-1}\)</span>,</p>
<p><span class="math display">\[\begin{split}
  P_{1\mid1} &amp;= (I - P_{1\mid0}\Phi_1^\intercal[\Sigma_\epsilon + \Phi_1^\intercal P_{1\mid0}\Phi_1]^{-1}\Phi_1)P_{1\mid0}\\
  &amp;= (P_{1\mid0}^{-1} + \Phi_1^\intercal \Sigma_\epsilon^{-1}\Phi_1)^{-1}\\
  &amp;= (\Phi_1^\intercal \Sigma_\epsilon^{-1}\Phi_1)^{-1},
\end{split}
\]</span></p>
<p>as needed. </p>
</div>
<p>It is worth noting that (<a href="#eq-kalmanvague" class="quarto-xref">Equation&nbsp;26</a>) seems to make a lot of sense; namely, we expect the estimate for <span class="math inline">\(\bv m_0\)</span> to look like a correlated least squares-type estimator like this.</p>



</section>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-assimakis2012information" class="csl-entry" role="listitem">
Assimakis, Nicholas, Maria Adam, and Anargyros Douladiris. 2012. <span>“Information Filter and Kalman Filter Comparison: Selection of the Faster Filter.”</span> In <em>Information Engineering</em>, 2:1–5. 1.
</div>
<div id="ref-cressie2015statistics" class="csl-entry" role="listitem">
Cressie, Noel, and Christopher K Wikle. 2015. <em>Statistics for Spatio-Temporal Data</em>. John Wiley &amp; Sons.
</div>
<div id="ref-dewar2008data" class="csl-entry" role="listitem">
Dewar, Michael, Kenneth Scerri, and Visakan Kadirkamanathan. 2008. <span>“Data-Driven Spatio-Temporal Modeling Using the Integro-Difference Equation.”</span> <em>IEEE Transactions on Signal Processing</em> 57 (1): 83–91.
</div>
<div id="ref-kaminski1971discrete" class="csl-entry" role="listitem">
Kaminski, Paul, Arthur Bryson, and Stanley Schmidt. 1971. <span>“Discrete Square Root Filtering: A Survey of Current Techniques.”</span> <em>IEEE Transactions on Automatic Control</em> 16 (6): 727–36.
</div>
<div id="ref-khan2005matrix" class="csl-entry" role="listitem">
Khan, Mohammad Emtiyaz. 2005. <span>“Matrix Inversion Lemma and Information Filter.”</span> <em>Honeywell Techonology Solutions Lab, Bangalore, India</em>.
</div>
<div id="ref-liu2022statistical" class="csl-entry" role="listitem">
Liu, Xiao, Kyongmin Yeo, and Siyuan Lu. 2022. <span>“Statistical Modeling for Spatio-Temporal Data from Stochastic Convection-Diffusion Processes.”</span> <em>Journal of the American Statistical Association</em> 117 (539): 1482–99.
</div>
<div id="ref-sarkka2020temporal" class="csl-entry" role="listitem">
Särkkä, Simo, and Ángel F Garcı́a-Fernández. 2020. <span>“Temporal Parallelization of Bayesian Smoothers.”</span> <em>IEEE Transactions on Automatic Control</em> 66 (1): 299–306.
</div>
<div id="ref-shumway2000time" class="csl-entry" role="listitem">
Shumway, Robert H, David S Stoffer, and David S Stoffer. 2000. <em>Time Series Analysis and Its Applications</em>. Vol. 3. Springer.
</div>
<div id="ref-tracy2022square" class="csl-entry" role="listitem">
Tracy, Kevin. 2022. <span>“A Square-Root Kalman Filter Using Only QR Decompositions.”</span> <em>arXiv Preprint arXiv:2208.06452</em>.
</div>
<div id="ref-wikle1999dimension" class="csl-entry" role="listitem">
Wikle, Christopher K, and Noel Cressie. 1999. <span>“A Dimension-Reduced Approach to Space-Time Kalman Filtering.”</span> <em>Biometrika</em> 86 (4): 815–29.
</div>
<div id="ref-wikle2019spatio" class="csl-entry" role="listitem">
Wikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019. <em>Spatio-Temporal Statistics with r</em>. CRC Press.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Historically, this has been abbreviated as IDE. However, with that abbreviation almost universally meaning ‘Integrated Development Environment’, here, we choose to include the ‘M’ in the abbreviation.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>at least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see <span class="citation" data-cites="liu2022statistical">Liu, Yeo, and Lu (<a href="#ref-liu2022statistical" role="doc-biblioref">2022</a>)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The bisquare functions, here, <span class="math inline">\(\phi_i(\bv s) = [1-\frac{\Vert \bv s - \bv c_i \Vert}{w_i}]^2 \cdot I(\Vert \bv s - \bv c_i \Vert &lt; w_i)\)</span> for <span class="math inline">\(i\)</span> ‘centroids’ or ‘knots’, <span class="math inline">\(\bv c_i\in \mathcal D\)</span>, each with ‘radius’ <span class="math inline">\(w_i\)</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>that is, the parameters of the Gaussian distribution in it’s exponential family form<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>that being the process dimension, previously labelled <span class="math inline">\(r\)</span>, the number of basis functions used in the expansion of the process<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>A matrix <span class="math inline">\(A\)</span> is said to be a ‘square root’ of a positive-definate matrix <span class="math inline">\(X\)</span> if <span class="math inline">\(A^\intercal A = X\)</span>. Note that these square roots are not unique, but can be ‘rotated’ by an arbitrary unitary matrix. The ‘canonical’ square root is the Cholesky factor, the unique upper (or occasionally lower) triangular square root. This can be found for arbitraty square roots by taking the QR decomposition (or RQ decomposition), which effectively computes the upper-triangular square root, <span class="math inline">\(R\)</span>, and the unitary transformation <span class="math inline">\(Q^\intercal\)</span> necessary to get there.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>which must be explicitely enabled in JAX<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>