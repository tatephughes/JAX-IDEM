<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Evan Tate Paterson Hughes">

<title>Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations – JAX-IDEM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d4be639c637f3db3c684c66cefad7e0c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#integro-difference-based-dynamics" id="toc-integro-difference-based-dynamics" class="nav-link active" data-scroll-target="#integro-difference-based-dynamics"><span class="header-section-number">1</span> Integro-difference Based Dynamics</a></li>
  <li><a href="#process-decomposition" id="toc-process-decomposition" class="nav-link" data-scroll-target="#process-decomposition"><span class="header-section-number">2</span> Process Decomposition</a>
  <ul class="collapse">
  <li><a href="#process-noise" id="toc-process-noise" class="nav-link" data-scroll-target="#process-noise"><span class="header-section-number">2.1</span> Process Noise</a></li>
  <li><a href="#kernel-decomposition" id="toc-kernel-decomposition" class="nav-link" data-scroll-target="#kernel-decomposition"><span class="header-section-number">2.2</span> Kernel Decomposition</a></li>
  <li><a href="#idem-as-a-linear-dynamical-system" id="toc-idem-as-a-linear-dynamical-system" class="nav-link" data-scroll-target="#idem-as-a-linear-dynamical-system"><span class="header-section-number">2.3</span> IDEM as a linear dynamical system</a></li>
  </ul></li>
  <li><a href="#filtering-forecasting-and-maximum-likelihood-estimation" id="toc-filtering-forecasting-and-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#filtering-forecasting-and-maximum-likelihood-estimation"><span class="header-section-number">3</span> Filtering, Forecasting and Maximum Likelihood Estimation</a>
  <ul class="collapse">
  <li><a href="#sec-kalmanfilter" id="toc-sec-kalmanfilter" class="nav-link" data-scroll-target="#sec-kalmanfilter"><span class="header-section-number">3.1</span> The Kalman Filter</a></li>
  <li><a href="#the-information-filter" id="toc-the-information-filter" class="nav-link" data-scroll-target="#the-information-filter"><span class="header-section-number">3.2</span> The Information Filter</a></li>
  <li><a href="#kalman-smoothers" id="toc-kalman-smoothers" class="nav-link" data-scroll-target="#kalman-smoothers"><span class="header-section-number">3.3</span> Kalman Smoothers</a></li>
  </ul></li>
  <li><a href="#em-algorithm-needs-a-lot-of-work-probably-ignore-for-now" id="toc-em-algorithm-needs-a-lot-of-work-probably-ignore-for-now" class="nav-link" data-scroll-target="#em-algorithm-needs-a-lot-of-work-probably-ignore-for-now"><span class="header-section-number">4</span> EM Algorithm (NEEDS A LOT OF WORK, PROBABLY IGNORE FOR NOW)</a></li>
  <li><a href="#algorithm-for-maximum-complete-data-likelihood-estimation" id="toc-algorithm-for-maximum-complete-data-likelihood-estimation" class="nav-link" data-scroll-target="#algorithm-for-maximum-complete-data-likelihood-estimation"><span class="header-section-number">5</span> Algorithm for Maximum Complete-data Likelihood estimation</a></li>
  
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="mathematics.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Evan Tate Paterson Hughes </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p><a href="./index.html">Index</a></p>
<section id="integro-difference-based-dynamics" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Integro-difference Based Dynamics</h1>
<p>As common and widespread as the problem is, spatio-temporal modelling still presents a great deal of difficulty. Inherently, Spatio-Temporal datasets are almost always high-dimensional, and repeated observations are usually not possible.</p>
<p>Traditionally, the problem has been tackled by the moments (usually the means and covariances) of the process in order to make inference (<span class="citation" data-cites="wikle2019spatio">Wikle, Zammit-Mangion, and Cressie (<a href="#ref-wikle2019spatio" role="doc-biblioref">2019</a>)</span>, for example, call this ‘descriptive’ modelling). While this method can be sufficient for many problems, there are many cases where we are underutilizing some knowledge of the underlying dynamical systems involved. For instance, in temperature models, we know that temperature has movement (convection) and spread (diffusion), and that the state at any given time will depend on its state at previous times <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. We call models which make use of this ‘dynamical’ models. Of focus here is the Integro-Difference Equation Model (IDEM), which models diffusion and convection in discrete time by using convolution-like integral equations to model the relation between the process and it’s previous state. We use a hierarchical model to represent this type of system;</p>
<p><span id="eq-IDEM"><span class="math display">\[\begin{split}
Z_t(\boldsymbol s) &amp;= Y_t(\boldsymbol s) + \boldsymbol X(\boldsymbol s)^{\intercal}\boldsymbol \beta + \epsilon_t(\boldsymbol s)\\
Y_{t+1}(\boldsymbol s) &amp;= \int_{\mathcal D_s} \kappa(s,r) Y_t(r) d\boldsymbol r + \omega_t(\boldsymbol s).
\end{split}
\tag{1}\]</span></span></p>
<p>Where <span class="math inline">\(\omega_t(\boldsymbol s)\)</span> is a small scale gaussian variation with no temporal dynamics <span class="citation" data-cites="cressie2015statistics">(<a href="#ref-cressie2015statistics" role="doc-biblioref">Cressie and Wikle 2015</a> call this a ‘spatially descriptive’ component)</span>, <span class="math inline">\(\boldsymbol X(\boldsymbol s)\)</span> are spatially varying covariates (for example, in a large-scale climate scenario, this might simply be latitude), <span class="math inline">\(Z\)</span> is observed data, <span class="math inline">\(Y\)</span> is an unobserved dynamic process, <span class="math inline">\(\kappa\)</span> is the driving ‘kernel’ function, and <span class="math inline">\(\epsilon_t\)</span> is a gaussian white noise ‘measurement error’ term.</p>
</section>
<section id="process-decomposition" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Process Decomposition</h1>
<p>In order to work with the process, we likely want to consider the spectral decomposition of it. That is, choose a complete class of spatial spectral basis functions, <span class="math inline">\(\phi_i(\boldsymbol s)\)</span>, and decompose;</p>
<p><span id="eq-processdecomp"><span class="math display">\[\begin{split}
Y_t(\boldsymbol s) \approx \sum_{i=1}^{r} \alpha_{i,t} \phi_i(\boldsymbol s).
\end{split}
\tag{2}\]</span></span></p>
<p>where we truncate the expansion at some <span class="math inline">\(r\in\mathbb N\)</span>. Notice that we can write this in vector/matrix form; considering times <span class="math inline">\(t=1,2,\dots, T\)</span>, we set</p>
<p><span id="eq-vecmats"><span class="math display">\[\begin{split}
\boldsymbol \phi(\boldsymbol s) &amp;= (\phi_1(\boldsymbol s), \phi_2(\boldsymbol s), \dots, \phi_r(\boldsymbol s))^{\intercal}\\
\boldsymbol \alpha(t) &amp;= (\alpha_1(t), \alpha_2(t), \dots, \alpha_r(t))^{\intercal}
\end{split}
\tag{3}\]</span></span></p>
<p>Now, (<a href="#eq-processdecomp" class="quarto-xref">Equation&nbsp;2</a>) gives us</p>
<p><span id="eq-pbvec"><span class="math display">\[\begin{split}
Y(\boldsymbol s; t) &amp;= \boldsymbol \phi^{\intercal}(\boldsymbol s) \boldsymbol \alpha(t)\\
\end{split}
\tag{4}\]</span></span></p>
<p>We now want to find the equation defining the evolution of the spectral coefficients, <span class="math inline">\(\boldsymbol \alpha_t\)</span>.</p>
<div id="thm-state_form" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Basis form of the state evolution)</strong></span> Define the Gram matrix;</p>
<p><span id="eq-gram"><span class="math display">\[\Psi := \int_{\mathcal D_s} \boldsymbol{\mathbf{\phi}}(\boldsymbol s) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})^\intercal d\boldsymbol{\mathbf{s}}
\tag{5}\]</span></span></p>
<p>Then, the basis coefficients evolve by the equation</p>
<p><span class="math display">\[\boldsymbol{\mathbf{\alpha}}(t+1) = M \boldsymbol{\mathbf{\alpha}}(t) + \boldsymbol{\mathbf{\eta}}_t,
\]</span></p>
<p>where <span class="math inline">\(M = \Psi^{-1} \int\int \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}}) \kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}})\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{r}} d \boldsymbol{\mathbf{s}}\)</span> and <span class="math inline">\(\boldsymbol{\mathbf{\eta}}_t =\Psi^{-1} \int \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\omega_t(s)d\boldsymbol{\mathbf{s}}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="citation" data-cites="dewar2008data">(Adapting from <a href="#ref-dewar2008data" role="doc-biblioref">Dewar, Scerri, and Kadirkamanathan 2008</a>)</span>, write out the process equation, (<a href="#eq-IDEM" class="quarto-xref">Equation&nbsp;1</a>), using the first equation of (<a href="#eq-pbvec" class="quarto-xref">Equation&nbsp;4</a>);</p>
<p><span class="math display">\[Y(\boldsymbol{\mathbf{s}};t+1) = \boldsymbol{\mathbf{\phi}}(\boldsymbol s) \alpha(t+1) = \int_{\mathcal D_s} \kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^{\intercal}\boldsymbol{\mathbf{\alpha}}(t)d\boldsymbol{\mathbf{r}} + \omega_t(\boldsymbol{\mathbf{s}}),
\]</span></p>
<p>We then multiply both sides by <span class="math inline">\(\boldsymbol \phi(s)\)</span> and integrate over <span class="math inline">\(\boldsymbol s\)</span></p>
<p><span class="math display">\[\begin{split}
\int_{\mathcal D_s} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}}) d\boldsymbol{\mathbf{s}} \boldsymbol{\mathbf{\alpha}}(t+1) &amp;= \int\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\int \kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}})\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{r}}  d \boldsymbol{\mathbf{s}}\ \boldsymbol{\mathbf{\alpha}}(t) + \int \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\omega_t(s)d\boldsymbol{\mathbf{s}}\\
\Psi \boldsymbol{\mathbf{\alpha}}(t+1) &amp;= \int\int \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{r}} d \boldsymbol{\mathbf{s}}\ \boldsymbol{\mathbf{\alpha}}(t) + \int \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\omega_t(s)d\boldsymbol{\mathbf{s}}.
\end{split}
\]</span></p>
<p>So, finally, pre-multipling by the inverse of the gram matrix, <span class="math inline">\(\Psi^{-1}\)</span> (<a href="#eq-gram" class="quarto-xref">Equation&nbsp;5</a>), we arrive at the result.</p>
</div>
<section id="process-noise" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="process-noise"><span class="header-section-number">2.1</span> Process Noise</h2>
<p>We still have to set out what the process noise, <span class="math inline">\(\omega_t(\boldsymbol{\mathbf{s}})\)</span>, and it’s spectral couterpart, <span class="math inline">\(\boldsymbol{\mathbf{\eta}}_t\)</span>, are. Dewar <span class="citation" data-cites="dewar2008data">(<a href="#ref-dewar2008data" role="doc-biblioref">Dewar, Scerri, and Kadirkamanathan 2008</a>)</span> fixes the variance of <span class="math inline">\(\omega_t(\boldsymbol{\mathbf{s}})\)</span> to be uniform and uncorrelated across space and time, with <span class="math inline">\(\omega_t(\boldsymbol{\mathbf{s}}) \sim \mathcal N(0,\sigma^2)\)</span> It is then easily shown that <span class="math inline">\(\boldsymbol{\mathbf{\eta}}_t\)</span> is also normal, with <span class="math inline">\(\boldsymbol{\mathbf{\eta}}_t \sim \mathcal N(0, \sigma^2\Psi^{-1})\)</span>.</p>
<p>However, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of <span class="math inline">\(\boldsymbol{\mathbf{\eta}}_t\)</span>, and compute the variance of <span class="math inline">\(\omega_t(\boldsymbol{\mathbf{s}})\)</span> if needed. This is exactly what the IDE package <span class="citation" data-cites="zammit2022IDE">(<a href="#ref-zammit2022IDE" role="doc-biblioref">Zammit-Mangion 2022</a>)</span> in R does, and, correspondingly, what this JAX project does.</p>
<div id="lem-omegadist" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> Let <span class="math inline">\(\boldsymbol{\mathbf{\eta}}_t \sim \mathcal N(0, \Sigma_\eta)\)</span>, and <span class="math inline">\(\mathop{\mathrm{\mathbb{C}\mathrm{ov}}}[\boldsymbol{\mathbf{\eta}}_t, \boldsymbol{\mathbf{\eta}}_{t+\tau}] =0\)</span>, <span class="math inline">\(\forall \tau&gt;0\)</span>. Then <span class="math inline">\(\omega_t(\boldsymbol{\mathbf{s}})\)</span> has covariance</p>
<p><span class="math display">\[\mathop{\mathrm{\mathbb{C}\mathrm{ov}}}[\omega_t(\boldsymbol{\mathbf{s}}), \omega_{t+\tau}(\boldsymbol{\mathbf{r}})] = \begin{cases}
\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})^\intercal \Sigma_\eta \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}}) &amp; \text{if }\tau=0\\
0 &amp; \text{else}\\
\end{cases}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Consider <span class="math inline">\(\Psi \boldsymbol{\mathbf{\eta}}_t\)</span>. It is clearly normal, with expectation zero and variance (using (<a href="#eq-gram" class="quarto-xref">Equation&nbsp;5</a>)),</p>
<p><span id="eq-var1"><span class="math display">\[\begin{split}
\mathop{\mathrm{\mathbb{V}\mathrm{ar}}}[\Psi \boldsymbol{\mathbf{\eta}}_t] &amp;= \Psi \mathop{\mathrm{\mathbb{V}\mathrm{ar}}}[\boldsymbol{\mathbf{\omega}}_t] \Psi^\intercal = \Psi\Sigma_\eta\Psi^\intercal,\\
&amp;= \int_{\mathcal D_s} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})^\intercal d\boldsymbol{\mathbf{s}} \  \Sigma_\eta \ \int_{\mathcal D_s} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{r}}\\
&amp;=  \int\int_{\mathcal D_s^2} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})^\intercal \  \Sigma_\eta \  \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{r}} d\boldsymbol{\mathbf{s}}\\
\end{split}
\tag{6}\]</span></span></p>
<p>Since it has zero expectation, we also have</p>
<p><span id="eq-var2"><span class="math display">\[\begin{split}
\mathop{\mathrm{\mathbb{V}\mathrm{ar}}}[\Psi\boldsymbol{\mathbf{\eta}}_t] &amp;= \mathbb E[(\Psi\boldsymbol{\mathbf{\eta}}_t) (\Psi\boldsymbol{\mathbf{\eta}}_t)^\intercal] = \mathbb E[\Psi\boldsymbol{\mathbf{\eta}}_t\boldsymbol{\mathbf{\eta}}_t^\intercal\Psi^\intercal]\\
&amp;= \mathbb E \left[ \int_{\mathcal D_s} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\omega_t(\boldsymbol{\mathbf{s}})d\boldsymbol{\mathbf{s}} \int_{\mathcal D_s} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal \omega_t(\boldsymbol{\mathbf{r}}) d\boldsymbol{\mathbf{r}} \right]\\
&amp;= \int\int_{\mathcal D_s^2} \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})\  \mathbb E[\omega_t(\boldsymbol{\mathbf{s}})\omega_t(\boldsymbol{\mathbf{r}})]\  \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{s}} d \boldsymbol{\mathbf{r}}.
\end{split}
\tag{7}\]</span></span></p>
<p>We can see that, comparing (<a href="#eq-var1" class="quarto-xref">Equation&nbsp;6</a>) and (<a href="#eq-var2" class="quarto-xref">Equation&nbsp;7</a>), we have</p>
<p><span class="math display">\[\mathop{\mathrm{\mathbb{C}\mathrm{ov}}}[\omega_t(\boldsymbol{\mathbf{s}}), \omega_t(\boldsymbol{\mathbf{r}})] = \mathbb E[\omega_t(\boldsymbol{\mathbf{s}})\omega_t(\boldsymbol{\mathbf{r}})]= \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})^\intercal \Sigma_\eta \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}}).
\]</span></p>
</div>
</section>
<section id="kernel-decomposition" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="kernel-decomposition"><span class="header-section-number">2.2</span> Kernel Decomposition</h2>
<p>Next is the key part od the system, which defines the dynamics; the kernelf function, <span class="math inline">\(\kappa\)</span>. There are a few ways to handle the kernel. One of the most obvious is to expand it out into a spectral decomposition as well;</p>
<p><span class="math display">\[\kappa \approx \sum_i \beta_i\psi(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}}).
\]</span></p>
<p>This can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on <span class="math inline">\(\mathbb R^2\times \mathbb R^2\)</span>; to get a wide enough space of possible functions, we would likely need many terms of the basis expansion.</p>
<p>A much simpler approach would be to simply parameterise the kernel function, to <span class="math inline">\(\kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}}, \boldsymbol{\mathbf{\theta}}_\kappa)\)</span>. We then establish a simple shape for the kernel (e.g.&nbsp;Gaussian) and rely on very few parameters (for example, scale, shape, offsets). The example kernel used in the program is aGaussian kernel;</p>
<p><span class="math display">\[\kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}}; \boldsymbol{\mathbf{m}}, a, b) = a \exp \left( -\frac{1}{b} \Vert \boldsymbol{\mathbf{s}}- \boldsymbol{\mathbf{r}} +\boldsymbol{\mathbf{m}}\Vert^2 \right)
\]</span></p>
<p>Of course, this kernel lacks spatial dependance. We can add spatial variance back in in a nice way by adding dependance on <span class="math inline">\(\boldsymbol{\mathbf{s}}\)</span> to the parameters, for example, variyng the offset term as <span class="math inline">\(\boldsymbol{\mathbf{m}}(\boldsymbol{\mathbf{s}})\)</span>. Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground <span class="citation" data-cites="cressie2015statistics">(<a href="#ref-cressie2015statistics" role="doc-biblioref">Cressie and Wikle 2015</a>)</span>. The actual parameters of such a spatially-variant kernel are then the basis coefficients for the expansion of any spatially variant parameters, as well as any constant parameters.</p>
</section>
<section id="idem-as-a-linear-dynamical-system" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="idem-as-a-linear-dynamical-system"><span class="header-section-number">2.3</span> IDEM as a linear dynamical system</h2>
<p>To recap, we have discretized space in such a way that the Integro-difference model is of a more traditional linear dynamical system form. All that is left is to include our observations in our system.</p>
<p>Lets assume that at each time <span class="math inline">\(t\)</span> there are <span class="math inline">\(n_t\)</span> observations at locations <span class="math inline">\(\boldsymbol{\mathbf{s}}_{1,t},\dots, \boldsymbol{\mathbf{s}}_{n_{t},t}\)</span>. We write the vector of the process at these points as <span class="math inline">\(\boldsymbol{\mathbf{Y}}(t) = (Y(s_{1,t};t), \dots, Y(s_{n_{t},t};t))^\intercal\)</span>, and, in it’s expanded form <span class="math inline">\(\boldsymbol{\mathbf{Y}}_t = \Phi_t \boldsymbol{\mathbf{\alpha}}_t\)</span>, where <span class="math inline">\(\Phi \in \mathbb R^{r\times n_{t}}\)</span> is</p>
<p><span class="math display">\[\begin{split}
\{\Phi_{t}\}_{i, j} = \phi_{i}(s_{j,t}).
\end{split}
\]</span></p>
<p>For the covariates, we write the matrix <span class="math inline">\(X_t = (\boldsymbol{\mathbf{X}}(\boldsymbol{\mathbf{s}}_{1, t}), \dots, \boldsymbol{\mathbf{X}}(\boldsymbol{\mathbf{s}}_{1=n_{t}, t})^\intercal\)</span>. We then have</p>
<p><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{Z}}_t &amp;= \Phi \alpha_t + X_{t} \boldsymbol{\mathbf{\beta }}+ \boldsymbol{\mathbf{\epsilon}}_t, \quad t=0,1,\dots, T,\\
\boldsymbol{\mathbf{\alpha}}_{t+1} &amp;= M\boldsymbol{\mathbf{\alpha}}_t + \boldsymbol{\mathbf{\eta}}_t,\quad t = 1,2,\dots, T,\\
M &amp;= \int_{\mathcal D_s}\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}}) \boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}})^\intercal d\boldsymbol{\mathbf{s}} \int_{\mathcal D_s^2}\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{s}}) \kappa(\boldsymbol{\mathbf{s}}, \boldsymbol{\mathbf{r}}; \boldsymbol{\mathbf{\theta}}_\kappa)\boldsymbol{\mathbf{\phi}}(\boldsymbol{\mathbf{r}})^\intercal d\boldsymbol{\mathbf{r}} d \boldsymbol{\mathbf{s}},
\end{split}
\]</span></p>
<p>Writing <span class="math inline">\(\tilde{\boldsymbol{\mathbf{Z}}}_t = \boldsymbol{\mathbf{Z}}_t - X_t \boldsymbol{\mathbf{\beta}}\)</span>,</p>
<p><span id="eq-ldstm"><span class="math display">\[\begin{split}
\tilde{\boldsymbol{\mathbf{Z}}}_t &amp;= \Phi_{t} \boldsymbol{\mathbf{\alpha}}_t + \boldsymbol{\mathbf{\epsilon}}_t,\quad &amp;t = 1,2,\dots, T,\\
\boldsymbol{\mathbf{\alpha}}_{t+1} &amp;= M \boldsymbol{\mathbf{\alpha}}_t + \boldsymbol{\mathbf{\eta}}_t,\quad &amp;t = 0,1, \dots, T.\\
\end{split}
\tag{8}\]</span></span></p>
<p>We should also initialise <span class="math inline">\(\boldsymbol{\mathbf{\alpha}}_0 \sim \mathcal N^{r}(\boldsymbol{\mathbf{m}}_{0}, \Sigma_{0})\)</span>, and fix simple distrubtions to the noise terms,</p>
<p><span class="math display">\[\begin{split}
\epsilon_{t,i} \overset{\mathrm{iid}}{\sim} \mathcal N(0,\sigma^2_\epsilon),\\
\eta_{t,i} \overset{\mathrm{iid}}{\sim} \mathcal N(0,\sigma^2_\eta),
\end{split}
\]</span></p>
<p>which are (also) independant in time.</p>
<p>As in, for example, <span class="citation" data-cites="wikle1999dimension">(<a href="#ref-wikle1999dimension" role="doc-biblioref">Wikle and Cressie 1999</a>)</span>, this is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood. We can use these quantities in either an EM algorithm or a Bayesian approach. Firstly, we cover the EM algorithm applied to this system.</p>
<p>At most, the parameters to be estimated are</p>
<p><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{\theta }}= \left(\boldsymbol{\mathbf{\theta}}_\kappa^\intercal, \boldsymbol{\mathbf{\beta}}^\intercal, \boldsymbol{\mathbf{m}}_0^\intercal, \sigma^{2}_{\epsilon}, \sigma^{2}_{\eta}, \mathrm{vec}[\Sigma_0]\right),
\end{split}
\]</span></p>
<p>where the <span class="math inline">\(\mathrm{vec}[\cdot]\)</span> operator gives the elements of the matrix in a column vector. Of course, in practice, some of these may be estimated outside of the algorithm, fixed, given a much simpler form (e.g.&nbsp;<span class="math inline">\(\Sigma_\eta = \sigma_\eta^2 I_d\)</span>), etc.</p>
<p>There are two approaches we can make from here; directly maximising the marginal data likelihood using only the Kalman filter, or maximising the full likelihood with the EM algorithm.</p>
<p>Now (<a href="#eq-ldstm" class="quarto-xref">Equation&nbsp;8</a>) is of the very familar linear dynamical system (LDS) type. This is a well-understood problem, and optimal state estimation can be done using the kalman filter and (RTS) smoother.</p>
</section>
</section>
<section id="filtering-forecasting-and-maximum-likelihood-estimation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Filtering, Forecasting and Maximum Likelihood Estimation</h1>
<p>The Kalman filter gives us linear estimates for the distribution of <span class="math inline">\(\boldsymbol{\mathbf{\alpha}}_r\mid \{Z_t\}_{t=0,...,r}\)</span> in any dynamical system like <a href="#eq-ldstm" class="quarto-xref">Equation&nbsp;8</a>. For full discussions and proofs of the Kalman filter, see, for example, <span class="citation" data-cites="shumway2000time">(<a href="#ref-shumway2000time" role="doc-biblioref">Shumway, Stoffer, and Stoffer 2000</a>)</span>.</p>
<section id="sec-kalmanfilter" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-kalmanfilter"><span class="header-section-number">3.1</span> The Kalman Filter</h2>
<p>Firstly, we should establish some notation. Write</p>
<p><span class="math display">\[\begin{split}
m_{r \mid s} &amp;= \mathbb E[\boldsymbol{\mathbf{\alpha}}_r \mid \{Z_t\}_{t=0,\dots,s}]\\
P_{r \mid s} &amp;= \mathop{\mathrm{\mathbb{V}\mathrm{ar}}}[\boldsymbol{\mathbf{\alpha}}_r \mid \{Z_t\}_{t=0,\dots,s}]\\
P_{r,q \mid s} &amp;= \mathop{\mathrm{\mathbb{C}\mathrm{ov}}}[\boldsymbol{\mathbf{\alpha}}_r, \boldsymbol{\mathbf{\alpha}}_q \mid \{Z_t\}_{t=0,\dots,s}].
\end{split}
\]</span></p>
<p>For the initial terms, <span class="math inline">\(m_{0\mid0}=m_0\)</span> and <span class="math inline">\(P_{0\mid0}=\Sigma_0\)</span>. For convenience and generality, we write <span class="math inline">\(\Sigma_\eta\)</span> and <span class="math inline">\(\Sigma_\epsilon\)</span> for the variance matrices of the process and observations. Note that, if the number of observations change at each time point (for example, due to missing data), then <span class="math inline">\(\Sigma_\epsilon\)</span> should be time variyng; we could either always keep it as uncorrelated so that <span class="math inline">\(\Sigma_\epsilon = \mathrm{diag} (\sigma_\epsilon^2)\)</span>, or perhaps put some kind of distance-dependant covariance function to it.</p>
<p>To move the filter forward, that is, given <span class="math inline">\(m_{r\mid s}\)</span> and <span class="math inline">\(P_{r\mid s}\)</span>, to get <span class="math inline">\(m_{t+1\mid t+1}\)</span> and <span class="math inline">\(P_{t+1\mid t+1}\)</span>, we first <em>predict</em></p>
<p><span id="eq-kalman-predict"><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{m}}_{t+1\mid t} &amp;= M \boldsymbol{\mathbf{m}}_{t\mid t}\\
P_{t+1\mid t} &amp;= M P_{t\mid t} M^\intercal + \Sigma_\eta,
\end{split}
\tag{9}\]</span></span></p>
<p>then we add our new information, <span class="math inline">\(z_{t}\)</span>, adjusted for the <em>Kalman gain</em>;</p>
<p><span id="eq-kalman-update"><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{m}}_{t+1\mid t+1} &amp;= \boldsymbol{\mathbf{m}}_{t+1\mid t} + K_{t+1} \boldsymbol{\mathbf{e}}_{t+1}\\
P_{t+1\mid t+1} &amp;= [I- K_{t+1}\Phi_{t+1}]P_{t+1\mid t}
\end{split}
\tag{10}\]</span></span></p>
<p>where <span class="math inline">\(K_{t+1}\)</span> is the <em>Kalman gain</em>;</p>
<p><span class="math display">\[\begin{split}
K_{t+1} = P_{t+1\mid t}\Phi_{t+1}^\intercal [\Phi_{t+1} P_{t+1\mid t} \Phi_{t+1}^\intercal + \Sigma_\epsilon]^{-1}, \quad t=0,\dots,T-1
\end{split}
\]</span></p>
<p>and <span class="math inline">\(\boldsymbol{\mathbf{e}}_{t+1}\)</span> are the <em>prediction errors</em></p>
<p><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{e}}_{t+1} = \tilde{\boldsymbol{\mathbf{z}}}_{t+1}-\Phi_{t+1} \boldsymbol{\mathbf{m}}_{t+1\mid t}, \quad t=1,\dots,T
\end{split}
\]</span></p>
<p>Starting with <span class="math inline">\(m_{0\mid0} = m_0\)</span> and <span class="math inline">\(P_{0\mid0} =\Sigma_0\)</span>, we can then iteratively move across the data to eventually compute <span class="math inline">\(m_{T\mid T}\)</span> and <span class="math inline">\(P_{T\mid T}\)</span>.</p>
<p>Assuming Gaussian all random variables here are Guassian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of <em>linear</em> operators.</p>
<p>We can compute the marginal data likelihood alongside the kalman fiter using the prediciton errors <span class="math inline">\(\boldsymbol{\mathbf{e}}_t\)</span>. These, under the assumptions we have made about <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\epsilon\)</span> being normal, are also normal with zero mean and variance</p>
<p><span class="math display">\[\begin{split}
\mathbb V\mathrm{ar}[\boldsymbol{\mathbf{e}}_t]=\Sigma_t= \Phi_{t} P_{t+1\mid t} \Phi_{t}^\intercal + \Sigma_\epsilon.
\end{split}
\]</span></p>
<p>Therefore, the log-likelihood at each time is</p>
<p><span class="math display">\[\begin{split}
\mathcal L(Z\mid\boldsymbol{\mathbf{\theta}}) = -\frac12\sum \log\det(\Sigma_t(\boldsymbol{\mathbf{\theta}})) - \frac12 \sum\boldsymbol{\mathbf{e}}_t(\boldsymbol{\mathbf{\theta}})^\intercal\Sigma_{t}(\boldsymbol{\mathbf{\theta}})^{-1} \boldsymbol{\mathbf{e}}_t(\boldsymbol{\mathbf{\theta}}) - \frac{n_{t}}{2}\log(2*\pi).
\end{split}
\]</span></p>
<p>Summing these across time, we get the log likelihood for all the data.</p>
<p>A simplified example of the kalman filter function, written to be jax compatible, used in the package is this;</p>
<div id="b4960e34" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Replace this with a simpler 'naive' implementation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kalman_filter(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    m_0: ArrayLike,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    P_0: ArrayLike,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    M: ArrayLike,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    PHI_obs: ArrayLike,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    Sigma_eta: ArrayLike,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    Sigma_eps: ArrayLike,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    ztildes: ArrayLike,  <span class="co"># data matrix, with time across columns</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">tuple</span>:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    nbasis <span class="op">=</span> m_0.shape[<span class="dv">0</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    nobs <span class="op">=</span> ztildes.shape[<span class="dv">0</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(carry, z_t):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        m_tt, P_tt, _, _, ll, _ <span class="op">=</span> carry</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predict</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        m_pred <span class="op">=</span> M <span class="op">@</span> m_tt</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        P_pred <span class="op">=</span> M <span class="op">@</span> P_tt <span class="op">@</span> M.T <span class="op">+</span> Sigma_eta</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prediction Errors</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        eps_t <span class="op">=</span> z_t <span class="op">-</span> PHI_obs <span class="op">@</span> m_pred</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        Sigma_t <span class="op">=</span> PHI_obs <span class="op">@</span> P_pred <span class="op">@</span> PHI_obs.T <span class="op">+</span> Sigma_eps</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Kalman Gain</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        K_t <span class="op">=</span> (</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            jnp.linalg.solve(Sigma_t, PHI_obs)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            <span class="op">@</span> P_pred.T</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        ).T</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        m_up <span class="op">=</span> m_pred <span class="op">+</span> K_t <span class="op">@</span> eps_t</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        P_up <span class="op">=</span> (jnp.eye(nbasis) <span class="op">-</span> K_t <span class="op">@</span> PHI_obs) <span class="op">@</span> P_pred</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># likelihood of epsilon, using cholesky decomposition</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        chol_Sigma_t <span class="op">=</span> jnp.linalg.cholesky(Sigma_t)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> jax.scipy.linalg.solve_triangular(chol_Sigma_t, eps_t)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        ll_new <span class="op">=</span> ll <span class="op">-</span> jnp.<span class="bu">sum</span>(jnp.log(jnp.diag(chol_Sigma_t))</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>                              ) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> jnp.dot(z, z)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (m_up, P_up, m_pred, P_pred, ll_new, K_t), (</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>            m_up,</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>            P_up,</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            m_pred,</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>            P_pred,</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            ll_new,</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            K_t,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    carry, seq <span class="op">=</span> jl.scan(</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        step,</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        (m_0, P_0, m_0, P_0, <span class="dv">0</span>, jnp.zeros((nbasis, nobs))),</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        ztildes.T,</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (carry[<span class="dv">4</span>], seq[<span class="dv">0</span>], seq[<span class="dv">1</span>], seq[<span class="dv">2</span>][<span class="dv">1</span>:], seq[<span class="dv">3</span>][<span class="dv">1</span>:], seq[<span class="dv">5</span>][<span class="dv">1</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For the documentation of the method proveded by the package, see [WORK OUT HOW TO LINK DO PAGES]</p>
</section>
<section id="the-information-filter" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-information-filter"><span class="header-section-number">3.2</span> The Information Filter</h2>
<p>In some computational scenarios, it is beneficial to work with vectors of consistent dimension. In python jax, the efficient <code>scan</code> and <code>map</code> operations work only with such operations; JAX has no support for jagged arrays, and traditional for loops with have long compile times when jit-compiled. Although there are some tools in JAX to get around this problem (namely the <code>jax.tree</code> functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it’s core, a scan-type operation, this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.</p>
<p>But it is possible to re-write the kalman filter in a way which is compatible with this kind of data. the sometimes called ‘information filter’ involves transforming the data into a kind of ‘information form’, which will always have consistent dimension.</p>
<p>The information filter is simply the kalman filter re-written to use the Gaussian distribution’s canonical parameters, those being the information vector and the information matrix. If a Gaussian distribution has mean <span class="math inline">\(\boldsymbol{\mathbf{\mu}}\)</span> and variance matrix <span class="math inline">\(\Sigma\)</span>, then the corresponding <em>information vector</em> and <em>information matrix</em> is <span class="math inline">\(\nu = \Sigma^{-1}\mu\)</span> and <span class="math inline">\(Q = \Sigma^{-1}\)</span>, correspondingly.</p>
<div id="thm-information_filter" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (The Information Filter)</strong></span> The Kalman filter can be rewritten in information form as follows <span class="citation" data-cites="khan2005matrix">(for example, <a href="#ref-khan2005matrix" role="doc-biblioref">Khan 2005</a>)</span>. Write</p>
<p><span class="math display">\[\begin{split}
Q_{i\mid j} &amp;= P_{i\mid j}\\
\boldsymbol{\mathbf{\nu}}_{i\mid j} &amp;= Q_{i\mid j} \boldsymbol{\mathbf{m}}_{i\mid j}
\end{split}
\]</span></p>
<p>and transform the observations into their ‘information form’, for <span class="math inline">\(t=1,\dots, T\)</span></p>
<p><span id="eq-obsinfo"><span class="math display">\[\begin{split}
I_{t} = \Phi_{t}^{\intercal} \Sigma_{\epsilon}^{-1}\Phi_{t},\\
i_{t} = \Phi_{t}^{\intercal} \Sigma_{\epsilon}^{-1} \boldsymbol{\mathbf{z}}_{t}.
\end{split}
\tag{11}\]</span></span></p>
<p>The prediction step now becomes</p>
<p><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{\nu}}_{t+1\mid t} &amp;= (I-J_t) M^{-1}\boldsymbol{\mathbf{\nu}}_{t\mid t}\\
Q_{t+1\mid t} &amp;= (I-J_t) S_{t}
\end{split}
\]</span></p>
<p>where <span class="math inline">\(S_t = M^{-\intercal} Q_{t\mid t} M^{-1}\)</span> and <span class="math inline">\(J_t = S_t [S_{t}+\Sigma_{\eta}^{-1}]^{-1}\)</span>.</p>
<p>Updating is now as simple as adding the information-form observations;</p>
<p><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{\nu}}_{t+1\mid t+1} &amp;= \boldsymbol{\mathbf{\nu}}_{t+1\mid t} + i_{t+1}\\
  Q_{t+1\mid t+1} &amp;= Q_{t+1\mid t} + I_{t+1}.
\end{split}
\]</span></p>
</div>
<p>Proof in Appendix (<a href="#sec-app1" class="quarto-xref">Section&nbsp;6.2</a>.)</p>
<p>We can see that the information form of the observations (<a href="#eq-obsinfo" class="quarto-xref">Equation&nbsp;11</a>) will always have the same dimension (that being the process dimension, previously labelled <span class="math inline">\(r\)</span>, the number of basis functions used in the expansion). For our purposes, this means that <code>jax.lax.scan</code> will work after we ‘informationify’ the data, which can be done using <code>jax.tree.map</code>. This is implemented in the functions <code>information_filter</code> and <code>information_filter_indep</code> (for uncorrelated errors).</p>
<p>There are other often cited advantages to filtering in this form. It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with <span class="math inline">\([S_t + \Sigma_\eta]^{-1}\)</span> in the process dimesnion instead of <span class="math inline">\([\Phi_t P_{t+1\mid t} \Phi_t^\intercal + \Sigma_\epsilon]^{-1}\)</span> in the observation dimension) <span class="citation" data-cites="assimakis2012information">(<a href="#ref-assimakis2012information" role="doc-biblioref">Assimakis, Adam, and Douladiris 2012</a>)</span>.</p>
<p>The other often mentioned advantage is the ability to use a truly vague prior for <span class="math inline">\(\alpha_0\)</span>; that is, we can set <span class="math inline">\(Q_0\)</span> as the zero matrix, without worriying about an infinite variance matrix. While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see (<a href="#sec-vagueprior" class="quarto-xref">Section&nbsp;6.3</a>).</p>
</section>
<section id="kalman-smoothers" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="kalman-smoothers"><span class="header-section-number">3.3</span> Kalman Smoothers</h2>
<p>Beyond the Kalman filters, we can also do Kalman smoothers. That is, filters estimate <span class="math inline">\(\boldsymbol{\mathbf{m}}_{T\mid T}\)</span> and <span class="math inline">\(P_{T\mid T}\)</span>, but there is use for estimating <span class="math inline">\(\boldsymbol{\mathbf{m}}_t\mid T\)</span> and <span class="math inline">\(P_{t\mid T}\)</span> for all <span class="math inline">\(t=0,\dots, T\)</span>.</p>
<p>We can then work backwards from these values using what is known as the <em>Rauch-Tung-Striebel (RTS) smoother</em>;</p>
<p><span id="eq-kalmansmooth"><span class="math display">\[\begin{split}
\boldsymbol{\mathbf{m}}_{t-1\mid T} &amp;= \boldsymbol{\mathbf{m}}_{t-1\mid t-1} + J_{t-1}(\boldsymbol{\mathbf{m}}_{t\mid T} - \boldsymbol{\mathbf{m}}_{t\mid t-1}),\\
P_{t-1\mid T} &amp;= P_{t-1\mid t-1} + J_{t-1}(P_{t\mid T} - P_{t\mid t-1})J_{t-1}^\intercal,
\end{split}
\tag{12}\]</span></span></p>
<p>where,</p>
<p><span class="math display">\[\begin{split}
J_{t-1} = P_{t-1\mid t-1}M^\intercal[P_{t\mid t-1}]^{-1}.
\end{split}
\]</span></p>
<p>We can clearly see, then, that it is crucial to keep the values in <a href="#eq-kalman-predict" class="quarto-xref">Equation&nbsp;9</a>.</p>
<p>We can then also compute the lag-one cross-covariance matrices <span class="math inline">\(P_{t,t-1\mid T}\)</span> using the <em>Lag-One Covariance Smoother</em> (is this what they call the RTS smoother?) From</p>
<p><span class="math display">\[\begin{split}
P_{T,T-1\mid T} = (I - K_T\Phi_{T}) MP_{T-1\mid T-1},
\end{split}
\]</span></p>
<p>we can compute the lag-one covariances</p>
<p><span id="eq-lag1smooth"><span class="math display">\[\begin{split}
P_{t, t-1\mid T} = P_{t\mid t}J_{t-1}^\intercal + J_{t}[P_{t+1,t\mid T} - MP_{t-1\mid t-1}]J_{t-1}^\intercal
\end{split}
\tag{13}\]</span></span></p>
<p>These values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later.</p>
</section>
</section>
<section id="em-algorithm-needs-a-lot-of-work-probably-ignore-for-now" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> EM Algorithm (NEEDS A LOT OF WORK, PROBABLY IGNORE FOR NOW)</h1>
<p>Instead of the marginal data likelihood, we may instead want to work with the ‘full’ likelihood, including the unobserved process, <span class="math inline">\(l(\boldsymbol{\mathbf{z}}(1),\dots, \boldsymbol{\mathbf{z}}(T), \boldsymbol{\mathbf{Y}}(1), \dots, \boldsymbol{\mathbf{Y}}(T)\mid \boldsymbol{\mathbf{\theta}})\)</span>, or, equivalently, <span class="math inline">\(l(\boldsymbol{\mathbf{z}}(1),\dots, \boldsymbol{\mathbf{z}}(t), \boldsymbol{\mathbf{\alpha}}(1), \dots, \boldsymbol{\mathbf{\alpha}}(T)\mid \boldsymbol{\mathbf{\theta}})\)</span>. This is difficult to maximise directly, but can be done with the EM algorithm, consisting of two steps, which can be shown to always increase the full likelihood.</p>
<p>Firstly, the E step is to find the function</p>
<p><span id="eq-Qdef"><span class="math display">\[\begin{split}
\mathcal Q(\boldsymbol{\mathbf{\theta}}; \boldsymbol{\mathbf{\theta}}') = \mathbb E_{\boldsymbol{\mathbf{Z}}(t)\sim p(Z \mid \boldsymbol{\mathbf{\alpha}}(t),\boldsymbol{\mathbf{\theta}})}[\log p_{\boldsymbol{\mathbf{\theta}}}(Z^{(T)}, A^{(T)})\mid Z^{(T)}],
\end{split}
\tag{14}\]</span></span></p>
<p>where <span class="math inline">\(Z^{(T)} = \{\boldsymbol{\mathbf{z}}_t\}_{t=0,\dots,T}\)</span>, <span class="math inline">\(A^{(T)} = \{\boldsymbol{\mathbf{\alpha}}_t\}_{t=0,\dots,T}\)</span> and <span class="math inline">\(A^{(T-1)} = \{\boldsymbol{\mathbf{\alpha}}_t\}_{t=0,\dots,T-1}\)</span>. This approximates <span class="math inline">\(\log p_\theta(Z^{(T)}, A^{(T)})\)</span>.</p>
<div id="prp-EMQ" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1</strong></span> We have <span style="color: red;">[NOTE: This may well be wrong in places…]</span></p>
<p><span id="eq-Q"><span class="math display">\[\begin{split}
-2\mathcal Q(\boldsymbol{\mathbf{\theta}};\boldsymbol{\mathbf{\theta}}') &amp;= \mathbb E_{Z^{(T)}\sim p(Z \mid A^{(T)},\boldsymbol{\mathbf{\theta}}')}[\log p_{\boldsymbol{\mathbf{\theta}}}(Z^{(T)}, A^{(T)}\mid Z^{(T)} = z^{(T)})]\\
&amp;\stackrel{c}{=}\sigma_\epsilon^2 [\sum_{t=0}^{T}\boldsymbol{\mathbf{z}}_t^{\intercal}z_t - 2\Phi_t(\sum_{t=1}^{T} \boldsymbol{\mathbf{z}}_t^\intercal \boldsymbol{\mathbf{m}}_{t\mid T}) - 2(\sum_{t=0}^{T} \boldsymbol{\mathbf{z}}_t^T)X_t\boldsymbol{\mathbf{\beta}}\\
&amp;\quad\quad\quad +\Phi_t^\intercal(\sum_{t=0}^{T}\mathrm{tr}\{P_{t\mid T} - \boldsymbol{\mathbf{m}}_{t\mid T}\boldsymbol{\mathbf{m}}_{t\mid T}^{\intercal}\})\Phi_t + 2X_t\boldsymbol{\mathbf{\beta}}\Phi_t(\sum_{t=0}^{T}\boldsymbol{\mathbf{m}}_{t\mid T}) + (\sum_{t=1}^{T}X_t^\intercal \boldsymbol{\mathbf{\beta}}^{\intercal}\boldsymbol{\mathbf{\beta }}X_t)]\\
&amp;\quad + \mathrm{tr}\{\Sigma_\eta^{-1}[(\sum_{t=1}^{T}P_{t\mid T} - m_{t\mid T}) - 2M(\sum_{t=1}^{T}P_{t,t-1\mid T} - \boldsymbol{\mathbf{m}}_{t-1,T}\boldsymbol{\mathbf{m}}_{t\mid T}^{\intercal})\\
&amp;\quad\quad\quad\quad\quad + M(\sum_{t=1}^{T}P_{t-1\mid T} - \boldsymbol{\mathbf{m}}_{t-1\mid T}\boldsymbol{\mathbf{m}}_{t-1\mid T}^{\intercal})M^\intercal]\}\\
&amp;\quad + \mathrm{tr}\{\Sigma_0^{-1}[P_{0\mid T} - m_{0\mid T}m_{0\mid T}^{\intercal} - 2\boldsymbol{\mathbf{m}}_{0\mid T}\boldsymbol{\mathbf{m}}_0 + \boldsymbol{\mathbf{m}}_0\boldsymbol{\mathbf{m}}_0^\intercal]\}\\
&amp;\quad + \log(\det(\sigma_\epsilon^{2T}\Sigma_\eta^{T+1}\Sigma_0))
\end{split}
\tag{15}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>See appendix.</p>
</div>
<p>In the EM algorithm, we maximise the full likelihood by changing <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> in order to increase (<a href="#eq-Q" class="quarto-xref">Equation&nbsp;15</a>), which can be shown to guarantee that the Likelihood <span class="math inline">\(L(\boldsymbol{\mathbf{\theta}})\)</span> also increases. The idea is then that repeatedly alternating between adjusting <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> to increase <a href="#eq-Q" class="quarto-xref">Equation&nbsp;15</a>, and then doing the filters and smoothers to obtain new values for <span class="math inline">\(\boldsymbol{\mathbf{m}}_{t\mid T}\)</span>, <span class="math inline">\(P_{t\mid T}\)</span>, and <span class="math inline">\(P_{t,t-1\mid T}\)</span>.</p>
</section>
<section id="algorithm-for-maximum-complete-data-likelihood-estimation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Algorithm for Maximum Complete-data Likelihood estimation</h1>
<p>Overall, our algorithm for Maximum Likelihood estimation is:</p>
<ol type="1">
<li>Set <span class="math inline">\(i=0\)</span> and take an initial guess for the parameters we are considering, <span class="math inline">\(\boldsymbol{\mathbf{\theta}}_0=\boldsymbol{\mathbf{\theta}}_i\)</span></li>
<li>Starting from <span class="math inline">\(\boldsymbol{\mathbf{m}}_{0\mid 0}=\boldsymbol{\mathbf{m}}_0, P_{0\mid0}=\Sigma_0\)</span>, run the <strong>Kalman Filter</strong> to get <span class="math inline">\(\boldsymbol{\mathbf{m}}_{t\mid t}\)</span>, <span class="math inline">\(P_{t\mid t}\)</span>, and <span class="math inline">\(K_t\)</span> for all <span class="math inline">\(t\)</span> <a href="#eq-kalman-update" class="quarto-xref">Equation&nbsp;10</a>,</li>
<li>Starting from <span class="math inline">\(\boldsymbol{\mathbf{m}}_{T\mid T}, P_{T\mid T}\)</span>, run the <strong>Kalman Smoother</strong> to get <span class="math inline">\(\boldsymbol{\mathbf{m}}_{t\mid T}\)</span>, <span class="math inline">\(P_{t\mid T}\)</span>, and <span class="math inline">\(J_t\)</span> for all <span class="math inline">\(t\)</span> (<a href="#eq-kalmansmooth" class="quarto-xref">Equation&nbsp;12</a>),</li>
<li>Starting from <span class="math inline">\(P_{T,T-1\mid T} = (I - K_nA_n) MP_{T-1\mid T-1}\)</span>, run the <strong>Lag-One Smoother</strong> to get <span class="math inline">\(\boldsymbol{\mathbf{m}}_{t,t-1\mid T}\)</span> and <span class="math inline">\(P_{t,t-1\mid T}\)</span> for all <span class="math inline">\(t\)</span> <a href="#eq-lag1smooth" class="quarto-xref">Equation&nbsp;13</a>,</li>
<li>Use the above values to construct <span class="math inline">\(\mathcal Q(\boldsymbol{\mathbf{\theta}};\boldsymbol{\mathbf{\theta}}')\)</span> in <a href="#eq-Q" class="quarto-xref">Equation&nbsp;15</a>,</li>
<li>Maximise the function <span class="math inline">\(\mathcal Q(\boldsymbol{\mathbf{\theta}};\boldsymbol{\mathbf{\theta}}')\)</span> to get a new guess <span class="math inline">\(\boldsymbol{\mathbf{\theta}}_{i+1}\)</span>, then return to step 2,</li>
<li>Stop once a certain criteria is met.</li>
</ol>
</section>



<div id="quarto-appendix" class="default"><section id="appendix" class="level1 appendix" data-number="6"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">6</span> Appendix</h2><div class="quarto-appendix-contents">


<section id="sec-app1" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec-app1"><span class="header-section-number">6.2</span> Proof of <a href="#thm-information_filter" class="quarto-xref">Theorem&nbsp;2</a></h2>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Firstly, for the prediction step, using <span class="math inline">\(S_t = M^{-\intercal}Q_{t\mid t}M^{-1}\)</span> and <span class="math inline">\(J_t = S_t(\Sigma_\eta^{-1} + S_t)^{-1}\)</span> and the identities <a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;16</a> and <a href="#eq-woodbury2" class="quarto-xref">Equation&nbsp;17</a>,</p>
<p><span class="math display">\[\begin{split}
  Q_{t+1\mid t} &amp;= P_{t+1\mid t}^{-1} = (MQ_{t\mid t}^{-1}M^\intercal + \Sigma_\eta)^{-1}\\
  &amp;= S_t - J_t S_t = (I-J_t)S_t,
\end{split}
\]</span></p>
<p>where we used <span class="math inline">\(A=MQ_{t\mid t}^{-1}M^\intercal\)</span>, <span class="math inline">\(C=\Sigma_\eta\)</span> and <span class="math inline">\(U=C=I\)</span> in <a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;16</a>. Thurthermore,</p>
<p><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{\nu}}_{t+1\mid t} &amp;= Q_{t+1\mid t} \boldsymbol{\mathbf{m}}_{t+1\mid t}\\
  &amp;= Q_{t+1\mid t} M Q_{t\mid t}^{-1} \boldsymbol{\mathbf{\nu}}_{t\mid t} = Q_{t+1\mid t} (M Q_{t\mid t}^{-1}) \boldsymbol{\mathbf{\nu}}_{t\mid t}\\
  &amp;= (I-J_t)M^{-\intercal}Q_{t\mid t}M^{-1} (M Q_{t\mid t}^{-1}) \boldsymbol{\mathbf{\nu}}_{t\mid t}\\
  &amp;= (I-J_t)M^{-\intercal} \boldsymbol{\mathbf{\nu}}_{t\mid t}.
\end{split}
\]</span></p>
<p>For the update step,</p>
<p><span class="math display">\[\begin{split}
  Q_{t+1\mid t+1} &amp;= P_{t+1\mid t+1}^{-1}\\
  &amp;= (Q_{t+1}^{-1} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}\Sigma_\epsilon\Phi_{t+1}^\intercal + \Sigma_\epsilon]^{-1}\Phi_{t+1}Q_{t+1\mid t}^{-1})^{-1}\\
  &amp;= ((Q_{t+1\mid t} + \Phi_{t+1}^\intercal\Sigma_\epsilon^{-1}\Phi_{t+1})^{-1})^{-1} = Q_{t+1\mid t} + \Phi_{t+1}^\intercal\Sigma_\epsilon^{-1}\Phi_{t+1}\\
  &amp;= Q_{t+1\mid t} + I_{t+1}.
\end{split}
\]</span></p>
<p>Then, writing <span class="math inline">\(\boldsymbol{\mathbf{m}}_{t+1\mid t+1}\)</span> in terms of <span class="math inline">\(Q_{t+1\mid t}\)</span> and <span class="math inline">\(\boldsymbol{\mathbf{\nu}}_{t+1\mid t}\)</span></p>
<p><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{m}}_{t+1\mid t+1} &amp;= Q_{t+1\mid t}^{-1} \boldsymbol{\mathbf{\nu}}_{t+1\mid t} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1} [\tilde{\boldsymbol{\mathbf{z}}}_{t+1} - \Phi_{t+1}Q_{t+1\mid t}^{-1}\boldsymbol{\mathbf{\nu}}_{t+1\mit t}]\\
  &amp;= (Q_{t+1\mid t}^{-1} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1}\Phi_{t+1}Q_{t+1\mid t}^{-1})\boldsymbol{\mathbf{\nu}}_{t+1\mid t} \\
  &amp;\quad + Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1}\tilde{\boldsymbol{\mathbf{z}}}_{t+1}\\
  &amp;= [Q_{t+1\mid t} + I_{t+1}]^{-1}\boldsymbol{\mathbf{\nu}}_{t+1\mid t}\\
  &amp;\quad + [Q_{t+1\mid t} + I_{t+1}]^{-1}\Phi_{t+1}\Sigma_\epsilon^{-1}\tilde{\boldsymbol{\mathbf{z}}}_{t+1},
\end{split}
\]</span></p>
<p>and now noting that <span class="math inline">\(\boldsymbol{\mathbf{\nu}}_{t+1\mid t+1} = (Q_{t+1\mid t} + I_{t+1}) \boldsymbol{\mathbf{m}}_{t+1\mid t+1}\)</span>, we complete the proof.</p>
</div>
</section>
<section id="sec-vagueprior" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-vagueprior"><span class="header-section-number">6.3</span> Truly Vague Prior with the Kalman Filter</h2>
<p>It has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior <span class="math inline">\(Q_{0}=0\)</span>. While this is true, it is actually possible to do this in the Kalman filter by ‘skipping’ the first step (contrary to some sources, such as the wikipedia page as of January 2025).</p>
<div id="thm-vagueprior" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3</strong></span> In the Kalman Filter (<a href="#sec-kalmanfilter" class="quarto-xref">Section&nbsp;3.1</a>), if we allow <span class="math inline">\(P_{0}^{-1} = 0\)</span>, effectively setting infinite variance, and assuming the propegator matrix <span class="math inline">\(M\)</span> is invertible, we have</p>
<p><span id="eq-kalmanvague"><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{m}}_{1\mid1} &amp;= (\Phi_1^\intercal \Sigma_\epsilon^{-1} \Phi_1)^{-1} \Phi_1 \Sigma_\epsilon^{-1} \tilde{\boldsymbol{\mathbf{z}}}_1,\\
  P_{1\mid1} &amp;= (\Phi_1^\intercal \Sigma_\epsilon^{-1} \Phi_1)^{-1}.
\end{split}
\tag{18}\]</span></span></p>
<p>Therefore, starting with these values then continuing the filter as normal, we can perform the kalman filter with ‘infinite’ prior variance.</p>
<p><span style="color: red;">[NOTE: The requirement that M be invertible should be droppable, see the proof below]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Unsuprisingly, the proof is effectively equivalent to proving the information filter and setting <span class="math inline">\(Q_0 = P_0^{-1}=0\)</span>.</p>
<p>For the first predict step (<a href="#eq-kalman-predict" class="quarto-xref">Equation&nbsp;9</a>),</p>
<p><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{m}}_{1\mid0} &amp;= M \boldsymbol{\mathbf{m}}_0,\\
  P_{1\mid0} &amp;= M P_0 M^\intercal + \Sigma_\eta.
\end{split}
\]</span></p>
<p>By (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;16</a>),</p>
<p><span class="math display">\[\begin{split}
  P_{1\mid0}^{-1} &amp;= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} M (P_0^{-1} + M^\intercal \Sigma_\eta^{-1} M)^{-1}M^\intercal\Sigma_\eta^{-1}\\
  &amp;= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} M (M^\intercal \Sigma_\eta^{-1} M)^{-1}M^\intercal\Sigma_\eta^{-1}\\
  &amp;= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} = 0.
\end{split}
\]</span></p>
<p>So, moving to the update step (<a href="#eq-kalman-update" class="quarto-xref">Equation&nbsp;10</a>),</p>
<p><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{m}}_{1\mid1} = M \boldsymbol{\mathbf{m}}_0 + P_{1\mid0}\Phi_1 [\Phi_1 P_{1\mid0} \Phi_1^\intercal + \Sigma_\epsilon]^{-1}(\tilde{\boldsymbol{\mathbf{z}}}_1 - \Phi M \boldsymbol{\mathbf{m}}_0).\\
\end{split}
\]</span></p>
<p>Applying (<a href="#eq-woodbury2" class="quarto-xref">Equation&nbsp;17</a>) with <span class="math inline">\(A = P_{1\mid0}^{-1}, U=\Phi_1, V=\Phi_1^\intercal, C=\Sigma_\epsilon^{-1}\)</span>,</p>
<p><span class="math display">\[\begin{split}
  \boldsymbol{\mathbf{m}}_{1\mid1} &amp;= M \boldsymbol{\mathbf{m}}_0 + (P_{1\mid0}^{-1} + \Phi_1^\intercal\Sigma_\epsilon^{-1} \Phi_1)^{-1}\Phi_1^\intercal \Sigma_\epsilon^{-1}(\tilde{\boldsymbol{\mathbf{z}}}_1 - \Phi_1 M\boldsymbol{\mathbf{m}}_0)\\
  &amp;= M \boldsymbol{\mathbf{m}}_0 + (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1} \tilde{\boldsymbol{\mathbf{z}}}_1 - (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1M\boldsymbol{\mathbf{m}}_0\\
  &amp;= (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1} \tilde{\boldsymbol{\mathbf{z}}}_1.
\end{split}
\]</span></p>
<p>For the variance, we apply the (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;16</a>) with <span class="math inline">\(A = P_{1\mid0}^{-1}, U=\Phi_1^\intercal, V=\Phi_1, C=\Sigma_\epsilon^{-1}\)</span>,</p>
<p><span class="math display">\[\begin{split}
  P_{1\mid1} &amp;= (I - P_{1\mid0}\Phi_1^\intercal[\Sigma_\epsilon + \Phi_1^\intercal P_{1\mid0}\Phi_1]^{-1}\Phi_1)P_{1\mid0}\\
  &amp;= (P_{1\mid0}^{-1} + \Phi_1^\intercal \Sigma_\epsilon^{-1}\Phi_1)^{-1}\\
  &amp;= (\Phi_1^\intercal \Sigma_\epsilon^{-1}\Phi_1)^{-1},
\end{split}
\]</span></p>
<p>as needed.</p>
</div>
<p>It is worth noting that (<a href="#eq-kalmanvague" class="quarto-xref">Equation&nbsp;18</a>) seems to make a lot of sense; namely, we expect the estimate for <span class="math inline">\(\boldsymbol{\mathbf{m}}_0\)</span> to look like a correlated least squares-type estimator like this.</p>



</section>
</div></section><section id="woodburys-identity" class="level2 appendix" data-number="6.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">6.1</span> Woodbury’s identity</h2><div class="quarto-appendix-contents">

<p>The following two sections will make heavy use of the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury identity</a>.</p>
<div id="lem-woodbury" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2 (Woodbury’s Identity)</strong></span> We have, for conformable matrices <span class="math inline">\(A, U, C, V\)</span>,</p>
<p><span id="eq-woodbury"><span class="math display">\[\begin{split}
(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
\end{split}
\tag{16}\]</span></span></p>
<p>Additionally, we have the variant</p>
<p><span id="eq-woodbury2"><span class="math display">\[\begin{split}
(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.
\end{split}
\tag{17}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We only prove (<a href="#eq-woodbury2" class="quarto-xref">Equation&nbsp;17</a>), since various proofs of (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;16</a>) are well known (see, for example, the wikipedia page).</p>
<p>Simply multipliying (<a href="#eq-woodbury" class="quarto-xref">Equation&nbsp;16</a>) by <span class="math inline">\(CU\)</span>, <span class="citation" data-cites="khan2005matrix">(similar to <a href="#ref-khan2005matrix" role="doc-biblioref">Khan 2005</a>, although there is an error in their proof)</span></p>
<p><span class="math display">\[\begin{split}
(A+UCV)^{-1}UC &amp;= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\
&amp;= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\
&amp;= A^{-1}U(C^{-1}+VA^{-1}U)
\end{split}
\]</span></p>
<p>as needed.</p>
</div>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-assimakis2012information" class="csl-entry" role="listitem">
Assimakis, Nicholas, Maria Adam, and Anargyros Douladiris. 2012. <span>“Information Filter and Kalman Filter Comparison: Selection of the Faster Filter.”</span> In <em>Information Engineering</em>, 2:1–5. 1.
</div>
<div id="ref-cressie2015statistics" class="csl-entry" role="listitem">
Cressie, Noel, and Christopher K Wikle. 2015. <em>Statistics for Spatio-Temporal Data</em>. John Wiley &amp; Sons.
</div>
<div id="ref-dewar2008data" class="csl-entry" role="listitem">
Dewar, Michael, Kenneth Scerri, and Visakan Kadirkamanathan. 2008. <span>“Data-Driven Spatio-Temporal Modeling Using the Integro-Difference Equation.”</span> <em>IEEE Transactions on Signal Processing</em> 57 (1): 83–91.
</div>
<div id="ref-khan2005matrix" class="csl-entry" role="listitem">
Khan, Mohammad Emtiyaz. 2005. <span>“Matrix Inversion Lemma and Information Filter.”</span> <em>Honeywell Techonology Solutions Lab, Bangalore, India</em>.
</div>
<div id="ref-liu2022statistical" class="csl-entry" role="listitem">
Liu, Xiao, Kyongmin Yeo, and Siyuan Lu. 2022. <span>“Statistical Modeling for Spatio-Temporal Data from Stochastic Convection-Diffusion Processes.”</span> <em>Journal of the American Statistical Association</em> 117 (539): 1482–99.
</div>
<div id="ref-shumway2000time" class="csl-entry" role="listitem">
Shumway, Robert H, David S Stoffer, and David S Stoffer. 2000. <em>Time Series Analysis and Its Applications</em>. Vol. 3. Springer.
</div>
<div id="ref-wikle1999dimension" class="csl-entry" role="listitem">
Wikle, Christopher K, and Noel Cressie. 1999. <span>“A Dimension-Reduced Approach to Space-Time Kalman Filtering.”</span> <em>Biometrika</em> 86 (4): 815–29.
</div>
<div id="ref-wikle2019spatio" class="csl-entry" role="listitem">
Wikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019. <em>Spatio-Temporal Statistics with r</em>. CRC Press.
</div>
<div id="ref-zammit2022IDE" class="csl-entry" role="listitem">
Zammit-Mangion, Andrew. 2022. <em>IDE: Integro-Difference Equation Spatio-Temporal Models</em>. <a href="https://CRAN.R-project.org/package=IDE">https://CRAN.R-project.org/package=IDE</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>at least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see <span class="citation" data-cites="liu2022statistical">Liu, Yeo, and Lu (<a href="#ref-liu2022statistical" role="doc-biblioref">2022</a>)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>