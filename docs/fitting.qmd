---
title: "Integro-Difference Equation Models: Fitting (Prototype)"
format:
  html:
    code-fold: true
    toc: true
    mathjax: 
      extensions: ["breqn", "bm"]
jupyter: python3
include-in-header:
  - text: |
      <script>
      window.MathJax = {
        loader: {
          load: ['[tex]/upgreek', '[tex]/boldsymbol', '[tex]/physics', '[tex]/breqn'
        },
        tex: {
          packages: {
            '[+]': ['upgreek', 'boldsymbol', 'physics', 'breqn']
          }
        }
      };
      </script>
bibliography: Bibliography.bib
---

[Index](./index.html)

\DeclareMathOperator{\var}{\mathbb{V}\mathrm{ar}}
\DeclareMathOperator{\cov}{\mathbb{C}\mathrm{ov}}
\renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand\eqc{\stackrel{\mathclap{c}}{=}}

## Target Spatially Invariant Kernel Model

Using ```gen_example_idem``` with the argument ````k_spat_inv=True```, we can easily generate a model to create a synthetic dataset to fit to.


```{python}
import sys
import os
sys.path.append(os.path.abspath('../src/jax_idem'))

import jax
import utilities
import IDEM

from utilities import *
from IDEM import *
import warnings

key = jax.random.PRNGKey(12)
keys = rand.split(key, 3)

process_basis = place_basis(nres=2, min_knot_num=5)
nbasis = process_basis.nbasis

m_0 = jnp.zeros(nbasis).at[16].set(1)
sigma2_0 = 0.001



truemodel = gen_example_idem(
    keys[0], k_spat_inv=True,
    process_basis=process_basis,
    m_0=m_0, sigma2_0=sigma2_0
)

# Simulation
T = 10
                                            
process_data, obs_data = truemodel.simulate(nobs=50, T=T + 1, key=keys[1])


# Plotting
gif_st_grid(process_data, output_file="target_process.gif")
gif_st_pts(obs_data, output_file="synthetic_observations.gif")
plot_kernel(truemodel.kernel, output_file="target_kernel.png")
```

::: {#fig-example layout-ncol=3}

![Process](target_process.gif)

![Observations](synthetic_observations.gif)

![Kernel](target_kernel.png)

An example target simulation, with the underlying process (left), noisy observations, and the direction of 'flow' dictated by the kernel (right).

:::

We now create a 'shell' model, which we will fit to the above data, initialising all relevent parameters.

```{python}
# use the same kernel basis as the true model for now
K_basis = truemodel.kernel.basis
# scale and shape of the kernel will be the same, but the offsets will be estimated
k = (
    jnp.array([150]),
    jnp.array([0.002]),
    jnp.array([0]),
    jnp.array([0]),
)
# This is the kind of kernel used by ```gen_example_idem```
kernel = param_exp_kernel(K_basis, k)

process_basis2 = place_basis(nres=1, min_knot_num=5) # courser process basis with 25 total basis functions
nbasis0 = process_basis2.nbasis

model0 = IDEM(
        process_basis = process_basis2,
        kernel=kernel,
        process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),
        sigma2_eta = truemodel.sigma2_eta,
        sigma2_eps = truemodel.sigma2_eps,
        beta = jnp.array([0, 0, 0]),
        m_0 = jnp.zeros(nbasis0),
        sigma2_0=truemodel.sigma2_0)
```

And we can simulate from this initial model with

```{python}
unfit_process_data, unfit_obs_data = model0.simulate(nobs=1, T=T + 1, key=key)
# Plotting
gif_st_grid(unfit_process_data, output_file="unfit_process.gif")
```

::: {#fig-example-2 layout-ncol=1}

![Unfit Process](unfit_process.gif)

The unfit 'shell' model which we will use as an initial model for fittingto the synthetic 'true' model. As we can see, the numbers are significantly lower, due to the initial value of the process being 0, and no motion being present.

:::

## Filtering (and smoothing)

The first step is to apply the kalman filter to this model, using the data contained in ```obs_data``` from the 'true' model. 
This can be easily do through the functions ```IDEM.filter``` and ```IDEM.smooth```, which output a tuple with the relevant output quantities.
These filtered and smoothed processes can be plotted, and look good. However, given ```model0``` has no movement, unsurprisingly, the likelihood is lower than that of the true model.

```{python}

# Currently, the Kalman filter requires the data to be in wide format.
obs_data_wide = ST_towide(obs_data)
     
# although it is irrelevent for this particular model, we need to put in the covariate matrix into filter
obs_locs = jnp.column_stack((obs_data_wide.x, obs_data_wide.y))
nobs = obs_locs.shape[0]
X_obs = jnp.column_stack([jnp.ones(nobs), obs_locs])

     
ll, ms, Ps, mpreds, Ppreds, Ks = model0.filter(obs_data_wide, X_obs)

# Make this filtered means into an ST_Data_long in the Y space
filt_data = basis_params_to_st_data(ms, model0.process_basis, model0.process_grid)


# We can similarily smooth the model as well
m_tTs, P_tTs, Js = model0.smooth(ms, Ps, mpreds, Ppreds)
smooth_data = basis_params_to_st_data(
    m_tTs, model0.process_basis, model0.process_grid
)

# plot the filtered and smoothed data
gif_st_grid(filt_data, output_file="filtered.gif")
gif_st_grid(smooth_data, output_file="smoothed.gif")

true_ll, _, _, _, _, _ = truemodel.filter(obs_data_wide, X_obs)

print(f"The log likelihood (up to a constant) of the unfit model is {ll}")
print(f"The log likelihood (up to a constant) of the true model is {true_ll}")
```

::: {#fig-example-3 layout-ncol=2}

![Filtered process means](filtered.gif)

![Smoothed process means](smoothed.gif)

(write a description)

:::

## Fitting

We will now fit all the parameters that differ between the true model and ```model0```; these are the kernel 'drift'  parameters, ```IDEM.kernel.parmas[2:3]```, and the initial value of the process basis coefficients, ```IDEM.m_0```.
We can do this simply by creating an objective function which takes these parameters and outputs the negative log-likelihood from the kalman filter.
Since most functions of the project are written with jit and auto-differentiation, we can also get the gradient of this objective.

```{python}
nobs = obs_locs.shape[0]
PHI_obs = model0.process_basis.mfun(obs_locs)
PHI = model0.process_basis.mfun(model0.process_grid.coords)
GRAM = (PHI.T @ PHI) * model0.process_grid.area

# Function to construct the M matrix from the kernel parameters; this will be built in to IDEM in the future
@jax.jit
def con_M(k):
    @jax.jit
    def kernel(s, r):
        theta = (
            k[0] @ model0.kernel.basis[0].vfun(s),
            k[1] @ model0.kernel.basis[1].vfun(s),
            jnp.array(
                [
                    k[2] @ model0.kernel.basis[2].vfun(s),
                    k[3] @ model0.kernel.basis[3].vfun(s),
                ]
            ),
        )
        return theta[0] * jnp.exp(-(jnp.sum((r - s - theta[2]) ** 2)) / theta[1])

    K = outer_op(model0.process_grid.coords, model0.process_grid.coords, kernel)
    return solve(GRAM, PHI.T @ K @ PHI) * model0.process_grid.area**2

@jax.jit
def objective(params):
    m_0 = params[:nbasis0]

    # and the first two kernel params struggle very much to fit
    ks = (jnp.array([150]), jnp.array([0.002]), jnp.array([params[nbasis0]]), jnp.array([params[nbasis0+1]]))
     
    M = con_M(ks)
     
    Sigma_eta = model0.sigma2_eta * jnp.eye(nbasis0)
    Sigma_eps = model0.sigma2_eps * jnp.eye(nobs)
    P_0 = model0.sigma2_0 * jnp.eye(nbasis0)
    
    carry, seq = kalman_filter(
        m_0,
        P_0,
        M,
        PHI_obs,
        Sigma_eta,
        Sigma_eps,
        model0.beta,
        obs_data_wide,
        X_obs,
    )
    return -carry[4]

param0 = jnp.concatenate(
    [model0.m_0, jnp.array([model0.kernel.params[2][0], model0.kernel.params[3][0]])]
)

obj_grad = jax.grad(objective)
     
print("The initial value of the negative log-likelihood is", objective(param0))
print("with gradient", obj_grad(param0))
```

We can then use standard optimisation techniques to optimise. 
For example, using the ADAM optimiser in OPTAX,

```{python}
import optax
     
start_learning_rate = 1e-1
optimizer = optax.adam(start_learning_rate)

param_ad = param0
opt_state = optimizer.init(param_ad)

# A simple update loop.
for i in range(10):
    grad = obj_grad(param_ad)
    updates, opt_state = optimizer.update(grad, opt_state)
    param_ad = optax.apply_updates(param_ad, updates)
    nll = objective(param_ad)

print(param_ad)
```

Putting these parameters into a new model;

```{python}
fitted_m_0 = param_ad[:nbasis0]
fitted_ks = (jnp.array([150]), jnp.array([0.002]), jnp.array([param_ad[nbasis0]]), jnp.array([param_ad[nbasis0+1]]))
fitted_kernel = param_exp_kernel(K_basis, fitted_ks)

fitted_model = IDEM(
        process_basis = process_basis2,
        kernel = fitted_kernel,
        process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),
        sigma2_eta = truemodel.sigma2_eta,
        sigma2_eps = truemodel.sigma2_eps,
        beta = jnp.array([0, 0, 0]),
        m_0 = fitted_m_0,
        sigma2_0=truemodel.sigma2_0)

fit_process_data, fit_obs_data = fitted_model.simulate(nobs=50, T=T + 1, key=key)
gif_st_grid(fit_process_data, output_file="fitted_process.gif")
plot_kernel(fitted_model.kernel, output_file="fitted_kernel.png")
```

::: {#fig-example-3 layout-ncol=2}

![Fitted process simulation](fitted_process.gif)

![Fitted kernel](fitted_kernel.png)

(write description)

:::
