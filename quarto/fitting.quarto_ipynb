{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Integro-Difference Equation Models: Fitting (Prototype)\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    toc: true\n",
        "    mathjax: \n",
        "      extensions: [\"breqn\", \"bm\"]\n",
        "jupyter: python3\n",
        "include-in-header:\n",
        "  - text: |\n",
        "      <script>\n",
        "      window.MathJax = {\n",
        "        loader: {\n",
        "          load: ['[tex]/upgreek', '[tex]/boldsymbol', '[tex]/physics', '[tex]/breqn'\n",
        "        },\n",
        "        tex: {\n",
        "          packages: {\n",
        "            '[+]': ['upgreek', 'boldsymbol', 'physics', 'breqn']\n",
        "          }\n",
        "        }\n",
        "      };\n",
        "      </script>\n",
        "bibliography: Bibliography.bib\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[Index](./index.html)\n",
        "\n",
        "\\DeclareMathOperator{\\var}{\\mathbb{V}\\mathrm{ar}}\n",
        "\\DeclareMathOperator{\\cov}{\\mathbb{C}\\mathrm{ov}}\n",
        "\\renewcommand*{\\vec}[1]{\\boldsymbol{\\mathbf{#1}}}\n",
        "\\newcommand\\eqc{\\stackrel{\\mathclap{c}}{=}}\n",
        "\n",
        "## Target Spatially Invariant Kernel Model\n",
        "\n",
        "Using ```gen_example_idem``` with the argument ````k_spat_inv=True```, we can easily generate a model to create a synthetic dataset to fit to.\n"
      ],
      "id": "d1090e1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('../src/jax_idem'))\n",
        "\n",
        "import jax\n",
        "import utilities\n",
        "import IDEM\n",
        "\n",
        "from utilities import *\n",
        "from IDEM import *\n",
        "import warnings\n",
        "\n",
        "key = jax.random.PRNGKey(12)\n",
        "keys = rand.split(key, 3)\n",
        "\n",
        "process_basis = place_basis(nres=2, min_knot_num=5)\n",
        "nbasis = process_basis.nbasis\n",
        "\n",
        "m_0 = jnp.zeros(nbasis).at[16].set(1)\n",
        "sigma2_0 = 0.001\n",
        "\n",
        "truemodel = gen_example_idem(\n",
        "    keys[0], k_spat_inv=True,\n",
        "    process_basis=process_basis,\n",
        "    m_0=m_0, sigma2_0=sigma2_0\n",
        ")\n",
        "\n",
        "# Simulation\n",
        "T = 10\n",
        "                                            \n",
        "process_data, obs_data = truemodel.simulate(nobs=50, T=T + 1, key=keys[1])\n",
        "\n",
        "\n",
        "# Plotting\n",
        "gif_st_grid(process_data, output_file=\"target_process.gif\")\n",
        "gif_st_pts(obs_data, output_file=\"synthetic_observations.gif\")\n",
        "plot_kernel(truemodel.kernel, output_file=\"target_kernel.png\")"
      ],
      "id": "12fe0f7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example layout-ncol=3}\n",
        "\n",
        "![Process](target_process.gif)\n",
        "\n",
        "![Observations](synthetic_observations.gif)\n",
        "\n",
        "![Kernel](target_kernel.png)\n",
        "\n",
        "An example target simulation, with the underlying process (left), noisy observations, and the direction of 'flow' dictated by the kernel (right).\n",
        "\n",
        ":::\n",
        "\n",
        "We now create a 'shell' model, which we will fit to the above data, initialising all relevent parameters.\n"
      ],
      "id": "f8181728"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "# use the same kernel basis as the true model for now\n",
        "K_basis = truemodel.kernel.basis\n",
        "# scale and shape of the kernel will be the same, but the offsets will be estimated\n",
        "k = (\n",
        "    jnp.array([150]),\n",
        "    jnp.array([0.002]),\n",
        "    jnp.array([0]),\n",
        "    jnp.array([0]),\n",
        ")\n",
        "# This is the kind of kernel used by ```gen_example_idem```\n",
        "kernel = param_exp_kernel(K_basis, k)\n",
        "\n",
        "process_basis2 = place_basis(nres=1, min_knot_num=5) # courser process basis with 25 total basis functions\n",
        "nbasis0 = process_basis2.nbasis\n",
        "\n",
        "model0 = IDEM_model(\n",
        "        process_basis = process_basis2,\n",
        "        kernel=kernel,\n",
        "        process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n",
        "        sigma2_eta = truemodel.sigma2_eta,\n",
        "        sigma2_eps = truemodel.sigma2_eps,\n",
        "        beta = jnp.array([0, 0, 0]),\n",
        "        m_0 = jnp.zeros(nbasis0),\n",
        "        sigma2_0=truemodel.sigma2_0)"
      ],
      "id": "8a66531c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can simulate from this initial model with\n"
      ],
      "id": "93204050"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "unfit_process_data, unfit_obs_data = model0.simulate(nobs=1, T=T + 1, key=key)\n",
        "# Plotting\n",
        "gif_st_grid(unfit_process_data, output_file=\"unfit_process.gif\")"
      ],
      "id": "91e39c3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example-2 layout-ncol=1}\n",
        "\n",
        "![Unfit Process](unfit_process.gif)\n",
        "\n",
        "The unfit 'shell' model which we will use as an initial model for fittingto the synthetic 'true' model. As we can see, the numbers are significantly lower, due to the initial value of the process being 0, and no motion being present.\n",
        "\n",
        ":::\n",
        "\n",
        "## Filtering (and smoothing)\n",
        "\n",
        "The first step is to apply the kalman filter to this model, using the data contained in ```obs_data``` from the 'true' model. \n",
        "This can be easily do through the functions ```IDEM.filter``` and ```IDEM.smooth```, which output a tuple with the relevant output quantities.\n",
        "These filtered and smoothed processes can be plotted, and look good. However, given ```model0``` has no movement, unsurprisingly, the likelihood is lower than that of the true model.\n"
      ],
      "id": "1cae44fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Currently, the Kalman filter requires the data to be in wide format.\n",
        "obs_data_wide = ST_towide(obs_data)\n",
        "     \n",
        "# although it is irrelevent for this particular model, we need to put in the covariate matrix into filter\n",
        "obs_locs = jnp.column_stack((obs_data_wide.x, obs_data_wide.y))\n",
        "nobs = obs_locs.shape[0]\n",
        "X_obs = jnp.column_stack([jnp.ones(nobs), obs_locs])\n",
        "\n",
        "     \n",
        "ll, ms, Ps, mpreds, Ppreds, Ks = model0.filter(obs_data_wide, X_obs)\n",
        "\n",
        "# Make this filtered means into an ST_Data_long in the Y space\n",
        "filt_data = basis_params_to_st_data(ms, model0.process_basis, model0.process_grid)\n",
        "\n",
        "\n",
        "# We can similarily smooth the model as well\n",
        "m_tTs, P_tTs, Js = model0.smooth(ms, Ps, mpreds, Ppreds)\n",
        "smooth_data = basis_params_to_st_data(\n",
        "    m_tTs, model0.process_basis, model0.process_grid\n",
        ")\n",
        "# plot the filtered and smoothed data\n",
        "gif_st_grid(filt_data, output_file=\"filtered.gif\")\n",
        "gif_st_grid(smooth_data, output_file=\"smoothed.gif\")\n",
        "\n",
        "true_ll, _, _, _, _, _ = truemodel.filter(obs_data_wide, X_obs)\n",
        "\n",
        "print(f\"The log likelihood (up to a constant) of the unfit model is {ll}\")\n",
        "print(f\"The log likelihood (up to a constant) of the true model is {true_ll}\")"
      ],
      "id": "a448305f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example-3 layout-ncol=2}\n",
        "\n",
        "![Filtered process means](filtered.gif)\n",
        "\n",
        "![Smoothed process means](smoothed.gif)\n",
        "\n",
        "(write a description)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Fitting\n",
        "\n",
        "We will now fit all the parameters that differ between the true model and ```model0```; these are the kernel 'drift'  parameters, ```IDEM.kernel.parmas[2:3]```, and the initial value of the process basis coefficients, ```IDEM.m_0```.\n",
        "We can do this simply by creating an objective function which takes these parameters and outputs the negative log-likelihood from the kalman filter.\n",
        "Since most functions of the project are written with jit and auto-differentiation, we can also get the gradient of this objective.\n"
      ],
      "id": "23290889"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "nobs = obs_locs.shape[0]\n",
        "PHI_obs = model0.process_basis.mfun(obs_locs)\n",
        "PHI = model0.process_basis.mfun(model0.process_grid.coords)\n",
        "GRAM = (PHI.T @ PHI) * model0.process_grid.area\n",
        "\n",
        "# Function to construct the M matrix from the kernel parameters; this will be built in to IDEM in the future\n",
        "@jax.jit\n",
        "def con_M(k):\n",
        "    @jax.jit\n",
        "    def kernel(s, r):\n",
        "        theta = (\n",
        "            k[0] @ model0.kernel.basis[0].vfun(s),\n",
        "            k[1] @ model0.kernel.basis[1].vfun(s),\n",
        "            jnp.array(\n",
        "                [\n",
        "                    k[2] @ model0.kernel.basis[2].vfun(s),\n",
        "                    k[3] @ model0.kernel.basis[3].vfun(s),\n",
        "                ]\n",
        "            ),\n",
        "        )\n",
        "        return theta[0] * jnp.exp(-(jnp.sum((r - s - theta[2]) ** 2)) / theta[1])\n",
        "\n",
        "    K = outer_op(model0.process_grid.coords, model0.process_grid.coords, kernel)\n",
        "    return solve(GRAM, PHI.T @ K @ PHI) * model0.process_grid.area**2\n",
        "\n",
        "@jax.jit\n",
        "def objective(params):\n",
        "    m_0 = params[\"m_0\"]\n",
        "\n",
        "    # and the first two kernel params struggle very much to fit\n",
        "    ks = (jnp.array([150]), jnp.array([0.002]), jnp.array([params[\"k1\"]]), jnp.array([params[\"k2\"]]))\n",
        "     \n",
        "    M = model0.con_M(ks)\n",
        "     \n",
        "    Sigma_eta = model0.sigma2_eta * jnp.eye(nbasis0)\n",
        "    Sigma_eps = model0.sigma2_eps * jnp.eye(nobs)\n",
        "    P_0 = model0.sigma2_0 * jnp.eye(nbasis0)\n",
        "    \n",
        "    carry, seq = kalman_filter(\n",
        "        m_0,\n",
        "        P_0,\n",
        "        M,\n",
        "        PHI_obs,\n",
        "        Sigma_eta,\n",
        "        Sigma_eps,\n",
        "        model0.beta,\n",
        "        obs_data_wide.z,\n",
        "        X_obs,\n",
        "    )\n",
        "    return -carry[4]\n",
        "\n",
        "#param0 = jnp.concatenate(\n",
        "#    [model0.m_0, jnp.array([model0.kernel.params[2][0], model0.kernel.params[3][0]])]\n",
        "#)\n",
        "\n",
        "param0 = {\"m_0\": model0.m_0,\n",
        "          \"k1\": jnp.array(0.0),\n",
        "          \"k2\": jnp.array(0.0)}\n",
        "\n",
        "obj_grad = jax.grad(objective)\n",
        "     \n",
        "print(\"The initial value of the negative log-likelihood is\", objective(param0))\n",
        "print(\"with gradient\", obj_grad(param0))"
      ],
      "id": "e26605d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then use standard optimisation techniques to optimise. \n",
        "For example, using the ADAM optimiser in OPTAX,\n"
      ],
      "id": "a495ec72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "import optax\n",
        "     \n",
        "start_learning_rate = 1e-1\n",
        "optimizer = optax.adam(start_learning_rate)\n",
        "\n",
        "param_ad = param0\n",
        "opt_state = optimizer.init(param_ad)\n",
        "\n",
        "# A simple update loop.\n",
        "for i in range(10):\n",
        "    grad = obj_grad(param_ad)\n",
        "    updates, opt_state = optimizer.update(grad, opt_state)\n",
        "    param_ad = optax.apply_updates(param_ad, updates)\n",
        "    nll = objective(param_ad)\n",
        "\n",
        "print(param_ad)"
      ],
      "id": "46e78d53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting these parameters into a new model;\n"
      ],
      "id": "a753e01f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "fitted_m_0 = param_ad[\"m_0\"]\n",
        "fitted_ks = (jnp.array([150]), jnp.array([0.002]), jnp.array([param_ad[\"k1\"]]), jnp.array([param_ad[\"k2\"]]))\n",
        "fitted_kernel = param_exp_kernel(K_basis, fitted_ks)\n",
        "\n",
        "fitted_model = IDEM(\n",
        "        process_basis = process_basis2,\n",
        "        kernel = fitted_kernel,\n",
        "        process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n",
        "        sigma2_eta = truemodel.sigma2_eta,\n",
        "        sigma2_eps = truemodel.sigma2_eps,\n",
        "        beta = jnp.array([0, 0, 0]),\n",
        "        m_0 = fitted_m_0,\n",
        "        sigma2_0=truemodel.sigma2_0)\n",
        "\n",
        "fit_process_data, fit_obs_data = fitted_model.simulate(nobs=50, T=T + 1, key=key)\n",
        "gif_st_grid(fit_process_data, output_file=\"fitted_process.gif\")\n",
        "plot_kernel(fitted_model.kernel, output_file=\"fitted_kernel.png\")"
      ],
      "id": "ff86ab22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example-3 layout-ncol=2}\n",
        "\n",
        "![Fitted process simulation](fitted_process.gif)\n",
        "\n",
        "![Fitted kernel](fitted_kernel.png)\n",
        "\n",
        "(write description)\n",
        "\n",
        ":::\n",
        "\n",
        "# Optimising over all parameters, with constraints\n",
        "\n",
        "Above, we only fitted the data to the initial process coefficients and the two offset terms in the kernel. \n",
        "We actually fixed all the variances, and the scale and shape of the kernel. \n",
        "Ideally, we want to be able to fit for those too.\n",
        "Of course, these parameters are constrained; each one is non negative, and it may be worth bounding them above too to avoid anything going up indefinitely.\n",
        "\n",
        "To ensure all optimsers work in a similar way, OPTAX uses projections to handle this kind of constraints. Firstly, lets re-write some of the above code to also include the other parameters.\n"
      ],
      "id": "fa78e777"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "k = (\n",
        "    jnp.array([100.0]),\n",
        "    jnp.array([0.001]),\n",
        "    jnp.array([0.0]),\n",
        "    jnp.array([0.0]),\n",
        ")\n",
        "# This is the kind of kernel used by ```gen_example_idem```\n",
        "kernel = param_exp_kernel(K_basis, k)\n",
        "\n",
        "model1 = IDEM(\n",
        "        process_basis = process_basis2,\n",
        "        kernel=kernel,\n",
        "        process_grid = create_grid(jnp.array([[0, 1], [0, 1]]), jnp.array([41, 41])),\n",
        "        sigma2_eta = 0.01,\n",
        "        sigma2_eps = 0.01,\n",
        "        beta = jnp.array([0.0, 0.0, 0.0]),\n",
        "        m_0 = jnp.zeros(nbasis0),\n",
        "        sigma2_0=0.01)\n",
        "# a model with inaccurate 'guesses'\n",
        "v_unfit_process_data, v_unfit_obs_data = model1.simulate(nobs=1, T=T + 1, key=key)\n",
        "# Plotting\n",
        "gif_st_grid(v_unfit_process_data, output_file=\"very_unfit_process.gif\")\n",
        "\n",
        "ll, _, _, _, _, _ = model1.filter(obs_data_wide, X_obs)\n",
        "print(ll)"
      ],
      "id": "bca96119",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example-4 layout-ncol=1}\n",
        "\n",
        "![Very unfit process](very_unfit_process.gif)\n",
        "\n",
        "(write description)\n",
        "\n",
        ":::\n",
        "\n",
        "Presumably due to the higher variances, the log likelihood here is higher that that of ```model0```, but still much short of the true model.\n",
        "Making the optimisation function,\n"
      ],
      "id": "40d22582"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "@jax.jit\n",
        "def objective(params):\n",
        "    m_0, sigma2_0, sigma2_eta, sigma2_eps, ks = params\n",
        "\n",
        "    #sigma2_0, sigma2_eta, sigma2_eps = truemodel.sigma2_0, truemodel.sigma2_eta, truemodel.sigma2_eps\n",
        "    #ks = (ks[0], ks[1], ks[2], ks[3])\n",
        "    M = con_M(ks)\n",
        "     \n",
        "    Sigma_eta = sigma2_eta * jnp.eye(nbasis0)\n",
        "    Sigma_eps = sigma2_eps * jnp.eye(nobs)\n",
        "    P_0 = sigma2_0 * jnp.eye(nbasis0)\n",
        "    \n",
        "    carry, seq = kalman_filter(\n",
        "        m_0,\n",
        "        P_0,\n",
        "        M,\n",
        "        PHI_obs,\n",
        "        Sigma_eta,\n",
        "        Sigma_eps,\n",
        "        model0.beta,\n",
        "        obs_data_wide.z,\n",
        "        X_obs,\n",
        "    )\n",
        "    return -carry[4]\n",
        "\n",
        "obj_grad = jax.grad(objective)\n",
        "\n",
        "params0 = (model1.m_0,\n",
        "           model1.sigma2_0,\n",
        "           model1.sigma2_eta,\n",
        "           model1.sigma2_eps,\n",
        "           model1.kernel.params)"
      ],
      "id": "6a6fb6c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we naively apply what we did before, we will get nans as some variances become negative, causing some cholesky decomposition to fail. \n",
        "We can remedy this by defining a bounding box for the data;\n"
      ],
      "id": "cd9c2501"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "lower = (jnp.full(nbasis0, -jnp.inf),\n",
        "         jnp.array(0.0),\n",
        "         jnp.array(0.0),\n",
        "         jnp.array(0.0),\n",
        "         (jnp.array(0.0), jnp.array(0.0), jnp.array(-jnp.inf), jnp.array(-jnp.inf)))\n",
        "\n",
        "upper = (jnp.full(nbasis0, jnp.inf),\n",
        "         jnp.array(0.1),\n",
        "         jnp.array(0.1), \n",
        "         jnp.array(0.1),\n",
        "         (jnp.array(500.0), jnp.array(0.01), jnp.array(jnp.inf), jnp.array(jnp.inf)))\n",
        "\n",
        "# with this many parameters, must use a lower starting learning rate\n",
        "start_learning_rate = 1e-3\n",
        "optimizer = optax.adam(start_learning_rate)\n",
        "\n",
        "params_ad = params0\n",
        "opt_state = optimizer.init(params_ad)\n",
        "\n",
        "# A simple update loop.\n",
        "for i in range(10):\n",
        "    grad = obj_grad(params_ad)\n",
        "    updates, opt_state = optimizer.update(grad, opt_state)\n",
        "    params_ad = optax.apply_updates(params_ad, updates)\n",
        "    params_ad = optax.projections.projection_box(params_ad, lower, upper)\n",
        "    nll = objective(params_ad)\n",
        "\n",
        "print(params_ad)"
      ],
      "id": "cf84f3ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can take some time, so this code is not run interactively here. Instead, there are some scripts included here to run the built-in methods containing the logic above.\n",
        "\n",
        "There is now a built in method for fitting this kind of model, ```IDEM.fit```. Reading some data (generated from the R-IDE package) and formatting it with ```panda```,\n"
      ],
      "id": "6c3e4b41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"z_obs.csv\")\n",
        "\n",
        "date_mapping = {date: i-1 for i, date in enumerate(df['time'].unique(), 1)}\n",
        "df['time'] = df['time'].map(date_mapping)"
      ],
      "id": "6732e4dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example-5 layout-ncol=2}\n",
        "\n",
        "![Fitted process simulation](new_fitted_process.gif)\n",
        "\n",
        "![Fitted kernel](new_fitted_kernel.png)\n",
        "\n",
        "(write description)\n",
        ":::\n",
        "\n",
        "\n",
        "# Information Filter\n",
        "\n",
        "We can write the Kalman filter in a different, and possibly more useful, form; called the information filter.\n",
        "This uses, instead of means and variances, the information matrices and vectors, $Q_{k\\mid l} = P_{k\\mid l}^{-1}$$ and $$\\nu_{k\\mid l} = Q_{k\\mid l}m_{k\\mid l}$, respectively.\n",
        "See the [subheading on this in the Mathematics page](./mathematics.html) for more detail.\n",
        "\n",
        "Theoretically, these are identical. \n",
        "However, this form allows to more easily begin with infinite variance ($Q_0 = 0$), which is possible with the kalman filter, but reuires skipping the first step with analytically obtained results.\n",
        "The information filter also accounts for time-variyng data better (changing number of observation locations, as well as those observations changing locations), since it requires a scan over vectors in the (always constant) state space, as opposed tot he changing data space.\n",
        "\n",
        "Let's start by generating some data with much more random observation locations.\n"
      ],
      "id": "e0904627"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "key = jax.random.PRNGKey(1)\n",
        "keys = rand.split(key, 3)\n",
        "\n",
        "# We'll re-use truemodel form before, but simulate from different locations\n",
        "\n",
        "nobs = jax.random.randint(keys[1], (T,), 50, 101)\n",
        "\n",
        "locs_keys = jax.random.split(keys[2], T)\n",
        "\n",
        "obs_locs = jnp.vstack(\n",
        "            [\n",
        "                jnp.column_stack(\n",
        "                    [\n",
        "                        jnp.repeat(t + 1, n),\n",
        "                        rand.uniform(\n",
        "                            locs_keys[t],\n",
        "                            shape=(n, 2),\n",
        "                            minval=0,\n",
        "                            maxval=1,\n",
        "                        ),\n",
        "                    ]\n",
        "                )\n",
        "                for t, n in enumerate(nobs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# Simulation, but this time providing a custom value for obs_locs\n",
        "process_data, obs_data = truemodel.simulate(\n",
        "            T=T, key=keys[1], obs_locs=obs_locs\n",
        "        )\n",
        "\n",
        "# Plotting\n",
        "gif_st_grid(process_data, output_file=\"target_process_2.gif\")\n",
        "gif_st_pts(obs_data, output_file=\"synthetic_observations_2.gif\")"
      ],
      "id": "f76afdb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example layout-ncol=2}\n",
        "\n",
        "![Process](target_process_2.gif)\n",
        "\n",
        "![Observations](synthetic_observations_2.gif)\n",
        "\n",
        "An example target simulation, with the underlying process (left), noisy observations (right), this time randomly variying in number and positions.\n",
        "\n",
        ":::\n",
        "\n",
        "The function  ```kalman_filter``` has no support for these kinds of observation, but ```information_filter``` does;\n"
      ],
      "id": "143ee5ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nbasis = truemodel.process_basis.nbasis\n",
        "nu_0 = jnp.zeros(nbasis)\n",
        "Q_0 = jnp.zeros((nbasis, nbasis)) # infinite variance!\n",
        "\n",
        "obs_locs = jnp.column_stack(\n",
        "           jnp.column_stack((obs_data.x, obs_data.y))\n",
        ").T\n",
        "\n",
        "X_obs = jnp.column_stack([jnp.ones(obs_locs.shape[0]), obs_locs[:, -2:]])\n",
        "\n",
        "nus, Qs = truemodel.filter_information(\n",
        "           obs_data,\n",
        "           X_obs,\n",
        "           nu_0,\n",
        "           Q_0,\n",
        ")"
      ],
      "id": "99b33090",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, the $\\nu_{t}$ values which are the output of the information filter need a little more work to visualise like the means of the Kalman filter, so let's use a ```jnp.solve``` to extract the means;\n"
      ],
      "id": "1f7851a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ms = jnp.linalg.solve(Qs, nus)\n",
        "print(ms.shape)\n",
        "print(Qs)\n",
        "print(ms[0])\n",
        "\n",
        "filt_data = basis_params_to_st_data(ms, truemodel.process_basis, truemodel.process_grid)\n",
        "\n",
        "gif_st_grid(filt_data, output_file=\"filtered_2.gif\")"
      ],
      "id": "4c36dfbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {#fig-example-80orsomething layout-ncol=1}\n",
        "\n",
        "![Filtered process means](filtered_2.gif)\n",
        "\n",
        "(write a description)\n",
        "\n",
        ":::"
      ],
      "id": "416de982"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/tate/MyProjects/JAX-IDEM/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}