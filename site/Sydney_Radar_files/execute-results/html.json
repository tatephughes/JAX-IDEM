{
  "hash": "ecf6a7d3e7eee385a17035e939377503",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Sydney Radar Data\"\nauthor: \"Evan Tate Paterson Hughes\"\nformat:\n  html:\n    code-fold: false\n    toc: true\n    inclue-in-header: header.html\n    mathjax: \n      extensions: [\"breqn\", \"bm\", \"ams\"]\njupyter: python3\nbibliography: Bibliography.bib\n---\n\n\n::: {.content-visible unless-format=\"pdf\"}\n\n[Index](../index.html)\n\n:::\n\n# The Sydney Radar Data Set\n\nThis is a data set... [more information here and plotting here]\n\n# Importing the relevant packages and Loading the data\n\nFirstly, we load the relevant libraries and import the data.\n\n::: {#8494e7c9 .cell execution_count=1}\n``` {.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nimport jax.random as rand\nimport pandas as pd\n\nimport jaxidem.idem as idem\nimport jaxidem.utils as utils\n\n\nradar_df = pd.read_csv('../data/radar_df.csv')\n```\n:::\n\n\nWe should put this data into `jax-idem`s `st_data` type;\n\n::: {#b3f2486b .cell execution_count=2}\n``` {.python .cell-code}\nradar_data = utils.pd_to_st(radar_df, 's2', 's1', 'time', 'z')\n```\n:::\n\n\nWe now create an initial model for this data\n\n::: {#14a6492b .cell execution_count=3}\n``` {.python .cell-code}\nmodel = idem.init_model(data=radar_data)\n```\n:::\n\n\nThis, by default, creates an invariant kernel model with no covariates beside an intercept, with cosine basis function for the process decomposition.\nWe can now get the marginal data likelihood function of this model with the `get_log_like` method;\n\n::: {#3893d8e4 .cell execution_count=4}\n``` {.python .cell-code}\nlog_marginal = model.get_log_like(radar_data, method=\"sqinf\", likelihood='partial')\n```\n:::\n\n\nWe can then use this function to do various inference techniques, like direclty maximising it or Bayesian MCMC methods. It is auto-differentiation compatible, so can easily be dropped into packages like `optax` or `blackjax` for these purposes.\n\nThe function takes, as an input, an object of type `IdemParams`, which is a `NamedTuple` containing the variances `sigma2_eps` and `sigma2_eta`\n\n::: {#2d3e2081 .cell execution_count=5}\n``` {.python .cell-code}\nidem.print_params(model.params)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameters:\n  sigma2_eps: 49.88551330566406\n  sigma2_eta: 49.88551330566406\n  Kernel Parameters:\n    Scale: [150.00001525878906]\n    Shape: [0.001999999862164259]\n    Offset X: [0.0]\n    Offset Y: [0.0]\n  beta: [0.0]\n```\n:::\n:::\n\n\n## Maximum Likelihood Estimation\n\nOnce we have this marginal likelihood, there are a few ways to progress.\nA good start is with a maximum likelihood method.\nObviously, we can no just take this lgo marginal function and maximise it in any way we see fit, but `jaxidem.Model` has a built-in method for this, `Model.fit_mle`. Given data, this will use a method from 'optax' to create a new output model with the fitted parameters.\n\n::: {#e75dacfb .cell execution_count=6}\n``` {.python .cell-code}\nimport optax\n\n\nfit_model_mle, mle_params = model.fit_mle(radar_data,\n                                          optimizer = optax.adam(1e-2),\n                                          max_its = 100,\n                                          method = 'sqinf')\n```\n:::\n\n\n\n\nThe resulting parameters are then\n\n::: {#cd4facec .cell execution_count=8}\n``` {.python .cell-code}\nidem.print_params(mle_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameters:\n  sigma2_eps: 5.723726272583008\n  sigma2_eta: 28.266475677490234\n  Kernel Parameters:\n    Scale: [0.08538345247507095]\n    Shape: [3.7510576248168945]\n    Offset X: [-5.437947750091553]\n    Offset Y: [-1.7626336812973022]\n  beta: [0.42389795184135437]\n```\n:::\n:::\n\n\nOf course, we can use any other method to maximmise this.\nWe can update the model with new parameters using the method `Model.update`, and `utils.flatten_and_unflatten` (see documentation) allows working with flat arrays instead of PyTrees if needed.\n\n## Simple Random-Walk MCMC\n\nOf course, for Bayesian analysis, we furthermore want to be able to sample from the posterior.\nNow, we will use a basic random walk RMH (Rosenbluth-Metropolis-Hastings, often just called Metropolis-Hastings) to sample from the models posterior.\n\n\nFirstly, in order to esaily handle everything, we will flatten the parameters into a single 1D JAX array.\nThe functions `jaxidem.utils.flatten` and `jaxidem.utils.unflatten` do this easily;\n\n::: {#a51da9b0 .cell execution_count=9}\n``` {.python .cell-code}\nfparams, unflat = utils.flatten_and_unflatten(model.params)\nprint(fparams)\nidem.print_params(unflat(fparams))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 3.9097307  3.9097307  5.0106354 -6.214608   0.         0.\n  0.       ]\nParameters:\n  sigma2_eps: 49.88551330566406\n  sigma2_eta: 49.88551330566406\n  Kernel Parameters:\n    Scale: [150.00001525878906]\n    Shape: [0.001999999862164259]\n    Offset X: [0.0]\n    Offset Y: [0.0]\n  beta: [0.0]\n```\n:::\n:::\n\n\nNow we can initialise a chain with variance 1 for each parameter\n\n::: {#f69f7e62 .cell execution_count=10}\n``` {.python .cell-code}\ninit_mean = fparams\n\n# initial run gave the following for estimated optimal tuning\nprop_var = jnp.array([0.16133152, 0.00453646, 0.01214727, 0.392362, 0.789936, 0.41011548, 0.14044523])\n```\n:::\n\n\nNow sampling from the proposal Gaussian distribution is as simple as\n\n::: {#2a0e395a .cell execution_count=11}\n``` {.python .cell-code}\nrng_key = jax.random.PRNGKey(1)\nparshape = init_mean.shape\nnpars = parshape[0]\nprint(init_mean + jax.random.normal(rng_key, shape=parshape) * jnp.sqrt(prop_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 3.8476994   3.915436    4.9956484  -6.311721    1.125793    0.09497016\n  0.80257165]\n```\n:::\n:::\n\n\nAnd finally, we can sample a chain with RMH as follows;\n\n::: {#5cdb5f1c .cell execution_count=12}\n``` {.python .cell-code}\nback_key, sample_key = jax.random.split(rng_key, 2)\n\nn = 100\n\nsample_keys = jax.random.split(sample_key, n)\n\ncurrent_state = init_mean        \nrmh_sample = [current_state]\naccepted = 0\n\nfor i in tqdm(range(n), desc=\"Sampling... \"):\n    current_state = rmh_sample[-1]\n    prop_key, acc_key = jax.random.split(sample_keys[i], 2)\n\n    proposal = current_state + jax.random.normal(prop_key, shape=parshape) * jnp.sqrt(init_vars)\n    r = log_marginal(unflat(proposal)) - log_marginal(unflat(current_state))\n    log_acc_prob = min((jnp.array(0.0), r))\n    if jnp.log(jax.random.uniform(acc_key)) > log_acc_prob:\n        rmh_sample.append(current_state)\n    else:\n        accepted = accepted + 1\n        rmh_sample.append(proposal)\n\nacc_ratio = accepted/n\n```\n:::\n\n\n\n\n::: {#def237f8 .cell execution_count=14}\n``` {.python .cell-code}\nprint(acc_ratio)\npost_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):]), axis=0)\npost_params_mean = unflat(post_mean)\nidem.print_params(post_params_mean)\nprint(log_marginal(post_params_mean)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06055\nParameters:\n  sigma2_eps: 5.692798614501953\n  sigma2_eta: 28.267518997192383\n  Kernel Parameters:\n    Scale: [0.09685702621936798]\n    Shape: [3.2974460124969482]\n    Offset X: [-5.449050426483154]\n    Offset Y: [-1.7590761184692383]\n  beta: [0.4384661912918091]\nnan\n```\n:::\n:::\n\n\n::: {#d2201c15 .cell execution_count=15}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\n\nsamples = jnp.array(rmh_sample[1000:])\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Sydney_Radar_files/figure-html/cell-16-output-1.png){width=948 height=1282}\n:::\n:::\n\n\n## MALA\n\nEven when tuned, the mixing of these chains leaves much to be desired.\nSince `jaxidem` supports JAX's autodifferentiation, we can easily incorporate the gradient into a MCMC chain using MALA.\n\n::: {#ef32a57e .cell execution_count=16}\n``` {.python .cell-code}\nprop_sd = 0.008\n\naccepted = 0\nlmvn = jax.scipy.stats.multivariate_normal.logpdf\n\nback_key, sample_key = jax.random.split(back_key, 2)\n\nsample_keys = jax.random.split(sample_key, mala_n)\n\n\nll_val_grad = jax.value_and_grad(lambda par: log_marginal(par))\n\n# start from the end of the last chain\nmala_sample = init_mean]\n\nfor i in tqdm(range(mala_n), desc=\"Sampling... \"):\n    current_state = mala_sample[-1]\n    prop_key, acc_key = jax.random.split(sample_keys[i], 2)\n\n    val, grad = ll_val_grad(unflat(current_state))\n    grad, _ = utils.flatten_and_unflatten(grad)\n\n    mean = 0.5* prop_sd**2 * grad + current_state\n\n    proposal = (mean + prop_sd * jax.random.normal(prop_key, shape=parshape))\n\n    r = (log_marginal(unflat(proposal)) - val\n         + lmvn(current_state, mean, prop_sd*jnp.eye(7)) - lmvn(proposal, mean, prop_sd*jnp.eye(7)))\n    log_acc_prob = min((jnp.array(0.0), r))\n        \n    if jnp.log(jax.random.uniform(acc_key)) > log_acc_prob:\n        mala_sample.append(current_state)\n    else:\n        accepted = accepted + 1\n        mala_sample.append(proposal)\n\nacc_ratio = accepted/mala_n\n```\n:::\n\n\n\n\n::: {#03ce6699 .cell execution_count=18}\n``` {.python .cell-code}\nprint(acc_ratio)\npost_mean = jnp.mean(jnp.array(mala_sample[int(len(mala_sample)/3):]), axis=0)\npost_params_mean = unflat(post_mean)\nidem.print_params(post_params_mean)\nprint(log_marginal(post_params_mean)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.02675\nParameters:\n  sigma2_eps: 5.694046974182129\n  sigma2_eta: 28.345043182373047\n  Kernel Parameters:\n    Scale: [0.0813666507601738]\n    Shape: [4.000741958618164]\n    Offset X: [-5.447969436645508]\n    Offset Y: [-1.7537591457366943]\n  beta: [0.1484980583190918]\nnan\n```\n:::\n:::\n\n\n::: {#c601ba10 .cell execution_count=19}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nsamples = jnp.array(mala_sample[1000:])\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Sydney_Radar_files/figure-html/cell-20-output-1.png){width=942 height=1282}\n:::\n:::\n\n\n## Using Blackjax\n\nFrom there, it is easy to sample from the posterior\n\n::: {#f386a460 .cell execution_count=20}\n``` {.python .cell-code}\nkey = jax.random.PRNGKey(1) # PRNG key\ninverse_mass_matrix = jnp.ones(model.nparams)\nnum_integration_steps = 10\nstep_size = 1e-5\nsample, _ = model.sample_posterior(key,\n                                   n=10,\n                                   burnin=0,\n                                   obs_data=radar_data,\n                                   X_obs=[X_obs for _ in range(T)],\n                                   inverse_mass_matrix=inverse_mass_matrix,\n                                   num_integration_steps=num_integration_steps,\n                                   step_size = step_size,\n                                   likelihood_method=\"sqinf\",)\n```\n:::\n\n\nThis initial run will likely mix poorly and have a low acceptance rate. Taking this initial sample, we can fit new (gaussian) priors on the paramterers and use a new mass matrix for the sampling based on the initial sample's variance.\n\n::: {#5d7069be .cell execution_count=21}\n``` {.python .cell-code}\npost_mean = jax.tree.map(lambda x: jnp.mean(x, axis=0), sample.position)\npost_var = jax.tree.map(lambda x: jnp.var(x, axis=0), sample.position)\n\ndef log_prior_density(param):\n\n    (\n        log_sigma2_eta,\n        log_sigma2_eps,\n        ks,\n        beta,\n    ) = param\n\n    logdens_log_sigma2_eta = jax.scipy.stats.norm.logpdf(log_sigma2_eta, loc = post_mean[0], scale=post_var[0])\n    logdens_log_sigma2_eps = jax.scipy.stats.norm.logpdf(log_sigma2_eps, loc = post_mean[1], scale=post_var[1])\n\n    logdens_ks1 = jax.scipy.stats.norm.logpdf(ks[0], post_mean[2][0], post_var[2][0])\n    logdens_ks2 = jax.scipy.stats.norm.logpdf(ks[1], post_mean[2][1], post_var[2][1])\n    logdens_ks3 = jax.scipy.stats.multivariate_normal.logpdf(ks[2], post_mean[2][2], jnp.diag(post_var[2][2]))\n    logdens_ks4 = jax.scipy.stats.multivariate_normal.logpdf(ks[3], post_mean[2][3], jnp.diag(post_var[2][3]))\n\n    logdens_beta = jax.scipy.stats.multivariate_normal.logpdf(beta, post_mean[3], jnp.diag(post_var[3]))\n    return logdens_log_sigma2_eta+logdens_log_sigma2_eps+logdens_ks1+logdens_ks2+logdens_ks3+logdens_ks4+logdens_beta\n\n\ninverse_mass_matrix = jnp.array(jax.tree.flatten(post_var)[0])\n```\n:::\n\n\n::: {#f8aad989 .cell execution_count=22}\n\n::: {.cell-output .cell-output-stdout}\n```\nParameters:\n  sigma2_eps: 47.1029052734375\n  sigma2_eta: 32.85206604003906\n  Kernel Parameters:\n    Scale: 0.021949168294668198\n    Shape: 13.138100624084473\n    Offset X: -0.09490437060594559\n    Offset Y: -0.020940084010362625\n  beta: 0.008254376240074635\n```\n:::\n:::\n\n\n::: {#8fcebc8a .cell execution_count=23}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nsamples = jnp.array(hmc_sample_array[1000:])\n\n# Number of parameters (columns in the array)\nnum_params = samples.shape[1]\n\n# Create a figure and axes for the stacked trace plots\nfig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n\n# Plot each parameter's trace\nfor i in range(num_params):\n    axes[i].plot(samples[:, i], lw=0.8, color='b')\n    axes[i].set_ylabel(f'Parameter {i+1}')\n    axes[i].grid(True)\n\n# Label the x-axis for the last subplot\naxes[-1].set_xlabel('Iteration')\n\n# Add a title to the entire figure\nfig.suptitle('Trace Plots', fontsize=16, y=0.95)\n\n# Adjust spacing\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Sydney_Radar_files/figure-html/cell-24-output-1.png){width=950 height=1282}\n:::\n:::\n\n\n",
    "supporting": [
      "Sydney_Radar_files"
    ],
    "filters": [],
    "includes": {}
  }
}