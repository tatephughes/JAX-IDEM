{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Sydney Radar Data\"\n",
        "author: \"Evan Tate Paterson Hughes\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "    inclue-in-header: header.html\n",
        "    mathjax: \n",
        "      extensions: [\"breqn\", \"bm\", \"ams\"]\n",
        "jupyter: python3\n",
        "bibliography: Bibliography.bib\n",
        "---\n",
        "\n",
        "\n",
        "::: {.content-visible unless-format=\"pdf\"}\n",
        "\n",
        "[Index](../index.html)\n",
        "\n",
        ":::\n",
        "\n",
        "# The Sydney Radar Data Set\n",
        "\n",
        "This is a data set... [more information here and plotting here]\n",
        "\n",
        "# Importing the relevant packages and Loading the data\n",
        "\n",
        "Firstly, we load the relevant libraries and import the data.\n"
      ],
      "id": "016321e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rand\n",
        "import pandas as pd\n",
        "\n",
        "import jaxidem.idem as idem\n",
        "import jaxidem.utils as utils\n",
        "\n",
        "\n",
        "radar_df = pd.read_csv('../data/radar_df.csv')"
      ],
      "id": "2b7a3cf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should put this data into `jax-idem`s `st_data` type;\n"
      ],
      "id": "2b119fd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "radar_data = utils.pd_to_st(radar_df, 's2', 's1', 'time', 'z')"
      ],
      "id": "b1744da7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create an initial model for this data\n"
      ],
      "id": "395f8d50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "model = idem.init_model(data=radar_data)"
      ],
      "id": "703edcb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This, by default, creates an invariant kernel model with no covariates beside an intercept, with cosine basis function for the process decomposition.\n",
        "We can now get the marginal data likelihood function of this model with the `get_log_like` method;\n"
      ],
      "id": "f8e9601e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_marginal = model.get_log_like(radar_data, method=\"sqinf\", likelihood='partial')"
      ],
      "id": "7cac62dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then use this function to do various inference techniques, like direclty maximising it or Bayesian MCMC methods. It is auto-differentiation compatible, so can easily be dropped into packages like `optax` or `blackjax` for these purposes.\n",
        "\n",
        "The function takes, as an input, an object of type `IdemParams`, which is a `NamedTuple` containing the variances `sigma2_eps` and `sigma2_eta`\n"
      ],
      "id": "0d25042f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "\n",
        "idem.print_params(model.params)"
      ],
      "id": "8fdc93a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Maximum Likelihood Estimation\n",
        "\n",
        "Once we have this marginal likelihood, there are a few ways to progress.\n",
        "A good start is with a maximum likelihood method.\n",
        "Obviously, we can no just take this lgo marginal function and maximise it in any way we see fit, but `jaxidem.Model` has a built-in method for this, `Model.fit_mle`. Given data, this will use a method from 'optax' to create a new output model with the fitted parameters.\n"
      ],
      "id": "e8d770a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "import optax\n",
        "\n",
        "\n",
        "fit_model_mle, mle_params = model.fit_mle(radar_data,\n",
        "                                          optimizer = optax.adam(1e-2),\n",
        "                                          max_its = 100,\n",
        "                                          method = 'sqinf')"
      ],
      "id": "3b272525",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "import pickle\n",
        "with open('./pickles/Hamilton/24_4_25/mle_params.pkl', 'rb') as file: \n",
        "    mle_params = pickle.load(file)\n",
        "fit_model_mle = model.update(mle_params)"
      ],
      "id": "9de6c7ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting parameters are then\n"
      ],
      "id": "13b3e7d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "idem.print_params(mle_params)"
      ],
      "id": "5d020150",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, we can use any other method to maximmise this.\n",
        "We can update the model with new parameters using the method `Model.update`, and `utils.flatten_and_unflatten` (see documentation) allows working with flat arrays instead of PyTrees if needed.\n",
        "\n",
        "## Simple Random-Walk MCMC\n",
        "\n",
        "Of course, for Bayesian analysis, we furthermore want to be able to sample from the posterior.\n",
        "Now, we will use a basic random walk RMH (Rosenbluth-Metropolis-Hastings, often just called Metropolis-Hastings) to sample from the models posterior.\n",
        "\n",
        "\n",
        "Firstly, in order to esaily handle everything, we will flatten the parameters into a single 1D JAX array.\n",
        "The functions `jaxidem.utils.flatten` and `jaxidem.utils.unflatten` do this easily;\n"
      ],
      "id": "ae0748f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fparams, unflat = utils.flatten_and_unflatten(model.params)\n",
        "print(fparams)\n",
        "idem.print_params(unflat(fparams))"
      ],
      "id": "773b806c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can initialise a chain with variance 1 for each parameter\n"
      ],
      "id": "74cf7adc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "init_mean = fparams\n",
        "\n",
        "# initial run gave the following for estimated optimal tuning\n",
        "prop_var = jnp.array([0.16133152, 0.00453646, 0.01214727, 0.392362, 0.789936, 0.41011548, 0.14044523])"
      ],
      "id": "93744311",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now sampling from the proposal Gaussian distribution is as simple as\n"
      ],
      "id": "d6eb210a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng_key = jax.random.PRNGKey(1)\n",
        "parshape = init_mean.shape\n",
        "npars = parshape[0]\n",
        "print(init_mean + jax.random.normal(rng_key, shape=parshape) * jnp.sqrt(prop_var))"
      ],
      "id": "42710c5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, we can sample a chain with RMH as follows;\n"
      ],
      "id": "2931ac60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "back_key, sample_key = jax.random.split(rng_key, 2)\n",
        "\n",
        "n = 100\n",
        "\n",
        "sample_keys = jax.random.split(sample_key, n)\n",
        "\n",
        "current_state = init_mean        \n",
        "rmh_sample = [current_state]\n",
        "accepted = 0\n",
        "\n",
        "for i in tqdm(range(n), desc=\"Sampling... \"):\n",
        "    current_state = rmh_sample[-1]\n",
        "    prop_key, acc_key = jax.random.split(sample_keys[i], 2)\n",
        "\n",
        "    proposal = current_state + jax.random.normal(prop_key, shape=parshape) * jnp.sqrt(init_vars)\n",
        "    r = log_marginal(unflat(proposal)) - log_marginal(unflat(current_state))\n",
        "    log_acc_prob = min((jnp.array(0.0), r))\n",
        "    if jnp.log(jax.random.uniform(acc_key)) > log_acc_prob:\n",
        "        rmh_sample.append(current_state)\n",
        "    else:\n",
        "        accepted = accepted + 1\n",
        "        rmh_sample.append(proposal)\n",
        "\n",
        "acc_ratio = accepted/n"
      ],
      "id": "098c5411",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "with open('./pickles/Hamilton/24_4_25/rmh_sample.pkl', 'rb') as file:\n",
        "    rmh_sample, acc_ratio = pickle.load(file)"
      ],
      "id": "1846cee0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(acc_ratio)\n",
        "post_mean = jnp.mean(jnp.array(rmh_sample[int(len(rmh_sample)/3):]), axis=0)\n",
        "post_params_mean = unflat(post_mean)\n",
        "idem.print_params(post_params_mean)\n",
        "print(log_marginal(post_params_mean)) "
      ],
      "id": "25331974",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "\n",
        "samples = jnp.array(rmh_sample[1000:])\n",
        "\n",
        "# Number of parameters (columns in the array)\n",
        "num_params = samples.shape[1]\n",
        "\n",
        "# Create a figure and axes for the stacked trace plots\n",
        "fig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n",
        "\n",
        "# Plot each parameter's trace\n",
        "for i in range(num_params):\n",
        "    axes[i].plot(samples[:, i], lw=0.8, color='b')\n",
        "    axes[i].set_ylabel(f'Parameter {i+1}')\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Label the x-axis for the last subplot\n",
        "axes[-1].set_xlabel('Iteration')\n",
        "\n",
        "# Add a title to the entire figure\n",
        "fig.suptitle('Trace Plots', fontsize=16, y=0.95)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
        "plt.show()"
      ],
      "id": "190b680a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MALA\n",
        "\n",
        "Even when tuned, the mixing of these chains leaves much to be desired.\n",
        "Since `jaxidem` supports JAX's autodifferentiation, we can easily incorporate the gradient into a MCMC chain using MALA.\n"
      ],
      "id": "10589b7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "prop_sd = 0.008\n",
        "\n",
        "accepted = 0\n",
        "lmvn = jax.scipy.stats.multivariate_normal.logpdf\n",
        "\n",
        "back_key, sample_key = jax.random.split(back_key, 2)\n",
        "\n",
        "sample_keys = jax.random.split(sample_key, mala_n)\n",
        "\n",
        "\n",
        "ll_val_grad = jax.value_and_grad(lambda par: log_marginal(par))\n",
        "\n",
        "# start from the end of the last chain\n",
        "mala_sample = init_mean]\n",
        "\n",
        "for i in tqdm(range(mala_n), desc=\"Sampling... \"):\n",
        "    current_state = mala_sample[-1]\n",
        "    prop_key, acc_key = jax.random.split(sample_keys[i], 2)\n",
        "\n",
        "    val, grad = ll_val_grad(unflat(current_state))\n",
        "    grad, _ = utils.flatten_and_unflatten(grad)\n",
        "\n",
        "    mean = 0.5* prop_sd**2 * grad + current_state\n",
        "\n",
        "    proposal = (mean + prop_sd * jax.random.normal(prop_key, shape=parshape))\n",
        "\n",
        "    r = (log_marginal(unflat(proposal)) - val\n",
        "         + lmvn(current_state, mean, prop_sd*jnp.eye(7)) - lmvn(proposal, mean, prop_sd*jnp.eye(7)))\n",
        "    log_acc_prob = min((jnp.array(0.0), r))\n",
        "        \n",
        "    if jnp.log(jax.random.uniform(acc_key)) > log_acc_prob:\n",
        "        mala_sample.append(current_state)\n",
        "    else:\n",
        "        accepted = accepted + 1\n",
        "        mala_sample.append(proposal)\n",
        "\n",
        "acc_ratio = accepted/mala_n"
      ],
      "id": "8c9885c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "with open('./pickles/Hamilton/24_4_25/mala_sample.pkl', 'rb') as file:\n",
        "    mala_sample, acc_ratio = pickle.load(file)"
      ],
      "id": "e2f5f243",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(acc_ratio)\n",
        "post_mean = jnp.mean(jnp.array(mala_sample[int(len(mala_sample)/3):]), axis=0)\n",
        "post_params_mean = unflat(post_mean)\n",
        "idem.print_params(post_params_mean)\n",
        "print(log_marginal(post_params_mean)) "
      ],
      "id": "b1c04af8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples = jnp.array(mala_sample[1000:])\n",
        "\n",
        "# Number of parameters (columns in the array)\n",
        "num_params = samples.shape[1]\n",
        "\n",
        "# Create a figure and axes for the stacked trace plots\n",
        "fig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n",
        "\n",
        "# Plot each parameter's trace\n",
        "for i in range(num_params):\n",
        "    axes[i].plot(samples[:, i], lw=0.8, color='b')\n",
        "    axes[i].set_ylabel(f'Parameter {i+1}')\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Label the x-axis for the last subplot\n",
        "axes[-1].set_xlabel('Iteration')\n",
        "\n",
        "# Add a title to the entire figure\n",
        "fig.suptitle('Trace Plots', fontsize=16, y=0.95)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "id": "882d7a95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Posterior Sampling\n",
        "\n",
        "It is obviously desirable to use MCMC methods in order to sample from the posterior distribution.\n",
        "This can be done manually using the lgo likelihood, or by using the method `Model.sample_posterior`. \n",
        "We need to provide it with a sampling kernel; that is, a method to get from one state to the next.\n",
        "Methods from the package BlackJAX, such as `hmc` can be used as follows;\n"
      ],
      "id": "96c95215"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Â¬ eval: false\n",
        "imm = jnp.ones(7) # inverse mass matrix\n",
        "num_int = 20 # number of integration steps\n",
        "step_size = 1e-3 # step size\n",
        "samp = blackjax.hmc(log_marginal, step_size, imm, num_int)\n",
        "\n",
        "step=samp.step\n",
        "init = samp.init(model.params)\n",
        "        \n",
        "def sampling_kernel(carry, i):\n",
        "    key = jax.random.fold_in(rng_key, i)\n",
        "    new_state, info = step(key, carry)\n",
        "    return new_state, (new_state, info)\n",
        "\n",
        "sample, info = model.sample_posterior(key, radar_data, n=100, burnin=100, init=init,\n",
        "                                      sampling_kernel=sampling_kernel)"
      ],
      "id": "2465c20d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Blackjax\n",
        "\n",
        "From there, it is easy to sample from the posterior\n"
      ],
      "id": "58baecd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "key = jax.random.PRNGKey(1) # PRNG key\n",
        "inverse_mass_matrix = jnp.ones(model.nparams)\n",
        "num_integration_steps = 10\n",
        "step_size = 1e-5\n",
        "sample, _ = model.sample_posterior(key,\n",
        "                                   n=10,\n",
        "                                   burnin=0,\n",
        "                                   obs_data=radar_data,\n",
        "                                   X_obs=[X_obs for _ in range(T)],\n",
        "                                   inverse_mass_matrix=inverse_mass_matrix,\n",
        "                                   num_integration_steps=num_integration_steps,\n",
        "                                   step_size = step_size,\n",
        "                                   likelihood_method=\"sqinf\",)"
      ],
      "id": "0c52ec93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This initial run will likely mix poorly and have a low acceptance rate. Taking this initial sample, we can fit new (gaussian) priors on the paramterers and use a new mass matrix for the sampling based on the initial sample's variance.\n"
      ],
      "id": "e13236fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "post_mean = jax.tree.map(lambda x: jnp.mean(x, axis=0), sample.position)\n",
        "post_var = jax.tree.map(lambda x: jnp.var(x, axis=0), sample.position)\n",
        "\n",
        "def log_prior_density(param):\n",
        "\n",
        "    (\n",
        "        log_sigma2_eta,\n",
        "        log_sigma2_eps,\n",
        "        ks,\n",
        "        beta,\n",
        "    ) = param\n",
        "\n",
        "    logdens_log_sigma2_eta = jax.scipy.stats.norm.logpdf(log_sigma2_eta, loc = post_mean[0], scale=post_var[0])\n",
        "    logdens_log_sigma2_eps = jax.scipy.stats.norm.logpdf(log_sigma2_eps, loc = post_mean[1], scale=post_var[1])\n",
        "\n",
        "    logdens_ks1 = jax.scipy.stats.norm.logpdf(ks[0], post_mean[2][0], post_var[2][0])\n",
        "    logdens_ks2 = jax.scipy.stats.norm.logpdf(ks[1], post_mean[2][1], post_var[2][1])\n",
        "    logdens_ks3 = jax.scipy.stats.multivariate_normal.logpdf(ks[2], post_mean[2][2], jnp.diag(post_var[2][2]))\n",
        "    logdens_ks4 = jax.scipy.stats.multivariate_normal.logpdf(ks[3], post_mean[2][3], jnp.diag(post_var[2][3]))\n",
        "\n",
        "    logdens_beta = jax.scipy.stats.multivariate_normal.logpdf(beta, post_mean[3], jnp.diag(post_var[3]))\n",
        "    return logdens_log_sigma2_eta+logdens_log_sigma2_eps+logdens_ks1+logdens_ks2+logdens_ks3+logdens_ks4+logdens_beta\n",
        "\n",
        "\n",
        "inverse_mass_matrix = jnp.array(jax.tree.flatten(post_var)[0])"
      ],
      "id": "8ec140b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "with open('./pickles/Hamilton/24_4_25/hmc_sample.pkl', 'rb') as file:\n",
        "    hmc_sample, hmc_info = pickle.load(file)\n",
        "\n",
        "idem.print_params(jax.tree.map(jnp.mean, hmc_sample.position))\n",
        "hmc_sample_array = jnp.column_stack([hmc_sample.position[0],\n",
        "                                     hmc_sample.position[1],\n",
        "                                     hmc_sample.position[2][0],\n",
        "                                     hmc_sample.position[2][1],\n",
        "                                     hmc_sample.position[2][2].reshape((10000,)),\n",
        "                                     hmc_sample.position[2][3].reshape((10000,)),\n",
        "                                     hmc_sample.position[3],])"
      ],
      "id": "8713a444",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples = jnp.array(hmc_sample_array[1000:])\n",
        "\n",
        "# Number of parameters (columns in the array)\n",
        "num_params = samples.shape[1]\n",
        "\n",
        "# Create a figure and axes for the stacked trace plots\n",
        "fig, axes = plt.subplots(num_params, 1, figsize=(10, 2 * num_params), sharex=True)\n",
        "\n",
        "# Plot each parameter's trace\n",
        "for i in range(num_params):\n",
        "    axes[i].plot(samples[:, i], lw=0.8, color='b')\n",
        "    axes[i].set_ylabel(f'Parameter {i+1}')\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Label the x-axis for the last subplot\n",
        "axes[-1].set_xlabel('Iteration')\n",
        "\n",
        "# Add a title to the entire figure\n",
        "fig.suptitle('Trace Plots', fontsize=16, y=0.95)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "id": "69147e51",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/tate/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}