---
title: "Efficient Filtering and Fitting of Models Derived from Integro-Difference Equations"
author: "Evan Tate Paterson Hughes"
format:
  html:
    code-fold: true
    toc: true
    include-in-header: header.html
    mathjax: 
      extensions: ["breqn", "bm", "ams"]
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    keep-tex: true
    include-in-header: header.tex
    documentclass: report
    geometry:
      - left=20.0mm
      - right=20.0mm
      - marginparsep=7.7mm
      - marginparwidth=70.3mm
      - top=20mm
    code-block-bg: "#EEEEEE"
jupyter: python3
bibliography: Bibliography.bib
---


\newtheorem{prototheorem}{Theorem}[section]

\renewenvironment{theorem}
   {\begin{tcolorbox}[leftrule=3mm,colback=red!10!white,colframe=red!75!black,breakable]\begin{prototheorem}}
   {\end{prototheorem}\end{tcolorbox}}

\newtheorem{protolemma}[prototheorem]{Lemma}
\renewenvironment{lemma}
   {\begin{tcolorbox}[leftrule=3mm,colback=purple!10!white,colframe=purple!75!black, breakable]\begin{protolemma}}
   {\end{protolemma}\end{tcolorbox}}
   
\newtheorem{protoproof}{Proof}
\renewenvironment{proof}
   {\begin{tcolorbox}[frame hidden, enhanced, colback=black!10!white, left=3mm,right=3mm,borderline vertical={4pt}{0pt}{gray},arc=8pt, center, width = 0.9\textwidth, breakable]\begin{protoproof}}
   {\end{protoproof}\end{tcolorbox}}
   
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[leftrule=3mm, colback={codebgcolor}, enhanced, breakable, width=0.9\textwidth, center]}{\end{tcolorbox}}\fi


::: {.content-visible unless-format="pdf"}
[Index](../index.html)
:::


# Integro-difference Based Dynamics

As common and widespread as the problem is, spatio-temporal modelling still presents a great deal of difficulty. Inherently, Spatio-Temporal datasets are almost always high-dimensional, and repeated observations are usually not possible.

Traditionally, the problem has been tackled by the moments (usually the means and covariances) of the process in order to make inference (@wikle2019spatio, for example, call this 'descriptive' modelling).
While this method can be sufficient for many problems, there are many cases where we are underutilizing some knowledge of the underlying dynamical systems involved.
For instance, in temperature models, we know that temperature has movement (convection) and spread (diffusion), and that the state at any given time will depend on its state at previous times ^[at least, in a discrete-time scenario. Integro-difference based mechanics can be derived from continuous-time convection-diffusion processes, see @liu2022statistical].
We call models which make use of this 'dynamical' models. 
Of focus here is the Integro-Difference Equation Model (IDEM), which models diffusion and convection in discrete time by using convolution-like integral equations to model the relation between the process and it's previous state. 
We use a hierarchical model to represent this type of system;

$$\begin{split}
Z_t(\bv s) &= Y_t(\bv s) + X(\bv s)^{\intercal}\bv \beta + \epsilon_t(\bv s)\\
Y_{t+1}(\bv s) &= \int_{\mathcal D_s} \kappa(s,r) Y_t(r) d\bv r + \omega_t(\bv s). 
\end{split}
$${#eq-IDEM}

Where $\omega_t(\bv s)$ is a small scale gaussian variation with no temporal dynamics [@cressie2015statistics call this a 'spatially descriptive' component], $\bv X(\bv s)$ are spatially varying covariates (for example, in a large-scale climate scenario, this might simply be latitude), $Z$ is observed data, $Y$ is an unobserved dynamic process, $\kappa$ is the driving 'kernel' function, and $\epsilon_t$ is a gaussian white noise 'measurement error' term.

# Process Decomposition

In order to work with the process, we likely want to consider the spectral decomposition of it. 
That is, choose a complete class of spatial spectral basis functions, $\phi_i(\bv s)$, and decompose;

$$\begin{split}
Y_t(\bv s) \approx \sum_{i=1}^{r} \alpha_{i,t} \phi_i(\bv s).
\end{split}
$${#eq-processdecomp}

where we truncate the expansion at some $r\in\mathbb N$. Notice that we can write this in vector/matrix form; considering times $t=1,2,\dots, T$, we set

$$\begin{split}
\bv \phi(\bv s) &= (\phi_1(\bv s), \phi_2(\bv s), \dots, \phi_r(\bv s))^{\intercal}\\
\bv \alpha_t &= (\alpha_{1,t}, \alpha_{2,t}, \dots, \alpha_{r, t})^{\intercal}
\end{split}
$${#eq-vecmats}

Now, (@eq-processdecomp) gives us

$$\begin{split}
Y(\bv s; t) &= \bv \phi^{\intercal}(\bv s)  \alpha(t)\\
\end{split}
$${#eq-pbvec}


We now want to find the equation defining the evolution of the spectral coefficients, $\bv \alpha_t$.

::: {#thm-state_form}
## Basis form of the state evolution
 
Define the Gram matrix;

$$\Psi := \int_{\mathcal D_s} \bv \phi(\bv s) \bv \phi(\bv s)^\intercal d\bv s
$${#eq-gram}

Then, the basis coefficients evolve by the equation
 
$$\bv \alpha(t+1) = M \bv\alpha(t) + \bv\eta_t,
$$

where $M = \Psi^{-1} \int\int \bv\phi(\bv s) \kappa(\bv s, \bv r)\bv\phi(\bv r)^\intercal d\bv r d \bv s$ and $\bv\eta_t =\Psi^{-1} \int \bv \phi(\bv s)\omega_t(s)d\bv s$.
:::

\newpage

::: {.proof}
[Adapting from @dewar2008data], write out the process equation, (@eq-IDEM), using the first equation of (@eq-pbvec);

$$Y(\bv s;t+1) = \bv \phi(\bv s) \alpha(t+1) = \int_{\mathcal D_s} \kappa(\bv s, \bv r) \bv\phi(\bv r)^{\intercal}\bv \alpha(t)d\bv r + \omega_t(\bv s),
$$

We then multiply both sides by $\bv \phi(s)$ and integrate over $\bv s$

$$\begin{split}
\int_{\mathcal D_s} \bv\phi(\bv s)\bv\phi(\bv s) d\bv s \bv\alpha(t+1) &= \int\bv\phi(\bv s)\int \kappa(\bv s, \bv r)\bv\phi(\bv r)^\intercal d\bv r  d \bv s\ \bv\alpha(t) + \int \bv \phi(\bv s)\omega_t(s)d\bv s\\
\Psi \bv\alpha(t+1) &= \int\int \bv\phi(\bv s)\kappa(\bv s, \bv r) \bv\phi(\bv r)^\intercal d\bv r d \bv s\ \bv\alpha(t) + \int \bv \phi(\bv s)\omega_t(s)d\bv s.
\end{split}
$$

So, finally, pre-multipling by the inverse of the gram matrix, $\Psi^{-1}$ (@eq-gram), we arrive at the result. \qed
:::



## Process Noise

We still have to set out what the process noise, $\omega_t(\bv s)$, and it's spectral couterpart, $\bv \eta_t$, are. 
Dewar [@dewar2008data] fixes the variance of $\omega_t(\bv s)$ to be uniform and uncorrelated across space and time, with $\omega_t(\bv s) \sim \mathcal N(0,\sigma^2)$
It is then easily shown that $\bv\eta_t$ is also normal, with $\bv\eta_t \sim \mathcal N(0, \sigma^2\Psi^{-1})$.

However, in practice, we simulate in the spectral domain; that is, if we want to keep things simple, it would make sense to specify (and fit) the distribution of $\bv\eta_t$, and compute the variance of $\omega_t(\bv s)$ if needed. 
This is exactly what the IDE package [@zammit2022IDE] in R does, and, correspondingly, what this JAX project does.



::: {#lem-omegadist}

Let $\bv\eta_t \sim \mathcal N(0, \Sigma_\eta)$, and $\cov[\bv\eta_t, \bv \eta_{t+\tau}] =0$, $\forall \tau>0$. 
Then $\omega_t(\bv s)$ has covariance

$$\cov [\omega_t(\bv s), \omega_{t+\tau}(\bv r)] = \begin{cases}
\bv\phi(\bv s)^\intercal \Sigma_\eta \bv\phi(\bv r) & \text{if }\tau=0\\
0 & \text{else}\\
\end{cases}
$$
:::



::: {.proof}

Consider $\Psi \bv\eta_t$.
It is clearly normal, with expectation zero and variance (using (@eq-gram)),

$$\begin{split}
\var[\Psi \bv\eta_t] &= \Psi \var[\bv\omega_t] \Psi^\intercal = \Psi\Sigma_\eta\Psi^\intercal,\\
&= \int_{\mathcal D_s} \bv\phi(\bv s) \bv\phi(\bv s)^\intercal d\bv s \  \Sigma_\eta \ \int_{\mathcal D_s} \bv\phi(\bv r) \bv\phi(\bv r)^\intercal d\bv r\\
&=  \int\int_{\mathcal D_s^2} \bv\phi(\bv s) \bv\phi(\bv s)^\intercal \  \Sigma_\eta \  \bv\phi(\bv r) \bv\phi(\bv r)^\intercal d\bv r d\bv s\\
\end{split}
$${#eq-var1}

Since it has zero expectation, we also have

$$\begin{split}
\var[\Psi\bv\eta_t] &= \mathbb E[(\Psi\bv\eta_t) (\Psi\bv\eta_t)^\intercal] = \mathbb E[\Psi\bv\eta_t\bv\eta_t^\intercal\Psi^\intercal]\\
&= \mathbb E \left[ \int_{\mathcal D_s} \bv\phi(\bv s)\omega_t(\bv s)d\bv s \int_{\mathcal D_s} \bv \phi(\bv r)^\intercal \omega_t(\bv r) d\bv r \right]\\
&= \int\int_{\mathcal D_s^2} \bv\phi(\bv s)\  \mathbb E[\omega_t(\bv s)\omega_t(\bv r)]\  \bv \phi(\bv r)^\intercal d\bv s d \bv r.
\end{split} 
$${#eq-var2}

We can see that, comparing (@eq-var1) and (@eq-var2), we have

$$\cov [\omega_t(\bv s), \omega_t(\bv r)] = \mathbb E[\omega_t(\bv s)\omega_t(\bv r)]= \bv\phi(\bv s)^\intercal \Sigma_\eta \bv\phi(\bv r).
$$
\qed
:::


## Kernel Decomposition

Next is the key part od the system, which defines the dynamics; the kernelf function, $\kappa$.
There are a few ways to handle the kernel. 
One of the most obvious is to expand it out into a spectral decomposition as well;

$$\kappa \approx \sum_i \beta_i\psi(\bv s, \bv r).
$$

This can allow for a wide range of interestingly shaped kernel functions, but see how these basis functions must now act on $\mathbb R^2\times \mathbb R^2$; to get a wide enough space of possible functions, we would likely need many terms of the basis expansion. 

A much simpler approach would be to simply parameterise the kernel function, to $\kappa(\bv s, \bv r, \bv \theta_\kappa)$. 
We then establish a simple shape for the kernel (e.g. Gaussian) and rely on very few parameters (for example, scale, shape, offsets). 
The example kernel used in the program is aGaussian kernel;

$$\kappa(\bv s, \bv r; \bv m, a, b) = a \exp \left( -\frac{1}{b} \vert \bv s- \bv r +\bv m\vert^2 \right)
$$

Of course, this kernel lacks spatial dependance.
We can add spatial variance back in in a nice way by adding dependance on $\bv s$ to the parameters, for example, variyng the offset term as $\bv m(\bv s)$.
Of course, now we are back to having entire functions as parameters, but taking the spectral decomposition of the parameters we actually want to be spatially variant seems like a reasonable middle ground [@cressie2015statistics].
The actual parameters of such a spatially-variant kernel are then the basis coefficients for the expansion of any spatially variant parameters, as well as any constant parameters.

## IDEM as a linear dynamical system

To recap, we have discretized space in such a way that the Integro-difference model is of a more traditional linear dynamical system form.
All that is left is to include our observations in our system.

Lets assume that at each time $t$ there are $n_t$ observations at locations $\bv s_{1,t},\dots, \bv s_{n_{t},t}$.
We write the vector of the process at these points as $\bv Y(t) = (Y(s_{1,t};t), \dots, Y(s_{n_{t},t};t))^\intercal$, and, in it's expanded form $\bv Y_t = \Phi_t \bv\alpha_t$, where $\Phi \in \mathbb R^{r\times n_{t}}$ is

$$\begin{split}
\{\Phi_{t}\}_{i, j} = \phi_{i}(s_{j,t}).
\end{split}
$$

For the covariates, we write the matrix $X_t = (\bv X(\bv s_{1, t}), \dots, \bv X(\bv s_{1=n_{t}, t})^\intercal$.
We then have

$$\begin{split}
\bv Z_t &= \Phi \alpha_t + X_{t} \bv \beta + \bv \epsilon_t, \quad t=0,1,\dots, T,\\
\bv \alpha_{t+1} &= M\bv \alpha_t + \bv\eta_t,\quad t = 1,2,\dots, T,\\
M &= \int_{\mathcal D_s}\bv\phi(\bv s) \bv\phi(\bv s)^\intercal d\bv s \int_{\mathcal D_s^2}\bv\phi(\bv s) \kappa(\bv s, \bv r; \bv\theta_\kappa)\bv\phi(\bv r)^\intercal d\bv r d \bv s,
\end{split}
$$

Writing $\tilde{\bv{Z}}_t = \bv Z_t - X_t \bv \beta$,

$$\begin{split}
\tilde{\bv Z}_t &= \Phi_{t} \bv \alpha_t + \bv \epsilon_t,\quad &t = 1,2,\dots, T,\\
\bv \alpha_{t+1} &= M \bv \alpha_t + \bv\eta_t,\quad &t = 0,1, \dots, T.\\
\end{split}
$$ {#eq-ldstm}

We should also initialise $\bv \alpha_0 \sim \mathcal N^{r}(\bv m_{0}, \Sigma_{0})$, and fix simple distrubtions to the noise terms,

$$\begin{split}
\epsilon_{t,i} \overset{\mathrm{iid}}{\sim} \mathcal N(0,\sigma^2_\epsilon),\\
\eta_{t,i} \overset{\mathrm{iid}}{\sim} \mathcal N(0,\sigma^2_\eta),
\end{split}
$$

which are (also) independant in time.

As in, for example, [@wikle1999dimension], this is now in a traditional enough form that the Kalman filter can be applied to filter and compute many necessary quantities for inference, including the marginal likelihood.
We can use these quantities in either an EM algorithm or a Bayesian approach. Firstly, we cover the EM algorithm applied to this system.

At most, the parameters to be estimated are

$$\begin{split}
\bv\theta = \left(\bv\theta_\kappa^\intercal, \bv\beta^\intercal, \bv m_0^\intercal, \sigma^{2}_{\epsilon}, \sigma^{2}_{\eta}, \mathrm{vec}[\Sigma_0]\right),
\end{split}
$$

where the $\mathrm{vec}[\cdot]$ operator gives the elements of the matrix in a column vector.
Of course, in practice, some of these may be estimated outside of the algorithm, fixed, given a much simpler form (e.g. $\Sigma_\eta = \sigma_\eta^2 I_d$), etc.

There are two approaches we can make from here; directly maximising the marginal data likelihood using only the Kalman filter, or maximising the full likelihood with the EM algorithm.

Now (@eq-ldstm) is of the very familar linear dynamical system (LDS) type.
This is a well-understood problem, and optimal state estimation can be done using the kalman filter and (RTS) smoother. 

# Filtering, Forecasting and Maximum Likelihood Estimation

The Kalman filter gives us linear estimates for the distribution of $\bv\alpha_r\mid \{Z_t\}_{t=0,...,r}$ in any dynamical system like @eq-ldstm.
For full discussions and proofs of the Kalman filter, see, for example, [@shumway2000time].


## The Kalman Filter {#sec-kalmanfilter}

Firstly, we should establish some notation.
Write

$$\begin{split}
m_{r \mid s} &= \mathbb E[\bv\alpha_r \mid \{Z_t\}_{t=0,\dots,s}]\\
P_{r \mid s} &= \var[\bv\alpha_r \mid \{Z_t\}_{t=0,\dots,s}]\\
P_{r,q \mid s} &= \cov[\bv\alpha_r, \bv\alpha_q \mid \{Z_t\}_{t=0,\dots,s}].
\end{split}
$$

For the initial terms, $m_{0\mid0}=m_0$ and $P_{0\mid0}=\Sigma_0$.
For convenience and generality, we write $\Sigma_\eta$ and $\Sigma_\epsilon$ for the variance matrices of the process and observations.
Note that, if the number of observations change at each time point (for example, due to missing data), then $\Sigma_\epsilon$ should be time variyng; we could either always keep it as uncorrelated so that $\Sigma_\epsilon = \mathrm{diag} (\sigma_\epsilon^2)$, or perhaps put some kind of distance-dependant covariance function to it.

To move the filter forward, that is, given $m_{r\mid s}$ and $P_{r\mid s}$, to get $m_{t+1\mid t+1}$ and $P_{t+1\mid t+1}$, we first _predict_

$$\begin{split}
\bv m_{t+1\mid t} &= M \bv m_{t\mid t}\\
P_{t+1\mid t} &= M P_{t\mid t} M^\intercal + \Sigma_\eta,
\end{split}
$${#eq-kalman-predict}

then we add our new information, $z_{t}$, adjusted for the _Kalman gain_;

$$\begin{split}
\bv m_{t+1\mid t+1} &= \bv m_{t+1\mid t} + K_{t+1} \bv e_{t+1}\\
P_{t+1\mid t+1} &= [I- K_{t+1}\Phi_{t+1}]P_{t+1\mid t}
\end{split} 
$${#eq-kalman-update}

where $K_{t+1}$ is the _Kalman gain_;

$$\begin{split}
K_{t+1} = P_{t+1\mid t}\Phi_{t+1}^\intercal [\Phi_{t+1} P_{t+1\mid t} \Phi_{t+1}^\intercal + \Sigma_\epsilon]^{-1}, \quad t=0,\dots,T-1
\end{split}
$$

and $\bv e_{t+1}$ are the _prediction errors_

$$\begin{split}
\bv e_{t+1} = \tilde{\bv z}_{t+1}-\Phi_{t+1} \bv m_{t+1\mid t}, \quad t=1,\dots,T 
\end{split}
$$

Starting with $m_{0\mid0} = m_0$ and $P_{0\mid0} =\Sigma_0$, we can then iteratively move across the data to eventually compute $m_{T\mid T}$ and $P_{T\mid T}$. 

Assuming Gaussian all random variables here are Guassian, this is the optimal mean-square estimators for these quantities, but even outside of the Gaussian case, these are optimal for the class of _linear_ operators.

We can compute the marginal data likelihood alongside the kalman fiter using the prediciton errors $\bv e_t$. 
These, under the assumptions we have made about $\eta$ and $\epsilon$ being normal, are also normal with zero mean and variance

$$\begin{split}
\mathbb V\mathrm{ar}[\bv e_t]=\Sigma_t= \Phi_{t} P_{t\mid t-1} \Phi_{t}^\intercal + \Sigma_\epsilon. 
\end{split}
$$

Therefore, the log-likelihood at each time is

$$\begin{split}
\mathcal L(Z\mid\bv\theta) = -\frac12\sum \log\det(\Sigma_t(\bv\theta)) - \frac12 \sum\bv e_t(\bv\theta)^\intercal\Sigma_{t}(\bv\theta)^{-1} \bv e_t(\bv\theta) - \frac{n_{t}}{2}\log(2*\pi).
\end{split}
$$

Summing these across time, we get the log likelihood for all the data. 


A simplified example of the kalman filter function, written to be jax compatible, used in the package is this;

```{python}
#| eval: false

# TODO: Replace this with a simpler 'naive' implementation
@jax.jit
def kalman_filter(
    m_0: ArrayLike,
    P_0: ArrayLike,
    M: ArrayLike,
    PHI_obs: ArrayLike,
    Sigma_eta: ArrayLike,
    Sigma_eps: ArrayLike,
    ztildes: ArrayLike,  # data matrix, with time across columns
) -> tuple:
    nbasis = m_0.shape[0]
    nobs = ztildes.shape[0]

    @jax.jit
    def step(carry, z_t):
        m_tt, P_tt, _, _, ll, _ = carry

        # predict
        m_pred = M @ m_tt
        P_pred = M @ P_tt @ M.T + Sigma_eta

        # Update

        # Prediction Errors
        eps_t = z_t - PHI_obs @ m_pred

        Sigma_t = PHI_obs @ P_pred @ PHI_obs.T + Sigma_eps

        # Kalman Gain
        K_t = (
            jnp.linalg.solve(Sigma_t, PHI_obs)
            @ P_pred.T
        ).T

        m_up = m_pred + K_t @ eps_t

        P_up = (jnp.eye(nbasis) - K_t @ PHI_obs) @ P_pred

        # likelihood of epsilon, using cholesky decomposition
        chol_Sigma_t = jnp.linalg.cholesky(Sigma_t)
        z = jax.scipy.linalg.solve_triangular(chol_Sigma_t, eps_t)
        ll_new = ll - jnp.sum(jnp.log(jnp.diag(chol_Sigma_t))
                              ) - 0.5 * jnp.dot(z, z)

        return (m_up, P_up, m_pred, P_pred, ll_new, K_t), (
            m_up,
            P_up,
            m_pred,
            P_pred,
            ll_new,
            K_t,
        )

    carry, seq = jl.scan(
        step,
        (m_0, P_0, m_0, P_0, 0, jnp.zeros((nbasis, nobs))),
        ztildes.T,
    )

    return (carry[4], seq[0], seq[1], seq[2][1:], seq[3][1:], seq[5][1:])
```

For the documentation of the method proveded by the package, see [WORK OUT HOW TO LINK DO PAGES]

## The Information Filter

In some computational scenarios, it is beneficial to work with vectors of consistent dimension.
In python jax, the efficient ```scan``` and ```map``` operations work only with such operations; JAX has no support for jagged arrays, and traditional for loops with have long compile times when jit-compiled.
Although there are some tools in JAX to get around this problem (namely the ```jax.tree``` functions which allow mapping over PyTrees), scan is still a large problem; since the Kalman filter is, at it's core, a scan-type operation, this causes a large problem when the observation dimension is changing, as is frequent with many spatio-temporal data.

But it is possible to re-write the kalman filter in a way which is compatible with this kind of data.
the sometimes called 'information filter' involves transforming the data into a kind of 'information form', which will always have consistent dimension.

The information filter is simply the kalman filter re-written to use the Gaussian distribution's canonical parameters, those being the information vector and the information matrix.
If a Gaussian distribution has mean $\bv\mu$ and variance matrix $\Sigma$, then the corresponding _information vector_ and _information matrix_ is $\nu = \Sigma^{-1}\mu$ and $Q = \Sigma^{-1}$, correspondingly.


::: {#thm-information_filter}
## The Information Filter

The Kalman filter can be rewritten in information form as follows [for example, @khan2005matrix].
Write

$$\begin{split}
Q_{i\mid j} &= P_{i\mid j}\\
\bv\nu_{i\mid j} &= Q_{i\mid j} \bv m_{i\mid j}
\end{split}
$$

and transform the observations into their 'information form', for $t=1,\dots, T$

$$\begin{split}
I_{t} = \Phi_{t}^{\intercal} \Sigma_{\epsilon}^{-1}\Phi_{t},\\
i_{t} = \Phi_{t}^{\intercal} \Sigma_{\epsilon}^{-1} \bv z_{t}.
\end{split}
$${#eq-obsinfo}

The prediction step now becomes

$$\begin{split}
\bv\nu_{t+1\mid t} &= (I-J_t) M^{-1}\bv\nu_{t\mid t}\\
Q_{t+1\mid t} &= (I-J_t) S_{t} 
\end{split}
$$

where $S_t = M^{-\intercal} Q_{t\mid t} M^{-1}$ and $J_t = S_t [S_{t}+\Sigma_{\eta}^{-1}]^{-1}$.

Updating is now as simple as adding the information-form observations;

$$\begin{split}
  \bv\nu_{t+1\mid t+1} &= \bv\nu_{t+1\mid t} + i_{t+1}\\
  Q_{t+1\mid t+1} &= Q_{t+1\mid t} + I_{t+1}.
\end{split}
$$

:::


Proof in Appendix (@sec-app1.)

We can see that the information form of the observations (@eq-obsinfo) will always have the same dimension (that being the process dimension, previously labelled $r$, the number of basis functions used in the expansion).
For our purposes, this means that ```jax.lax.scan``` will work after we 'informationify' the data, which can be done using ```jax.tree.map```.
This is implemented in the functions ```information_filter``` and ```information_filter_indep``` (for uncorrelated errors).

There are other often cited advantages to filtering in this form.
It can be quicker that the traditional form in certain cases, especially when the observation dimension is bigger than the state dimension (since you solve a smaller system of equations with $[S_t + \Sigma_\eta]^{-1}$ in the process dimesnion instead of $[\Phi_t P_{t+1\mid t} \Phi_t^\intercal + \Sigma_\epsilon]^{-1}$ in the observation dimension) [@assimakis2012information].

The other often mentioned advantage is the ability to use a truly vague prior for $\alpha_0$; that is, we can set $Q_0$ as the zero matrix, without worriying about an infinite variance matrix.
While this is indeed true, it is actually possible to do the same with the Kalman filter by doing the first step analytically, see (@sec-vagueprior).

As with the kalman filter, it is also possible to get the data likelihood in-line as well.
Again, we would like to stick with things in the state dimension, so working direclty with the prediction errors $\bv e_t$ should be avoided.
Luckily, by multipliying the errors by $\Phi_t^\intercal \Sigma_\epsilon^{-1}$, we can define the 'information errors' $\bv \iota_t$;

$$\begin{split}
  \bv \iota_t &= \Phi_t^\intercal \Sigma_\epsilon^{-1} \bv e_t = \Phi_t^\intercal \Sigma_\epsilon^{-1} \tilde{\bv z}_t -\Phi_t^\intercal \Sigma_\epsilon^{-1}\Phi_t m_{t\mid t-1}\\
  &= i_t - I_tQ_{t\mid t-1}^{-1}\bv \nu_{t\mid t-1}.
\end{split}
$$

The variance of this quantity is also easy to find;

$$\begin{split}
  \var[\bv \iota_t] &= \Phi_t^\intercal \Sigma_\epsilon^{-1}\var[\bv e_t]\Sigma_\epsilon^{-1}\Phi_t\\
  &= \Phi_t^\intercal \Sigma_\epsilon^{-1} [\Phi_{t} P_{t\mid t-1} \Phi_{t}^\intercal + \Sigma_\epsilon] \Sigma_\epsilon^{-1}\Phi_t\\
  &= \Phi_t^\intercal \Sigma_\epsilon^{-1}\Phi_{t} Q_{t\mid t-1}^{-1} \Phi_{t}^\intercal \Sigma_\epsilon^{-1}\Phi_t \Phi_t^\intercal \Sigma_\epsilon^{-1} \Phi_t\\
  &= I_t Q_{t\mid t-1}^{-1} I_t^\intercal + I_t =: \Sigma_{\iota, t}.
\end{split}
$$

Noting that $\bv \iota$ clearly still has mean zero, this allows us once again to compute the log likelihood, this time through $\bv\iota$


$$\begin{split}
\mathcal L(z_t\mid\bv\theta) = -\frac12\sum \log\det(\Sigma_{\iota, t}(\bv\theta)) - \frac12 \sum\bv \iota_t(\bv\theta)^\intercal\Sigma_{\iota, t}(\bv\theta)^{-1} \bv \iota_t(\bv\theta) - \frac{r}{2}\log(2*\pi).
\end{split}
$$

## Kalman Smoothers

Beyond the Kalman filters, we can also do Kalman smoothers. 
That is, filters estimate $\bv m_{T\mid T}$ and $P_{T\mid T}$, but there is use for estimating $\bv m_t\mid T$ and $P_{t\mid T}$ for all $t=0,\dots, T$.

We can then work backwards from these values using what is known as the _Rauch-Tung-Striebel (RTS) smoother_;

$$\begin{split}
\bv m_{t-1\mid T} &= \bv m_{t-1\mid t-1} + J_{t-1}(\bv m_{t\mid T} - \bv m_{t\mid t-1}),\\
P_{t-1\mid T} &= P_{t-1\mid t-1} + J_{t-1}(P_{t\mid T} - P_{t\mid t-1})J_{t-1}^\intercal,
\end{split}
$$ {#eq-kalmansmooth}
 
where,
 
$$\begin{split}
J_{t-1} = P_{t-1\mid t-1}M^\intercal[P_{t\mid t-1}]^{-1}.
\end{split}
$$

We can clearly see, then, that it is crucial to keep the values in @eq-kalman-predict.

We can then also compute the lag-one cross-covariance matrices $P_{t,t-1\mid T}$ using the _Lag-One Covariance Smoother_ (is this what they call the RTS smoother?)
From

$$\begin{split}
P_{T,T-1\mid T} = (I - K_T\Phi_{T}) MP_{T-1\mid T-1},
\end{split}
$$

we can compute the lag-one covariances

$$\begin{split}
P_{t, t-1\mid T} = P_{t\mid t}J_{t-1}^\intercal + J_{t}[P_{t+1,t\mid T} - MP_{t-1\mid t-1}]J_{t-1}^\intercal
\end{split}
$$ {#eq-lag1smooth}

These values can be used to implement the expectation-maximisation (EM) algorithm which will be introduced later.


# EM Algorithm (NEEDS A LOT OF WORK, PROBABLY IGNORE FOR NOW)

Instead of the marginal data likelihood, we may instead want to work with the 'full' likelihood, including the unobserved process, $l(\bv z(1),\dots, \bv z(T), \bv Y(1), \dots, \bv Y(T)\mid \bv\theta)$, or, equivalently, $l(\bv z(1),\dots, \bv z(t), \bv \alpha(1), \dots, \bv\alpha(T)\mid \bv\theta)$.
This is difficult to maximise directly, but can be done with the EM algorithm, consisting  of two steps, which can be shown to always increase the full likelihood.

Firstly, the E step is to find the function

$$\begin{split}
\mathcal Q(\bv \theta; \bv \theta') = \mathbb E_{\bv Z(t)\sim p(Z \mid \bv\alpha(t),\bv\theta)}[\log p_{\bv\theta}(Z^{(T)}, A^{(T)})\mid Z^{(T)}],
\end{split}
$${#eq-Qdef}

where $Z^{(T)} = \{\bv z_t\}_{t=0,\dots,T}$, $A^{(T)} = \{\bv \alpha_t\}_{t=0,\dots,T}$ and $A^{(T-1)} = \{\bv \alpha_t\}_{t=0,\dots,T-1}$. This approximates $\log p_\theta(Z^{(T)}, A^{(T)})$.

::: {#prp-EMQ}
We have <span style="color: red;">[NOTE: This may well be wrong in places...]</span>

$$\begin{split}
-2\mathcal Q(\bv\theta;\bv\theta') &= \mathbb E_{Z^{(T)}\sim p(Z \mid A^{(T)},\bv\theta')}[\log p_{\bv\theta}(Z^{(T)}, A^{(T)}\mid Z^{(T)} = z^{(T)})]\\
&\eqc \sigma_\epsilon^2 [\sum_{t=0}^{T}\bv z_t^{\intercal}z_t - 2\Phi_t(\sum_{t=1}^{T} \bv z_t^\intercal \bv m_{t\mid T}) - 2(\sum_{t=0}^{T} \bv z_t^T)X_t\bv\beta\\
&\quad\quad\quad +\Phi_t^\intercal(\sum_{t=0}^{T}\mathrm{tr}\{P_{t\mid T} - \bv m_{t\mid T}\bv m_{t\mid T}^{\intercal}\})\Phi_t + 2X_t\bv\beta\Phi_t(\sum_{t=0}^{T}\bv m_{t\mid T}) + (\sum_{t=1}^{T}X_t^\intercal \bv\beta^{\intercal}\bv\beta X_t)]\\
&\quad + \mathrm{tr}\{\Sigma_\eta^{-1}[(\sum_{t=1}^{T}P_{t\mid T} - m_{t\mid T}) - 2M(\sum_{t=1}^{T}P_{t,t-1\mid T} - \bv m_{t-1,T}\bv m_{t\mid T}^{\intercal})\\
&\quad\quad\quad\quad\quad + M(\sum_{t=1}^{T}P_{t-1\mid T} - \bv m_{t-1\mid T}\bv m_{t-1\mid T}^{\intercal})M^\intercal]\}\\
&\quad + \mathrm{tr}\{\Sigma_0^{-1}[P_{0\mid T} - m_{0\mid T}m_{0\mid T}^{\intercal} - 2\bv m_{0\mid T}\bv m_0 + \bv m_0\bv m_0^\intercal]\}\\
&\quad + \log(\det(\sigma_\epsilon^{2T}\Sigma_\eta^{T+1}\Sigma_0))
\end{split}
$${#eq-Q}

:::
 

::: {.proof}

See appendix.

:::


In the EM algorithm, we maximise the full likelihood by changing $\bv \theta$ in order to increase (@eq-Q), which can be shown to guarantee that the Likelihood $L(\bv \theta)$ also increases.
The idea is then that repeatedly alternating between adjusting $\bv \theta$ to increase @eq-Q, and then doing the filters and smoothers to obtain new values for $\bv m_{t\mid T}$, $P_{t\mid T}$, and $P_{t,t-1\mid T}$.

# Algorithm for Maximum Complete-data Likelihood estimation

Overall, our algorithm for Maximum Likelihood estimation is:

1. Set $i=0$ and take an initial guess for the parameters we are considering, $\bv\theta_0=\bv\theta_i$
2. Starting from $\bv m_{0\mid 0}=\bv m_0, P_{0\mid0}=\Sigma_0$, run the __Kalman Filter__ to get $\bv m_{t\mid t}$, $P_{t\mid t}$, and $K_t$ for all $t$ @eq-kalman-update,
3. Starting from $\bv m_{T\mid T}, P_{T\mid T}$, run the __Kalman Smoother__ to get $\bv m_{t\mid T}$, $P_{t\mid T}$, and $J_t$ for all $t$ (@eq-kalmansmooth),
4. Starting from $P_{T,T-1\mid T} = (I - K_nA_n) MP_{T-1\mid T-1}$, run the __Lag-One Smoother__ to get $\bv m_{t,t-1\mid T}$ and $P_{t,t-1\mid T}$ for all $t$ @eq-lag1smooth,
5. Use the above values to construct $\mathcal Q(\bv\theta;\bv \theta')$ in @eq-Q,
6. Maximise the function $\mathcal Q(\bv\theta;\bv \theta')$ to get a new guess $\bv \theta_{i+1}$, then return to step 2,
8. Stop once a certain criteria is met.

\newpage
\appendix
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}

# Appendix {.appendix}

## Woodbury's identity 

The following two sections will make heavy use of the [Woodbury identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity).



::: {#lem-woodbury}
## Woodbury's Identity

We have, for conformable matrices $A, U, C, V$,

$$\begin{split}
(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
\end{split}
$${#eq-woodbury}

Additionally, we have the variant 

$$\begin{split}
(A + UCV)^{-1}UC = A^{-1} U(C^{-1} + VA^{-1}U)^{-1}.
\end{split}
$${#eq-woodbury2}

:::



::: {.proof}

We only prove (@eq-woodbury2), since various proofs of (@eq-woodbury) are well known (see, for example, the wikipedia page).

Simply multipliying (@eq-woodbury) by $CU$, [similar to @khan2005matrix, although there is an error in their proof]

$$\begin{split}
(A+UCV)^{-1}UC &= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}UC\\
&= A^{-1}UC - A^{-1}U(C^{-1} + VA^{-1}U) [(C^{-1} +VA^{-1}U)C - I]\\
&= A^{-1}U(C^{-1}+VA^{-1}U)
\end{split}
$$

as needed. \qed

:::



## Proof of @thm-information_filter {#sec-app1}


::: {.proof}

Firstly, for the prediction step, using $S_t = M^{-\intercal}Q_{t\mid t}M^{-1}$ and $J_t = S_t(\Sigma_\eta^{-1} + S_t)^{-1}$ and the identities @eq-woodbury and @eq-woodbury2,

$$\begin{split}
  Q_{t+1\mid t} &= P_{t+1\mid t}^{-1} = (MQ_{t\mid t}^{-1}M^\intercal + \Sigma_\eta)^{-1}\\
  &= S_t - J_t S_t = (I-J_t)S_t,
\end{split}
$$

where we used $A=MQ_{t\mid t}^{-1}M^\intercal$, $C=\Sigma_\eta$ and $U=C=I$ in @eq-woodbury. 
Thurthermore,

$$\begin{split}
  \bv \nu_{t+1\mid t} &= Q_{t+1\mid t} \bv m_{t+1\mid t}\\
  &= Q_{t+1\mid t} M Q_{t\mid t}^{-1} \bv \nu_{t\mid t} = Q_{t+1\mid t} (M Q_{t\mid t}^{-1}) \bv \nu_{t\mid t}\\
  &= (I-J_t)M^{-\intercal}Q_{t\mid t}M^{-1} (M Q_{t\mid t}^{-1}) \bv \nu_{t\mid t}\\
  &= (I-J_t)M^{-\intercal} \bv \nu_{t\mid t}.
\end{split}
$$

For the update step,

$$\begin{split}
  Q_{t+1\mid t+1} &= P_{t+1\mid t+1}^{-1}\\
  &= (Q_{t+1}^{-1} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}\Sigma_\epsilon\Phi_{t+1}^\intercal + \Sigma_\epsilon]^{-1}\Phi_{t+1}Q_{t+1\mid t}^{-1})^{-1}\\
  &= ((Q_{t+1\mid t} + \Phi_{t+1}^\intercal\Sigma_\epsilon^{-1}\Phi_{t+1})^{-1})^{-1} = Q_{t+1\mid t} + \Phi_{t+1}^\intercal\Sigma_\epsilon^{-1}\Phi_{t+1}\\
  &= Q_{t+1\mid t} + I_{t+1}.
\end{split}
$$

Then, writing $\bv m_{t+1\mid t+1}$ in terms of $Q_{t+1\mid t}$ and $\bv \nu_{t+1\mid t}$

$$\begin{split}
  \bv m_{t+1\mid t+1} &= Q_{t+1\mid t}^{-1} \bv \nu_{t+1\mid t} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1} [\tilde{\bv z}_{t+1} - \Phi_{t+1}Q_{t+1\mid t}^{-1}\bv \nu_{t+1\mit t}]\\
  &= (Q_{t+1\mid t}^{-1} - Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1}\Phi_{t+1}Q_{t+1\mid t}^{-1})\bv \nu_{t+1\mid t} \\
  &\quad + Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal[\Phi_{t+1}Q_{t+1\mid t}^{-1}\Phi_{t+1}^\intercal +\Sigma_\epsilon]^{-1}\tilde{\bv z}_{t+1}\\
  &= [Q_{t+1\mid t} + I_{t+1}]^{-1}\bv \nu_{t+1\mid t}\\
  &\quad + [Q_{t+1\mid t} + I_{t+1}]^{-1}\Phi_{t+1}\Sigma_\epsilon^{-1}\tilde{\bv z}_{t+1},
\end{split} 
$$

and now noting that $\bv\nu_{t+1\mid t+1} = (Q_{t+1\mid t} + I_{t+1}) \bv m_{t+1\mid t+1}$, we complete the proof. \qed
:::


## Truly Vague Prior with the Kalman Filter {#sec-vagueprior}

It has been stated before that one of the large advantages of the information filter is the ability to use a completely vague prior $Q_{0}=0$.
While this is true, it is actually possible to do this in the Kalman filter by 'skipping' the first step (contrary to some sources, such as the wikipedia page as of January 2025).


::: {#thm-vagueprior}

In the Kalman Filter (@sec-kalmanfilter), if we allow $P_{0}^{-1} = 0$, effectively setting infinite variance, and assuming the propegator matrix $M$ is invertible, we have

$$\begin{split}
  \bv m_{1\mid1} &= (\Phi_1^\intercal \Sigma_\epsilon^{-1} \Phi_1)^{-1} \Phi_1 \Sigma_\epsilon^{-1} \tilde{\bv z}_1,\\
  P_{1\mid1} &= (\Phi_1^\intercal \Sigma_\epsilon^{-1} \Phi_1)^{-1}.
\end{split}
$$ {#eq-kalmanvague}

Therefore, starting with these values then continuing the filter as normal, we can perform the kalman filter with 'infinite' prior variance.

<span style="color: red;">[NOTE: The requirement that M be invertible should be droppable, see the proof below]</span>

:::




::: {.proof}

Unsuprisingly, the proof is effectively equivalent to proving the information filter and setting $Q_0 = P_0^{-1}=0$.

For the first predict step (@eq-kalman-predict),

$$\begin{split}
  \bv m_{1\mid0} &= M \bv m_0,\\
  P_{1\mid0} &= M P_0 M^\intercal + \Sigma_\eta.
\end{split}
$$

By (@eq-woodbury),

$$\begin{split}
  P_{1\mid0}^{-1} &= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} M (P_0^{-1} + M^\intercal \Sigma_\eta^{-1} M)^{-1}M^\intercal\Sigma_\eta^{-1}\\
  &= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} M (M^\intercal \Sigma_\eta^{-1} M)^{-1}M^\intercal\Sigma_\eta^{-1}\\
  &= \Sigma_\eta^{-1} - \Sigma_\eta^{-1} = 0.
\end{split}
$$

So, moving to the update step (@eq-kalman-update),

$$\begin{split}
  \bv m_{1\mid1} = M \bv m_0 + P_{1\mid0}\Phi_1 [\Phi_1 P_{1\mid0} \Phi_1^\intercal + \Sigma_\epsilon]^{-1}(\tilde{\bv{z}}_1 - \Phi M \bv m_0).\\
\end{split}
$$

Applying (@eq-woodbury2) with $A = P_{1\mid0}^{-1}, U=\Phi_1, V=\Phi_1^\intercal, C=\Sigma_\epsilon^{-1}$,

$$\begin{split}
  \bv m_{1\mid1} &= M \bv m_0 + (P_{1\mid0}^{-1} + \Phi_1^\intercal\Sigma_\epsilon^{-1} \Phi_1)^{-1}\Phi_1^\intercal \Sigma_\epsilon^{-1}(\tilde{\bv{z}}_1 - \Phi_1 M\bv m_0)\\
  &= M \bv m_0 + (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1} \tilde{\bv{z}}_1 - (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1M\bv m_0\\
  &= (\Phi_1^\intercal\Sigma_\epsilon^{-1}\Phi_1)^{-1}\Phi_1^\intercal\Sigma_\epsilon^{-1} \tilde{\bv{z}}_1.
\end{split}
$$

For the variance, we apply the (@eq-woodbury) with $A = P_{1\mid0}^{-1}, U=\Phi_1^\intercal, V=\Phi_1, C=\Sigma_\epsilon^{-1}$,

$$\begin{split}
  P_{1\mid1} &= (I - P_{1\mid0}\Phi_1^\intercal[\Sigma_\epsilon + \Phi_1^\intercal P_{1\mid0}\Phi_1]^{-1}\Phi_1)P_{1\mid0}\\
  &= (P_{1\mid0}^{-1} + \Phi_1^\intercal \Sigma_\epsilon^{-1}\Phi_1)^{-1}\\
  &= (\Phi_1^\intercal \Sigma_\epsilon^{-1}\Phi_1)^{-1},
\end{split}
$$

as needed. \qed

:::


It is worth noting that (@eq-kalmanvague) seems to make a lot of sense; namely, we expect the estimate for $\bv m_0$ to look like a correlated least squares-type estimator like this.
